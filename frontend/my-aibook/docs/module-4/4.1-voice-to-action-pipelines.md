---
sidebar_position: 1
---

# Chapter 4.1 â€“ Voice-to-Action Pipelines

## Learning Objectives
- Implement OpenAI Whisper for speech recognition in humanoid robots
- Create speech-to-intent systems for robot command interpretation
- Optimize for low-latency voice command processing
- Ensure command reliability in noisy environments

## OpenAI Whisper Integration

OpenAI Whisper provides state-of-the-art speech recognition capabilities that are well-suited for humanoid robots. Here's how to integrate it effectively:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from audio_common_msgs.msg import AudioData
from geometry_msgs.msg import Twist
from sensor_msgs.msg import JointState
import whisper
import torch
import numpy as np
import librosa
import threading
import queue
from typing import Optional, Dict, Any
import time

class VoiceToActionNode(Node):
    def __init__(self):
        super().__init__('voice_to_action_node')

        # Initialize Whisper model
        self.get_logger().info('Loading Whisper model...')
        self.model = whisper.load_model("medium")  # Choose based on compute resources

        # For real-time applications, you might want to use a lighter model
        # self.model = whisper.load_model("small")  # Faster but less accurate
        # self.model = whisper.load_model("tiny")   # Fastest but least accurate

        # Audio processing parameters
        self.sample_rate = 16000  # Whisper expects 16kHz
        self.audio_buffer = np.array([])
        self.buffer_size = self.sample_rate * 2  # 2 seconds of audio
        self.min_audio_length = self.sample_rate * 0.5  # Minimum 0.5 seconds

        # Command processing
        self.command_queue = queue.Queue()
        self.intent_processor = IntentProcessor()

        # Subscriptions
        self.audio_sub = self.create_subscription(
            AudioData,
            '/audio/input',
            self.audio_callback,
            10
        )

        # Publishers for different robot actions
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)
        self.speech_pub = self.create_publisher(String, '/tts_input', 10)

        # Timer for processing audio chunks
        self.process_timer = self.create_timer(1.0, self.process_audio_buffer)

        # Voice activity detection parameters
        self.energy_threshold = 0.01
        self.silence_duration = 0.5  # seconds of silence to trigger processing
        self.last_voice_time = time.time()

        self.get_logger().info('Voice-to-Action Node initialized')

    def audio_callback(self, msg: AudioData):
        """Receive audio data and add to processing buffer"""
        try:
            # Convert audio data to numpy array
            # Assuming audio data is in 16-bit PCM format
            audio_int16 = np.frombuffer(msg.data, dtype=np.int16)
            audio_float32 = audio_int16.astype(np.float32) / 32768.0

            # Resample to 16kHz if needed
            if msg.info.sample_rate != self.sample_rate:
                audio_float32 = librosa.resample(
                    audio_float32,
                    orig_sr=msg.info.sample_rate,
                    target_sr=self.sample_rate
                )

            # Append to buffer
            self.audio_buffer = np.concatenate([self.audio_buffer, audio_float32])

            # Check if we have enough audio and if voice activity is detected
            if len(self.audio_buffer) >= self.min_audio_length:
                energy = np.mean(np.abs(self.audio_buffer[-self.sample_rate:]))  # Last second
                if energy > self.energy_threshold:
                    self.last_voice_time = time.time()
                elif (time.time() - self.last_voice_time) > self.silence_duration:
                    # Silence detected after voice, process the utterance
                    self.process_audio_buffer()

            # Keep only the most recent buffer_size samples
            if len(self.audio_buffer) > self.buffer_size:
                self.audio_buffer = self.audio_buffer[-self.buffer_size:]

        except Exception as e:
            self.get_logger().error(f'Error processing audio: {e}')

    def process_audio_buffer(self):
        """Process accumulated audio buffer with Whisper"""
        if len(self.audio_buffer) < self.min_audio_length:
            return  # Not enough audio to process

        try:
            # Transcribe audio using Whisper
            result = self.model.transcribe(
                self.audio_buffer,
                language='en',
                temperature=0.0,  # More deterministic
                compression_ratio_threshold=2.4,  # Filter out low-quality transcriptions
                logprob_threshold=-1.0,  # Filter out low-confidence transcriptions
                no_speech_threshold=0.6  # Filter out silence
            )

            transcription = result['text'].strip()

            if transcription and len(transcription) > 3:  # Meaningful transcription
                self.get_logger().info(f'Transcribed: "{transcription}"')

                # Add to command queue for intent processing
                self.command_queue.put({
                    'transcription': transcription,
                    'timestamp': time.time()
                })

                # Process commands in a separate thread to avoid blocking
                threading.Thread(target=self.process_command, daemon=True).start()

            # Clear buffer after processing
            self.audio_buffer = np.array([])

        except Exception as e:
            self.get_logger().error(f'Error transcribing audio: {e}')

    def process_command(self):
        """Process voice commands and convert to robot actions"""
        try:
            if not self.command_queue.empty():
                command_data = self.command_queue.get_nowait()
                transcription = command_data['transcription']

                # Process intent
                intent_result = self.intent_processor.parse_intent(transcription)

                if intent_result:
                    self.execute_robot_action(intent_result)
                else:
                    self.get_logger().warn(f'Could not parse intent from: "{transcription}"')
                    # Respond with clarification request
                    self.request_clarification(transcription)

        except queue.Empty:
            pass
        except Exception as e:
            self.get_logger().error(f'Error processing command: {e}')

    def execute_robot_action(self, intent_result: Dict[str, Any]):
        """Execute the parsed intent as a robot action"""
        action_type = intent_result['action']

        if action_type == 'move':
            self.execute_move_command(intent_result)
        elif action_type == 'manipulate':
            self.execute_manipulation_command(intent_result)
        elif action_type == 'speak':
            self.execute_speak_command(intent_result)
        elif action_type == 'balance':
            self.execute_balance_command(intent_result)
        elif action_type == 'navigation':
            self.execute_navigation_command(intent_result)
        else:
            self.get_logger().warn(f'Unknown action type: {action_type}')

    def execute_move_command(self, intent_result: Dict[str, Any]):
        """Execute movement commands"""
        direction = intent_result.get('direction', 'forward')
        distance = intent_result.get('distance', 1.0)  # meters
        speed = intent_result.get('speed', 0.5)  # m/s

        cmd_vel = Twist()

        if direction == 'forward':
            cmd_vel.linear.x = speed
        elif direction == 'backward':
            cmd_vel.linear.x = -speed
        elif direction == 'left':
            cmd_vel.angular.z = speed
        elif direction == 'right':
            cmd_vel.angular.z = -speed
        elif direction == 'turn_left':
            cmd_vel.angular.z = speed
        elif direction == 'turn_right':
            cmd_vel.angular.z = -speed

        self.cmd_vel_pub.publish(cmd_vel)
        self.get_logger().info(f'Moving {direction} at {speed}m/s for {distance/speed:.1f}s')

    def execute_navigation_command(self, intent_result: Dict[str, Any]):
        """Execute navigation commands"""
        destination = intent_result.get('destination', 'unknown')

        # This would typically interface with navigation stack
        # For now, just log the command
        self.get_logger().info(f'Navigating to {destination}')

        # Publish to navigation system
        nav_cmd = String()
        nav_cmd.data = f"navigate_to_{destination.replace(' ', '_')}"
        self.speech_pub.publish(nav_cmd)  # Using speech pub temporarily

    def request_clarification(self, transcription: str):
        """Request clarification for ambiguous commands"""
        response = String()
        response.data = f"I heard you say '{transcription}' but I'm not sure what you mean. Could you please repeat or clarify?"
        self.speech_pub.publish(response)

class IntentProcessor:
    """Process voice transcriptions and extract intents"""

    def __init__(self):
        # Define command patterns
        self.move_patterns = {
            'forward': ['forward', 'ahead', 'go', 'move', 'straight', 'front'],
            'backward': ['back', 'backward', 'reverse', 'behind'],
            'left': ['left', 'port', 'starboard right'],
            'right': ['right', 'starboard', 'port left'],
            'turn_left': ['turn left', 'rotate left', 'pivot left'],
            'turn_right': ['turn right', 'rotate right', 'pivot right']
        }

        self.navigation_patterns = {
            'kitchen': ['kitchen', 'cooking area', 'food area'],
            'bedroom': ['bedroom', 'sleeping area', 'bed area'],
            'living_room': ['living room', 'living area', 'sitting area', 'couch'],
            'bathroom': ['bathroom', 'restroom', 'toilet', 'shower'],
            'office': ['office', 'study', 'work area', 'desk']
        }

        self.manipulation_patterns = {
            'pick_up': ['pick up', 'grab', 'take', 'lift', 'get'],
            'put_down': ['put down', 'place', 'drop', 'set'],
            'wave': ['wave', 'hello', 'greet', 'waving'],
            'point': ['point', 'show', 'indicate', 'direct']
        }

        self.balance_patterns = {
            'balance': ['balance', 'steady', 'stable', 'don\'t fall', 'keep balance'],
            'crouch': ['crouch', 'squat', 'bend down', 'lower'],
            'stand': ['stand', 'rise', 'up', 'straighten']
        }

    def parse_intent(self, transcription: str) -> Optional[Dict[str, Any]]:
        """Parse the intent from a transcription"""
        transcription_lower = transcription.lower()

        # Check for movement commands
        for action, patterns in self.move_patterns.items():
            if any(pattern in transcription_lower for pattern in patterns):
                # Extract distance if mentioned
                distance = self.extract_distance(transcription_lower)
                speed = self.extract_speed(transcription_lower)

                return {
                    'action': 'move',
                    'direction': action,
                    'distance': distance,
                    'speed': speed
                }

        # Check for navigation commands
        for destination, patterns in self.navigation_patterns.items():
            if any(pattern in transcription_lower for pattern in patterns):
                return {
                    'action': 'navigation',
                    'destination': destination
                }

        # Check for manipulation commands
        for action, patterns in self.manipulation_patterns.items():
            if any(pattern in transcription_lower for pattern in patterns):
                return {
                    'action': 'manipulate',
                    'action_type': action
                }

        # Check for balance commands
        for action, patterns in self.balance_patterns.items():
            if any(pattern in transcription_lower for pattern in patterns):
                return {
                    'action': 'balance',
                    'action_type': action
                }

        # Check for speaking commands
        speak_keywords = ['say', 'speak', 'tell', 'repeat', 'announce']
        if any(keyword in transcription_lower for keyword in speak_keywords):
            return {
                'action': 'speak',
                'text': self.extract_speech_content(transcription_lower)
            }

        return None  # No recognizable intent found

    def extract_distance(self, text: str) -> float:
        """Extract distance from text"""
        # Look for patterns like "2 meters", "3 feet", etc.
        import re

        # Pattern for distance: number + unit
        distance_pattern = r'(\d+(?:\.\d+)?)\s*(meter|meters|metre|metres|foot|feet|inch|inches|cm|centimeter|centimeters)'

        match = re.search(distance_pattern, text)
        if match:
            distance = float(match.group(1))
            unit = match.group(2).lower()

            # Convert to meters
            if unit in ['foot', 'feet']:
                return distance * 0.3048
            elif unit in ['inch', 'inches']:
                return distance * 0.0254
            elif unit in ['cm', 'centimeter', 'centimeters']:
                return distance * 0.01
            else:  # meters
                return distance

        return 1.0  # Default distance

    def extract_speed(self, text: str) -> float:
        """Extract speed from text"""
        import re

        # Look for speed indicators
        slow_indicators = ['slow', 'slowly', 'carefully', 'gently']
        fast_indicators = ['fast', 'quickly', 'hurry', 'hastily']

        if any(indicator in text for indicator in slow_indicators):
            return 0.3  # Slow speed
        elif any(indicator in text for indicator in fast_indicators):
            return 0.8  # Fast speed
        else:
            return 0.5  # Medium speed

    def extract_speech_content(self, text: str) -> str:
        """Extract what the robot should say from speech command"""
        import re

        # Pattern: say/repeat/tell "quoted text" or after the command
        quote_pattern = r'(?:say|repeat|tell|speak)\s*["\']([^"\']+)["\']'
        match = re.search(quote_pattern, text)

        if match:
            return match.group(1)
        else:
            # If no quotes, return everything after the command verb
            for verb in ['say', 'repeat', 'tell', 'speak']:
                if verb in text:
                    idx = text.find(verb) + len(verb)
                    content = text[idx:].strip()
                    # Remove common filler words
                    content = content.replace('to me', '').replace('to the robot', '').strip()
                    return content or "Hello, I received a speech command"

        return "I have something to say"

# Real-time audio capture node
class AudioCaptureNode(Node):
    def __init__(self):
        super().__init__('audio_capture_node')

        # Publisher for audio data
        self.audio_pub = self.create_publisher(AudioData, '/audio/input', 10)

        # Audio capture parameters
        self.sample_rate = 16000
        self.chunk_size = 1024
        self.channels = 1

        # Initialize PyAudio
        import pyaudio
        self.pyaudio_instance = pyaudio.PyAudio()

        # Open audio stream
        self.stream = self.pyaudio_instance.open(
            format=pyaudio.paInt16,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size
        )

        # Timer for capturing audio
        self.capture_timer = self.create_timer(0.1, self.capture_audio)

        self.get_logger().info('Audio Capture Node initialized')

    def capture_audio(self):
        """Capture audio from microphone and publish"""
        try:
            # Read audio data
            data = self.stream.read(self.chunk_size, exception_on_overflow=False)

            # Create AudioData message
            audio_msg = AudioData()
            audio_msg.data = data

            # Create AudioInfo (simplified)
            from audio_common_msgs.msg import AudioInfo
            audio_info = AudioInfo()
            audio_info.sample_rate = self.sample_rate
            audio_info.channels = self.channels
            audio_info.format = 2  # 16-bit PCM
            audio_info.type = 1  # Integer samples
            audio_info.bits_per_sample = 16

            audio_msg.info = audio_info

            # Publish audio data
            self.audio_pub.publish(audio_msg)

        except Exception as e:
            self.get_logger().error(f'Error capturing audio: {e}')

def main(args=None):
    rclpy.init(args=args)

    # Create both nodes
    voice_node = VoiceToActionNode()
    audio_node = AudioCaptureNode()

    # Create executor to run both nodes
    executor = rclpy.executors.MultiThreadedExecutor()
    executor.add_node(voice_node)
    executor.add_node(audio_node)

    try:
        executor.spin()
    except KeyboardInterrupt:
        pass
    finally:
        voice_node.destroy_node()
        audio_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Latency Considerations

For humanoid robots, minimizing latency in voice command processing is crucial for natural interaction:

```python
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor
import time

class LowLatencyVoiceProcessor:
    def __init__(self, model_size="small"):
        self.model_size = model_size
        self.model = None
        self.load_model_async()

        # Thread pool for parallel processing
        self.executor = ThreadPoolExecutor(max_workers=2)

        # Pre-allocated buffers
        self.audio_buffer = np.zeros(32000, dtype=np.float32)  # 2 seconds at 16kHz
        self.buffer_idx = 0

        # Real-time constraints
        self.max_processing_time = 0.2  # 200ms maximum processing time
        self.warmup_model()

    def load_model_async(self):
        """Load model in background thread"""
        def load():
            self.model = whisper.load_model(self.model_size)
        threading.Thread(target=load, daemon=True).start()

    def warmup_model(self):
        """Warm up the model with dummy data to reduce first-run latency"""
        dummy_audio = np.random.randn(16000).astype(np.float32)  # 1 second
        try:
            # Run a quick transcription to warm up the model
            self.model.transcribe(dummy_audio[:8000], language='en', temperature=0.0)  # Half second
        except:
            pass  # Model might not be loaded yet

    def process_audio_chunk(self, audio_chunk: np.ndarray) -> Optional[str]:
        """Process audio chunk with latency constraints"""
        start_time = time.time()

        if self.model is None:
            if time.time() - start_time > self.max_processing_time:
                return None
            time.sleep(0.01)  # Brief wait for model loading
            return None

        try:
            # Set a timeout for processing
            future = self.executor.submit(
                self.model.transcribe,
                audio_chunk,
                language='en',
                temperature=0.0
            )

            # Wait for result with timeout
            result = future.result(timeout=self.max_processing_time - (time.time() - start_time))

            if result and 'text' in result:
                return result['text'].strip()

        except Exception as e:
            self.get_logger().warn(f'Latency constraint exceeded or error: {e}')

        return None

# Voice activity detection for reduced processing
class VoiceActivityDetector:
    def __init__(self, threshold=0.01, silence_duration=0.5):
        self.threshold = threshold
        self.silence_duration = silence_duration
        self.last_voice_time = time.time()

    def is_voice_active(self, audio_chunk: np.ndarray) -> bool:
        """Detect if voice is active in the audio chunk"""
        energy = np.mean(np.abs(audio_chunk))
        is_active = energy > self.threshold

        if is_active:
            self.last_voice_time = time.time()

        return is_active

    def should_process(self) -> bool:
        """Determine if audio should be processed based on silence detection"""
        return (time.time() - self.last_voice_time) < self.silence_duration
```

## Command Reliability

Ensuring command reliability in noisy environments:

```python
import sounddevice as sd
import webrtcvad
import collections
import wave
import contextlib

class ReliableVoiceCommandProcessor:
    def __init__(self):
        # Initialize VAD (Voice Activity Detection)
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(3)  # Aggressive VAD mode

        # Audio parameters
        self.sample_rate = 16000
        self.frame_duration = 30  # ms
        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)

        # Command validation
        self.confidence_threshold = 0.8
        self.repetition_threshold = 2  # Require command repetition for critical actions
        self.command_history = collections.deque(maxlen=10)

        # Noise suppression
        self.noise_floor = 0.001
        self.snr_threshold = 10  # Signal-to-noise ratio threshold

    def preprocess_audio(self, audio_data: np.ndarray) -> np.ndarray:
        """Preprocess audio for better recognition in noisy environments"""
        # Apply noise reduction
        from scipy import signal

        # Estimate noise spectrum (assuming first 0.5s is noise)
        noise_samples = int(0.5 * self.sample_rate)
        if len(audio_data) > noise_samples:
            noise_spectrum = np.abs(np.fft.fft(audio_data[:noise_samples]))
        else:
            noise_spectrum = np.ones_like(audio_data) * self.noise_floor

        # Apply spectral subtraction
        audio_spectrum = np.fft.fft(audio_data)
        enhanced_spectrum = np.maximum(
            np.abs(audio_spectrum) - noise_spectrum * 0.5,  # Noise reduction factor
            0.1 * np.abs(audio_spectrum)  # Preserve some original signal
        )

        # Reconstruct signal
        enhanced_audio = np.real(np.fft.ifft(enhanced_spectrum * np.exp(1j * np.angle(audio_spectrum))))

        return enhanced_audio.astype(np.float32)

    def validate_command(self, transcription: str, confidence: float) -> bool:
        """Validate command reliability"""
        # Check confidence threshold
        if confidence < self.confidence_threshold:
            return False

        # Check for common misrecognitions
        if self.is_common_misrecognition(transcription):
            return False

        # Check for command repetition (for critical actions)
        if self.requires_repetition(transcription):
            return self.check_command_repetition(transcription)

        return True

    def is_common_misrecognition(self, transcription: str) -> bool:
        """Check if transcription is likely a common misrecognition"""
        common_misrecognitions = [
            '',  # Empty transcription
            'thank you',  # Often misrecognized
            'i love you',  # Often misrecognized
            'what is your name',  # Often misrecognized
            'hello',  # Often misrecognized
            'yes',  # Often misrecognized
            'no',  # Often misrecognized
        ]

        return transcription.lower().strip() in common_misrecognitions

    def requires_repetition(self, transcription: str) -> bool:
        """Check if command requires repetition (critical actions)"""
        critical_actions = [
            'shutdown', 'power off', 'stop', 'emergency', 'danger',
            'fire', 'help', 'alert', 'warning', 'stop moving'
        ]

        return any(action in transcription.lower() for action in critical_actions)

    def check_command_repetition(self, transcription: str) -> bool:
        """Check if command has been repeated recently"""
        recent_commands = list(self.command_history)[-3:]  # Check last 3 commands
        repetition_count = sum(1 for cmd in recent_commands if cmd == transcription)

        return repetition_count >= self.repetition_threshold

    def calculate_confidence(self, transcription: str, audio_features: Dict) -> float:
        """Calculate confidence score for transcription"""
        # Multiple factors contribute to confidence:

        # 1. Length factor (very short transcriptions less reliable)
        length_factor = min(len(transcription.split()), 10) / 10.0
        if length_factor < 0.2:
            return 0.0  # Too short to be reliable

        # 2. SNR factor
        snr = audio_features.get('snr', 0)
        snr_factor = min(snr / self.snr_threshold, 1.0) if self.snr_threshold > 0 else 1.0

        # 3. Language model coherence (simplified)
        coherence_factor = self.estimate_language_coherence(transcription)

        # 4. Acoustic confidence (from ASR model)
        acoustic_confidence = audio_features.get('acoustic_confidence', 0.8)

        # Weighted combination
        confidence = (
            0.3 * length_factor +
            0.2 * snr_factor +
            0.3 * coherence_factor +
            0.2 * acoustic_confidence
        )

        return min(confidence, 1.0)

    def estimate_language_coherence(self, transcription: str) -> float:
        """Estimate how coherent the transcription is"""
        import re

        # Check for repeated words or fragments (indicating poor recognition)
        words = transcription.lower().split()
        if len(words) < 2:
            return 0.3  # Very low confidence for single words

        # Count repeated words
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1

        repeated_words = sum(1 for count in word_counts.values() if count > 1)
        repetition_ratio = repeated_words / len(set(words)) if words else 0

        # Lower confidence for high repetition
        coherence = max(0.1, 1.0 - repetition_ratio * 0.5)

        # Check for common English word patterns
        common_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']
        common_word_ratio = sum(1 for word in words if word in common_words) / len(words)

        # Higher confidence if common words are present
        coherence *= (0.5 + common_word_ratio * 0.5)

        return coherence

# Example launch file for the voice system
"""
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        # Audio capture node
        Node(
            package='voice_to_action',
            executable='audio_capture_node',
            name='audio_capture',
            parameters=[
                {'sample_rate': 16000},
                {'channels': 1}
            ]
        ),

        # Voice-to-action processing node
        Node(
            package='voice_to_action',
            executable='voice_to_action_node',
            name='voice_processor',
            parameters=[
                {'model_size': 'small'},  # Use 'medium' for better accuracy if resources allow
                {'energy_threshold': 0.01},
                {'silence_duration': 0.5}
            ]
        )
    ])
"""
```

## Practical Exercise

1. Set up Whisper model for real-time voice recognition
2. Implement intent parsing for humanoid robot commands
3. Optimize for low-latency processing
4. Add noise reduction and reliability checks

## Summary

Voice-to-action pipelines enable natural human-robot interaction for humanoid robots. OpenAI Whisper provides robust speech recognition, while intent processing converts speech to meaningful robot actions. Optimizing for low latency and ensuring command reliability in noisy environments are crucial for effective voice control. The system should include voice activity detection, noise reduction, and validation mechanisms to provide reliable and responsive voice control for humanoid robots.