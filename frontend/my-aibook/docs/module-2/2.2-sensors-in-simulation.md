---
sidebar_position: 2
---

# Chapter 2.2 – Sensors in Simulation

## Learning Objectives
- Implement LiDAR sensors for humanoid robots in Gazebo
- Configure depth cameras with realistic parameters
- Set up IMU sensors with appropriate noise models
- Model sensor realism including noise and systematic errors

## LiDAR Simulation

LiDAR sensors are crucial for humanoid robots for navigation, mapping, and obstacle detection. In Gazebo, configure LiDAR with realistic parameters:

```xml
<sdf version="1.7">
  <sensor name="humanoid_lidar" type="ray">
    <always_on>true</always_on>
    <update_rate>10</update_rate>
    <pose>0.1 0 0.5 0 0 0</pose>  <!-- Position on robot head/torso -->
    <ray>
      <scan>
        <horizontal>
          <samples>360</samples>  <!-- 360 degree scan -->
          <resolution>1</resolution>
          <min_angle>-3.14159</min_angle>  <!-- -π radians -->
          <max_angle>3.14159</max_angle>   <!-- π radians -->
        </horizontal>
      </scan>
      <range>
        <min>0.1</min>
        <max>10.0</max>  <!-- 10 meter range -->
        <resolution>0.01</resolution>
      </range>
    </ray>
    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">
      <ros>
        <namespace>/humanoid</namespace>
        <remapping>~/out:=scan</remapping>
      </ros>
      <output_type>sensor_msgs/LaserScan</output_type>
    </plugin>
  </sensor>
</sdf>
```

### Multi-line LiDAR for 3D Perception

For humanoid robots that need 3D perception:

```xml
<sensor name="3d_lidar" type="ray">
  <ray>
    <scan>
      <horizontal>
        <samples>1024</samples>
        <resolution>1</resolution>
        <min_angle>-3.14159</min_angle>
        <max_angle>3.14159</max_angle>
      </horizontal>
      <vertical>
        <samples>64</samples>  <!-- 64 vertical beams -->
        <resolution>1</resolution>
        <min_angle>-0.2618</min_angle>  <!-- -15 degrees -->
        <max_angle>0.2618</max_angle>    <!-- 15 degrees -->
      </vertical>
    </scan>
    <range>
      <min>0.1</min>
      <max>30.0</max>
      <resolution>0.01</resolution>
    </range>
  </ray>
  <!-- Noise model for realistic behavior -->
  <noise type="gaussian">
    <mean>0.0</mean>
    <stddev>0.01</stddev>  <!-- 1cm accuracy -->
  </noise>
</sensor>
```

## Depth Camera Configuration

Depth cameras provide crucial 3D information for humanoid manipulation and navigation:

```xml
<sensor name="depth_camera" type="depth">
  <always_on>true</always_on>
  <update_rate>30</update_rate>
  <pose>0.15 0 0.4 0 0 0</pose>  <!-- Positioned like a head camera -->
  <camera name="humanoid_camera">
    <horizontal_fov>1.047</horizontal_fov>  <!-- 60 degrees -->
    <image>
      <width>640</width>
      <height>480</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.1</near>
      <far>10.0</far>
    </clip>
    <noise>
      <type>gaussian</type>
      <mean>0.0</mean>
      <stddev>0.007</stddev>
    </noise>
  </camera>
  <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">
    <alwaysOn>true</alwaysOn>
    <updateRate>30.0</updateRate>
    <cameraName>head_camera</cameraName>
    <imageTopicName>rgb/image_raw</imageTopicName>
    <depthImageTopicName>depth/image_raw</depthImageTopicName>
    <pointCloudTopicName>depth/points</pointCloudTopicName>
    <cameraInfoTopicName>rgb/camera_info</cameraInfoTopicName>
    <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>
    <frameName>head_camera_depth_optical_frame</frameName>
    <baseline>0.1</baseline>
    <distortion_k1>0.0</distortion_k1>
    <distortion_k2>0.0</distortion_k2>
    <distortion_k3>0.0</distortion_k3>
    <distortion_t1>0.0</distortion_t1>
    <distortion_t2>0.0</distortion_t2>
    <pointCloudCutoff>0.5</pointCloudCutoff>
    <pointCloudCutoffMax>3.0</pointCloudCutoffMax>
    <CxPrime>0.0</CxPrime>
    <Cx>320.5</Cx>
    <Cy>240.5</Cy>
    <focalLength>320.0</focalLength>
  </plugin>
</sensor>
```

### RGB-D Sensor Fusion

Combine RGB and depth for enhanced perception:

```cpp
#include <sensor_msgs/msg/image.hpp>
#include <sensor_msgs/msg/camera_info.hpp>
#include <cv_bridge/cv_bridge.h>
#include <opencv2/opencv.hpp>

class PerceptionFusion {
public:
  void imageCallback(const sensor_msgs::msg::Image::SharedPtr rgb_msg,
                     const sensor_msgs::msg::Image::SharedPtr depth_msg) {

    // Convert ROS images to OpenCV
    cv_bridge::CvImagePtr rgb_cv = cv_bridge::toCvCopy(rgb_msg, "bgr8");
    cv_bridge::CvImagePtr depth_cv = cv_bridge::toCvCopy(depth_msg, "32FC1");

    // Create point cloud from RGB-D data
    createPointCloud(rgb_cv->image, depth_cv->image);
  }

private:
  void createPointCloud(const cv::Mat& rgb, const cv::Mat& depth) {
    // Implementation to create 3D point cloud from 2D images
    // Uses camera intrinsics and depth values
  }
};
```

## IMU Simulation

IMU sensors are critical for humanoid balance and state estimation:

```xml
<sensor name="imu_sensor" type="imu">
  <always_on>true</always_on>
  <update_rate>100</update_rate>
  <pose>0 0 0.2 0 0 0</pose>  <!-- Position in torso -->
  <imu>
    <angular_velocity>
      <x>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.001</stddev>  <!-- 1 mrad/s accuracy -->
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.0001</bias_stddev>
        </noise>
      </x>
      <y>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.001</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.0001</bias_stddev>
        </noise>
      </y>
      <z>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.001</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.0001</bias_stddev>
        </noise>
      </z>
    </angular_velocity>
    <linear_acceleration>
      <x>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.017</stddev>  <!-- 17 mg accuracy -->
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.0017</bias_stddev>
        </noise>
      </x>
      <y>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.017</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.0017</bias_stddev>
        </noise>
      </y>
      <z>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.017</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.0017</bias_stddev>
        </noise>
      </z>
    </linear_acceleration>
  </imu>
  <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">
    <ros>
      <namespace>/humanoid</namespace>
      <remapping>~/out:=imu/data</remapping>
    </ros>
    <initial_orientation_as_reference>false</initial_orientation_as_reference>
  </plugin>
</sensor>
```

### IMU-Based Balance Control

Use IMU data for humanoid balance:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from geometry_msgs.msg import Vector3
import numpy as np

class BalanceController(Node):
    def __init__(self):
        super().__init__('balance_controller')

        self.imu_sub = self.create_subscription(
            Imu,
            '/humanoid/imu/data',
            self.imu_callback,
            10
        )

        self.roll = 0.0
        self.pitch = 0.0
        self.yaw = 0.0

    def imu_callback(self, msg: Imu):
        # Extract orientation from quaternion
        q = msg.orientation
        self.roll, self.pitch, self.yaw = self.quaternion_to_euler([
            q.x, q.y, q.z, q.w
        ])

        # Extract angular velocities
        self.angular_vel = {
            'x': msg.angular_velocity.x,
            'y': msg.angular_velocity.y,
            'z': msg.angular_velocity.z
        }

        # Extract linear accelerations
        self.linear_acc = {
            'x': msg.linear_acceleration.x,
            'y': msg.linear_acceleration.y,
            'z': msg.linear_acceleration.z
        }

        # Check balance and trigger corrections
        self.check_balance()

    def check_balance(self):
        # Simple balance check
        if abs(self.roll) > 0.3 or abs(self.pitch) > 0.3:
            self.get_logger().warn('Balance error detected!')
            # Implement balance recovery here
            self.recover_balance()

    def recover_balance(self):
        # Send corrective joint commands based on IMU data
        pass

    def quaternion_to_euler(self, q):
        # Convert quaternion to Euler angles
        import math
        sinr_cosp = 2 * (q[3] * q[0] + q[1] * q[2])
        cosr_cosp = 1 - 2 * (q[0] * q[0] + q[1] * q[1])
        roll = math.atan2(sinr_cosp, cosr_cosp)

        sinp = 2 * (q[3] * q[1] - q[2] * q[0])
        pitch = math.asin(sinp)

        siny_cosp = 2 * (q[3] * q[2] + q[0] * q[1])
        cosy_cosp = 1 - 2 * (q[1] * q[1] + q[2] * q[2])
        yaw = math.atan2(siny_cosp, cosy_cosp)

        return roll, pitch, yaw
```

## Noise Modeling and Realism

Realistic sensor simulation requires proper noise modeling:

### LiDAR Noise Parameters

```xml
<sensor name="lidar_with_noise" type="ray">
  <!-- ... other configuration ... -->
  <noise type="gaussian">
    <mean>0.0</mean>
    <stddev>0.02</stddev>  <!-- 2cm standard deviation -->
    <bias_mean>0.01</bias_mean>  <!-- 1cm systematic bias -->
    <bias_stddev>0.005</bias_stddev>  <!-- 0.5cm bias drift -->
  </noise>
</sensor>
```

### Camera Noise Models

```xml
<sensor name="camera_with_noise" type="camera">
  <camera>
    <!-- ... camera configuration ... -->
    <noise>
      <type>gaussian</type>
      <mean>0.0</mean>
      <stddev>0.005</stddev>  <!-- 0.5% of pixel value -->
    </noise>
  </camera>
</sensor>
```

## Practical Exercise

1. Add LiDAR, depth camera, and IMU sensors to your humanoid model
2. Configure realistic noise parameters for each sensor
3. Create a ROS2 node to subscribe to sensor data
4. Visualize sensor data in RViz and verify realistic behavior

## Summary

Realistic sensor simulation is crucial for humanoid robot development. LiDAR provides 2D/3D environmental information, depth cameras enable 3D perception, and IMUs are essential for balance control. Proper noise modeling ensures that algorithms developed in simulation will transfer to real robots. The combination of these sensors enables humanoid robots to perceive and interact with their environment effectively.