[
  {
    "id": "book-summary",
    "title": "Autonomous Humanoid Robotics Book - Complete Structure",
    "content": "# Autonomous Humanoid Robotics Book - Complete Structure\n\n## Overview\nThis document provides a complete summary of the \"Autonomous Humanoid Robotics: From ROS 2 to Vision-Language-Action\" book structure.\n\n## Book Structure\n\n### Module 1: The Robotic Nervous System (ROS 2)\n- **1.1** - ROS 2 Architecture & DDS\n- **1.2** - Nodes, Topics, Services, and Actions\n- **1.3** - Python Agents with rclpy\n- **1.4** - Humanoid Modeling with URDF & Xacro\n\n### Module 2: The Digital Twin (Gazebo & Unity)\n- **2.1** - Gazebo Physics Simulation\n- **2.2** - Sensors in Simulation\n- **2.3** - Human-Robot Interaction (HRI)\n- **2.4** - Digital Twin Validation\n\n### Module 3: The AI-Robot Brain (NVIDIA Isaac)\n- **3.1** - Introduction to Whole Body Control\n- **3.2** - Kinematic Control and Inverse Kinematics\n- **3.3** - Dynamics and Force Control\n- **3.4** - Balance Control and Locomotion\n\n### Module 4: Vision-Language-Action (VLA)\n- **4.1** - Voice-to-Action Pipelines\n- **4.2** - Language Grounding for Robotics\n- **4.3** - LLM-Based Cognitive Planning\n- **4.4** - Capstone Project: Autonomous Humanoid\n\n## Total Structure\n- **Modules**: 4\n- **Chapters per module**: 4\n- **Total chapters**: 16\n- **Total pages**: ~1000+ (estimated)\n\n## Key Technologies Covered\n- ROS 2 (Robot Operating System 2)\n- Gazebo Simulation\n- Unity 3D\n- NVIDIA Isaac Sim\n- Python Robotics\n- Whole Body Control\n- Computer Vision\n- Natural Language Processing\n- Reinforcement Learning\n- Human-Robot Interaction\n\n## Learning Path\nThe book follows a progressive learning path from basic ROS 2 concepts through simulation environments, advanced control systems, and finally to AI-powered autonomous behaviors. Each module builds upon the previous one, culminating in a comprehensive capstone project that integrates all concepts.\n\n## Target Audience\n- Robotics engineers and researchers\n- AI and machine learning practitioners\n- Students studying robotics and AI\n- Developers working on autonomous systems\n- Anyone interested in humanoid robotics\n\nThis structure ensures comprehensive coverage of autonomous humanoid robotics from foundational concepts to advanced AI integration, with each chapter building systematically toward the final capstone project.",
    "module": "Introduction",
    "chapter": "book-summary",
    "path": "/docs/book-summary"
  },
  {
    "id": "digital-twins",
    "title": "Digital Twins: Gazebo and Unity",
    "content": "\n# Digital Twins: Gazebo and Unity\n\n## Introduction to Digital Twins\n\nA digital twin is a virtual representation of a physical system that simulates its behavior in real-time. In robotics, digital twins enable testing, validation, and development of robot systems in a safe, virtual environment before deployment on physical hardware.\n\n## Physics Simulation with Gazebo\n\nGazebo is a powerful physics simulation engine that provides realistic simulation of robotic systems. It offers:\n\n- Accurate physics simulation with multiple physics engines\n- High-quality rendering for realistic visualization\n- Support for various sensors (LiDAR, cameras, IMU, etc.)\n- Plugin architecture for custom functionality\n\n### Setting up a Gazebo Environment\n\nTo create a basic simulation environment in Gazebo:\n\n1. Create a world file in SDF (Simulation Description Format)\n2. Define the physics engine parameters\n3. Add models and objects to the environment\n4. Configure sensors and plugins\n\nExample world file:\n\n```xml\n<sdf version=\"1.7\">\n  <world name=\"my_world\">\n    <physics type=\"ode\">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n    </physics>\n\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    <include>\n      <uri>model://sun</uri>\n    </include>\n  </world>\n</sdf>\n```\n\n## Unity for High-Fidelity Visualization\n\nUnity provides advanced rendering capabilities for creating visually impressive digital twins. Key features include:\n\n- High-quality graphics and lighting\n- Realistic material rendering\n- Advanced particle systems\n- VR/AR support\n- Cross-platform deployment\n\n### Unity Integration with Robotics\n\nUnity Robotics provides tools and packages to bridge Unity with robotics frameworks:\n\n- ROS# for ROS/ROS2 communication\n- Unity ML-Agents for reinforcement learning\n- Physics simulation with Unity's engine\n- Sensor simulation plugins\n\n## Sensor Simulation\n\nDigital twins must accurately simulate various sensors to provide realistic training and testing environments.\n\n### LiDAR Simulation\n\nLiDAR sensors provide 2D or 3D distance measurements. In simulation:\n\n- Ray tracing for accurate distance measurements\n- Noise models to simulate real sensor behavior\n- Multiple beam configurations\n- Variable range and resolution settings\n\n### Camera Simulation\n\nCamera sensors provide visual information for perception tasks:\n\n- RGB, depth, and semantic segmentation cameras\n- Different lens types and distortions\n- Frame rate and resolution settings\n- Dynamic lighting conditions\n\n### IMU Simulation\n\nInertial Measurement Units provide acceleration and angular velocity:\n\n- Accurate physics integration\n- Noise and drift modeling\n- Multiple IMU placement options\n- Calibration simulation\n\n## Human-Robot Interaction (HRI)\n\nDigital twins enable safe testing of HRI scenarios:\n\n- Gesture recognition in virtual environments\n- Voice command simulation\n- Social robotics scenarios\n- Safety zone validation\n- User experience testing\n\n## Integration Patterns\n\n### ROS-Gazebo Integration\n\nThe classic integration pattern for robotics simulation:\n\n- Gazebo as the physics simulation backend\n- ROS/ROS2 for robot control and communication\n- Gazebo plugins for ROS communication\n- Standard ROS message types for sensor data\n\n### Unity-ROS Bridge\n\nModern approach using Unity as the visualization layer:\n\n- Unity for high-fidelity rendering\n- ROS/ROS2 for robot control\n- Network communication between Unity and ROS\n- Custom message types for Unity-specific data\n\n## Best Practices\n\n1. **Validation**: Compare simulation results with real-world data\n2. **Realism**: Include appropriate noise and uncertainty models\n3. **Performance**: Balance simulation accuracy with computational efficiency\n4. **Scalability**: Design environments that can handle multiple robots\n5. **Safety**: Use simulation for safety-critical system testing",
    "module": "Module 2: Digital Twin (Gazebo + Unity)",
    "chapter": "digital-twins",
    "path": "/docs/digital-twins"
  },
  {
    "id": "intro",
    "title": "Introduction to Embodied AI Systems",
    "content": "\n# Introduction to Embodied AI Systems\n\nWelcome to the **Embodied AI Systems Book**, where you'll learn about building intelligent robotic systems that interact with the physical world.\n\n## About This Book\n\nThis comprehensive guide covers two fundamental aspects of embodied AI:\n\n- **Module 1**: Robotic Nervous System (ROS 2) - Learn about ROS 2 Nodes, Topics, Services, rclpy agent-to-ROS bridge, and URDF basics for humanoid robots\n- **Module 2**: Digital Twin (Gazebo + Unity) - Explore physics simulation, Unity rendering, HRI, and sensor simulation including LiDAR, Camera, and IMU\n\n## Getting Started\n\nThis book is designed to take you from beginner to advanced concepts in embodied AI systems. Each module builds on the previous knowledge while providing hands-on examples and practical applications.\n\n## What You'll Learn\n\n- How to design and implement robotic control systems using ROS 2\n- Techniques for creating digital twins with realistic physics simulation\n- Integration patterns between AI agents and robotic systems\n- Sensor fusion and perception for embodied systems\n- Human-robot interaction principles and implementation\n\n## Prerequisites\n\n- Basic programming knowledge (Python preferred)\n- Understanding of linear algebra and basic physics concepts\n- Familiarity with Linux command line (helpful but not required)\n\nLet's begin your journey into the fascinating world of embodied AI systems!\n",
    "module": "Introduction",
    "chapter": "intro",
    "path": "/docs/intro"
  },
  {
    "id": "module-1/1.1-ros2-architecture-dds",
    "title": "Chapter 1.1 – ROS 2 Architecture & DDS",
    "content": "\n# Chapter 1.1 – ROS 2 Architecture & DDS\n\n## Learning Objectives\n- Understand the fundamental differences between ROS 1 and ROS 2\n- Explain the role of DDS in ROS 2 communication\n- Describe real-time and distributed systems concepts in robotics\n- Identify humanoid-specific constraints in ROS 2 architecture\n\n## ROS 2 vs ROS 1\n\nROS 2 represents a major architectural shift from ROS 1, addressing several critical limitations:\n\n- **Middleware Independence**: ROS 2 uses Data Distribution Service (DDS) as its underlying communication middleware\n- **Real-time Support**: Native support for real-time systems with deterministic behavior\n- **Multi-robot Systems**: Improved support for distributed robotics applications\n- **Security**: Built-in security features for authenticated and encrypted communication\n\n```\nROS 1 Architecture: Master-Slave\n  - Centralized ROS Master\n  - XML-RPC communication\n  - Single point of failure\n\nROS 2 Architecture: Distributed\n  - DDS-based communication\n  - Peer-to-peer discovery\n  - No central coordinator\n```\n\n## DDS Fundamentals\n\nData Distribution Service (DDS) is a middleware standard for real-time, distributed systems. In ROS 2, DDS provides:\n\n- **Data-Centricity**: Focus on data rather than communication endpoints\n- **Quality of Service (QoS)**: Configurable policies for reliability, durability, and latency\n- **Discovery**: Automatic discovery of participants in the network\n- **Type Safety**: Strong typing with IDL (Interface Definition Language)\n\n```mermaid\ngraph TD\n    A[ROS 2 Node] --> B[DDS Domain]\n    C[ROS 2 Node] --> B\n    D[ROS 2 Node] --> B\n    B --> E[DDS Implementation]\n    E --> F[DDS Global Data Space]\n    F --> G[Discovery Service]\n    F --> H[Reliability Service]\n    F --> I[Durability Service]\n```\n\n## Real-time and Distributed Systems\n\nFor humanoid robots, real-time constraints are critical for:\n\n- **Balance Control**: Maintaining stability with millisecond-level response times\n- **Sensor Fusion**: Processing multiple sensor streams with low latency\n- **Trajectory Execution**: Precise timing for joint control\n\nDDS QoS profiles enable real-time behavior through:\n\n```python\nfrom rclpy.qos import QoSProfile, QoSReliabilityPolicy, QoSHistoryPolicy\n\n# Real-time profile for sensor data\nsensor_qos = QoSProfile(\n    reliability=QoSReliabilityPolicy.RELIABLE,\n    history=QoSHistoryPolicy.KEEP_LAST,\n    depth=10\n)\n\n# Best-effort profile for visualization\nviz_qos = QoSProfile(\n    reliability=QoSReliabilityPolicy.BEST_EFFORT,\n    history=QoSHistoryPolicy.KEEP_LAST,\n    depth=1\n)\n```\n\n## Humanoid Robotics Constraints\n\nHumanoid robots present unique challenges in ROS 2 architecture:\n\n- **Computational Limits**: Limited processing power on-board\n- **Power Constraints**: Battery life considerations\n- **Latency Requirements**: Critical for balance and safety\n- **Network Topology**: Multiple computers distributed across the robot\n\n## Practical Exercise\n\n1. Create a simple ROS 2 publisher and subscriber with different QoS profiles\n2. Observe the discovery process using `ros2 topic list`\n3. Experiment with reliability and durability settings\n\n## Summary\n\nROS 2's DDS-based architecture provides the foundation for robust, distributed humanoid robot systems. The middleware independence allows for various DDS implementations optimized for different use cases, while QoS profiles enable fine-tuning for specific requirements of humanoid robotics applications.",
    "module": "Module 1: Robotic Nervous System (ROS 2)",
    "chapter": "ros2 architecture dds.md",
    "path": "/docs/module-1/1.1-ros2-architecture-dds"
  },
  {
    "id": "module-1/1.2-nodes-topics-services-actions",
    "title": "Chapter 1.2 – Nodes, Topics, Services, and Actions",
    "content": "\n# Chapter 1.2 – Nodes, Topics, Services, and Actions\n\n## Learning Objectives\n- Understand the lifecycle of ROS 2 nodes\n- Implement topic-based streaming for humanoid control\n- Distinguish between services and actions for different use cases\n- Design control loops optimized for humanoid robotics\n\n## Node Lifecycle\n\nROS 2 nodes follow a well-defined lifecycle that enables more robust system management:\n\n```mermaid\nstateDiagram-v2\n    [*] --> Unconfigured: create\n    Unconfigured --> Inactive: configure()\n    Inactive --> Active: activate()\n    Active --> Inactive: deactivate()\n    Inactive --> Unconfigured: cleanup()\n    Unconfigured --> Finalized: destroy()\n\n    Active --> Error: error\n    Error --> Unconfigured: reset\n    Error --> Finalized: destroy()\n```\n\nFor humanoid robots, the lifecycle pattern is particularly useful for:\n- **Safe startup**: Gradual activation of joint controllers\n- **Error recovery**: Isolated recovery of specific subsystems\n- **Power management**: Deactivating unused components\n\n```python\nimport rclpy\nfrom rclpy.lifecycle import LifecycleNode, LifecycleState, TransitionCallbackReturn\nfrom rclpy.lifecycle import Publisher\nfrom std_msgs.msg import String\n\nclass HumanoidControlNode(LifecycleNode):\n    def __init__(self):\n        super().__init__('humanoid_control_node')\n        self.pub: Publisher = None\n\n    def on_configure(self, state: LifecycleState) -> TransitionCallbackReturn:\n        self.pub = self.create_lifecycle_publisher(String, 'robot_status', 10)\n        self.get_logger().info('Configured')\n        return TransitionCallbackReturn.SUCCESS\n\n    def on_activate(self, state: LifecycleState) -> TransitionCallbackReturn:\n        self.pub.activate()\n        self.get_logger().info('Activated')\n        return TransitionCallbackReturn.SUCCESS\n\n    def on_deactivate(self, state: LifecycleState) -> TransitionCallbackReturn:\n        self.pub.deactivate()\n        self.get_logger().info('Deactivated')\n        return TransitionCallbackReturn.SUCCESS\n\n    def on_cleanup(self, state: LifecycleState) -> TransitionCallbackReturn:\n        self.destroy_publisher(self.pub)\n        self.pub = None\n        self.get_logger().info('Cleaned up')\n        return TransitionCallbackReturn.SUCCESS\n```\n\n## Topic-Based Streaming\n\nTopics enable asynchronous, publisher-subscriber communication ideal for:\n- **Sensor streams**: IMU, joint encoders, camera feeds\n- **Control commands**: Joint positions, velocities, torques\n- **Status updates**: Battery level, joint temperatures\n\nFor humanoid robots, consider these QoS settings:\n\n```python\nfrom rclpy.qos import QoSProfile, DurabilityPolicy, ReliabilityPolicy\n\n# High-frequency sensor data\nsensor_qos = QoSProfile(\n    depth=1,  # Only keep latest message\n    reliability=ReliabilityPolicy.BEST_EFFORT,\n    durability=DurabilityPolicy.VOLATILE\n)\n\n# Critical control commands\ncontrol_qos = QoSProfile(\n    depth=5,  # Keep some history\n    reliability=ReliabilityPolicy.RELIABLE,\n    durability=DurabilityPolicy.TRANSIENT_LOCAL\n)\n```\n\n## Services vs Actions\n\n### Services\n- **Request/Response pattern**\n- **Synchronous** (caller waits for response)\n- **Suitable for**: Simple queries, configuration changes\n- **Example**: Get joint position, set mode\n\n### Actions\n- **Goal/Result/Feedback pattern**\n- **Asynchronous** with progress updates\n- **Suitable for**: Long-running tasks, trajectories\n- **Example**: Execute walking gait, move to position\n\n```python\n# Service call example\nfrom example_interfaces.srv import SetBool\n\nclient = node.create_client(SetBool, 'enable_controller')\nrequest = SetBool.Request(data=True)\n\nfuture = client.call_async(request)\n# Node continues execution while waiting\n\n# Action example\nfrom example_interfaces.action import Fibonacci\nfrom rclpy.action import ActionClient\n\nclass TrajectoryClient:\n    def __init__(self, node):\n        self._action_client = ActionClient(node, Fibonacci, 'execute_trajectory')\n\n    def send_goal(self, order):\n        goal_msg = Fibonacci.Goal(order=order)\n        self._action_client.wait_for_server()\n        return self._action_client.send_goal_async(goal_msg, feedback_callback=self.feedback_callback)\n\n    def feedback_callback(self, feedback_msg):\n        self.get_logger().info(f'Received feedback: {feedback_msg.feedback.sequence}')\n```\n\n## Control Loops for Humanoids\n\nHumanoid robots require multiple control loops running at different frequencies:\n\n- **High-frequency (1-10kHz)**: Joint position/velocity control\n- **Mid-frequency (100-500Hz)**: Balance control, inverse kinematics\n- **Low-frequency (10-50Hz)**: Walking pattern generation, path planning\n\n```python\nclass BalanceController(LifecycleNode):\n    def __init__(self):\n        super().__init__('balance_controller')\n        self.control_timer = self.create_timer(\n            0.002,  # 500Hz for balance control\n            self.balance_control_callback\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, 'imu/data', self.imu_callback, 1\n        )\n        self.joint_pub = self.create_publisher(\n            JointState, 'joint_commands', 1\n        )\n\n    def balance_control_callback(self):\n        # Implement balance control algorithm\n        # Calculate necessary joint adjustments\n        # Publish commands\n        pass\n```\n\n## Practical Exercise\n\n1. Create a lifecycle node that controls a simulated humanoid joint\n2. Implement both service and action interfaces for joint control\n3. Compare performance of different QoS profiles for sensor data\n\n## Summary\n\nThe ROS 2 communication patterns provide the foundation for robust humanoid robot control. Nodes manage lifecycle states for safe operation, topics enable high-frequency streaming, services handle synchronous requests, and actions manage long-running tasks with feedback. Understanding these patterns is crucial for designing efficient humanoid robot architectures.",
    "module": "Module 1: Robotic Nervous System (ROS 2)",
    "chapter": "nodes topics services actions.md",
    "path": "/docs/module-1/1.2-nodes-topics-services-actions"
  },
  {
    "id": "module-1/1.3-python-agents-rclpy",
    "title": "Chapter 1.3 – Python Agents with rclpy",
    "content": "\n# Chapter 1.3 – Python Agents with rclpy\n\n## Learning Objectives\n- Create ROS 2 nodes using the rclpy library\n- Implement publishing and subscribing patterns for robot control\n- Develop action servers and clients for humanoid tasks\n- Bridge AI logic with robot controllers using rclpy\n\n## rclpy Node Creation\n\nThe `rclpy` library provides Python bindings for ROS 2. Here's the basic structure of a humanoid control agent:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.msg import JointTrajectoryControllerState\nimport time\n\nclass HumanoidAgent(Node):\n    def __init__(self):\n        super().__init__('humanoid_agent')\n\n        # Publishers for sending commands\n        self.joint_cmd_pub = self.create_publisher(\n            JointState,\n            '/joint_commands',\n            10\n        )\n\n        # Subscribers for receiving sensor data\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        # Timer for control loop\n        self.control_timer = self.create_timer(\n            0.01,  # 100Hz control loop\n            self.control_loop\n        )\n\n        self.get_logger().info('Humanoid Agent initialized')\n\n    def joint_state_callback(self, msg: JointState):\n        \"\"\"Process incoming joint state messages\"\"\"\n        self.get_logger().info(f'Received {len(msg.position)} joint positions')\n        # Process sensor data for control decisions\n\n    def control_loop(self):\n        \"\"\"Main control loop for humanoid agent\"\"\"\n        # Implement control logic here\n        self.get_logger().debug('Control loop executing')\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    agent = HumanoidAgent()\n\n    try:\n        rclpy.spin(agent)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        agent.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Publishing and Subscribing\n\n### Publishing Joint Commands\n\nFor humanoid robots, publishing joint commands requires careful attention to timing and safety:\n\n```python\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom builtin_interfaces.msg import Duration\n\nclass JointCommander(Node):\n    def __init__(self):\n        super().__init__('joint_commander')\n        self.traj_pub = self.create_publisher(\n            JointTrajectory,\n            '/joint_trajectory_controller/joint_trajectory',\n            10\n        )\n\n    def send_joint_trajectory(self, joint_names, positions, velocities=None, duration=1.0):\n        \"\"\"Send a joint trajectory command\"\"\"\n        traj_msg = JointTrajectory()\n        traj_msg.joint_names = joint_names\n\n        point = JointTrajectoryPoint()\n        point.positions = positions\n        point.velocities = velocities if velocities else [0.0] * len(positions)\n        point.time_from_start = Duration(sec=int(duration), nanosec=(duration % 1) * 1e9)\n\n        traj_msg.points = [point]\n\n        self.traj_pub.publish(traj_msg)\n        self.get_logger().info(f'Sent trajectory for joints: {joint_names}')\n\n# Example usage for humanoid joints\ndef move_to_home_position(self):\n    joint_names = [\n        'left_hip_joint', 'left_knee_joint', 'left_ankle_joint',\n        'right_hip_joint', 'right_knee_joint', 'right_ankle_joint',\n        'left_shoulder_joint', 'left_elbow_joint',\n        'right_shoulder_joint', 'right_elbow_joint'\n    ]\n\n    home_positions = [0.0] * len(joint_names)  # All joints to zero position\n    self.send_joint_trajectory(joint_names, home_positions, duration=2.0)\n```\n\n### Subscribing to Sensor Data\n\nHumanoid robots require multiple sensor streams with appropriate QoS settings:\n\n```python\nfrom sensor_msgs.msg import Imu, JointState\nfrom geometry_msgs.msg import Twist\n\nclass SensorProcessor(Node):\n    def __init__(self):\n        super().__init__('sensor_processor')\n\n        # IMU for balance control (high frequency, best effort)\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            1,  # Keep only latest\n            # Use best effort for performance\n            qos_profile=rclpy.qos.qos_profile_sensor_data\n        )\n\n        # Joint states (reliable, with history)\n        self.joint_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_callback,\n            10  # Keep 10 messages in history\n        )\n\n        # Store latest sensor data\n        self.latest_imu = None\n        self.latest_joints = None\n\n    def imu_callback(self, msg: Imu):\n        \"\"\"Process IMU data for balance control\"\"\"\n        self.latest_imu = {\n            'orientation': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w],\n            'angular_velocity': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],\n            'linear_acceleration': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]\n        }\n\n        # Trigger balance control if needed\n        self.check_balance()\n\n    def joint_callback(self, msg: JointState):\n        \"\"\"Process joint state data\"\"\"\n        self.latest_joints = dict(zip(msg.name, msg.position))\n\n        # Check for joint limits or errors\n        self.validate_joints()\n\n    def check_balance(self):\n        \"\"\"Simple balance check based on IMU data\"\"\"\n        if self.latest_imu:\n            roll, pitch, yaw = self.quaternion_to_euler(self.latest_imu['orientation'])\n            if abs(roll) > 0.5 or abs(pitch) > 0.5:  # Threshold for balance error\n                self.get_logger().warn('Balance error detected!')\n                # Trigger recovery behavior\n\n    def quaternion_to_euler(self, q):\n        \"\"\"Convert quaternion to Euler angles (simplified)\"\"\"\n        import math\n        # Simplified conversion - in practice, use tf_transformations or similar\n        sinr_cosp = 2 * (q[3] * q[0] + q[1] * q[2])\n        cosr_cosp = 1 - 2 * (q[0] * q[0] + q[1] * q[1])\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (q[3] * q[1] - q[2] * q[0])\n        pitch = math.asin(sinp)\n\n        siny_cosp = 2 * (q[3] * q[2] + q[0] * q[1])\n        cosy_cosp = 1 - 2 * (q[1] * q[1] + q[2] * q[2])\n        yaw = math.atan2(siny_cosp, cosy_cosp)\n\n        return roll, pitch, yaw",
    "module": "Module 1: Robotic Nervous System (ROS 2)",
    "chapter": "python agents rclpy.md",
    "path": "/docs/module-1/1.3-python-agents-rclpy"
  },
  {
    "id": "module-1/1.4-humanoid-modeling-urdf-xacro",
    "title": "Chapter 1.4: Humanoid Modeling with URDF & Xacro",
    "content": "\n# Chapter 1.4: Humanoid Modeling with URDF & Xacro\n\n## Learning Objectives\n- Create detailed humanoid robot models using URDF\n- Implement parametric designs with Xacro macros\n- Add sensors and actuators to humanoid models\n- Validate kinematic chains and joint limits\n\n## URDF Fundamentals for Humanoids\n\nUnified Robot Description Format (URDF) is the standard for describing robot models in ROS. For humanoid robots, the complexity increases due to multiple degrees of freedom and anthropomorphic structure:\n\n```xml\n<?xml version=\"1.0\"?>\n<robot name=\"humanoid_robot\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n  <!-- Material definitions -->\n  <material name=\"black\">\n    <color rgba=\"0.0 0.0 0.0 1.0\"/>\n  </material>\n  <material name=\"blue\">\n    <color rgba=\"0.0 0.0 0.8 1.0\"/>\n  </material>\n  <material name=\"green\">\n    <color rgba=\"0.0 0.8 0.0 1.0\"/>\n  </material>\n  <material name=\"grey\">\n    <color rgba=\"0.5 0.5 0.5 1.0\"/>\n  </material>\n  <material name=\"orange\">\n    <color rgba=\"1.0 0.423529411765 0.0392156862745 1.0\"/>\n  </material>\n  <material name=\"brown\">\n    <color rgba=\"0.870588235294 0.811764705882 0.764705882353 1.0\"/>\n  </material>\n  <material name=\"red\">\n    <color rgba=\"0.8 0.0 0.0 1.0\"/>\n  </material>\n  <material name=\"white\">\n    <color rgba=\"1.0 1.0 1.0 1.0\"/>\n  </material>\n\n  <!-- Base link - pelvis -->\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.3 0.2 0.15\"/>\n      </geometry>\n      <material name=\"grey\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.3 0.2 0.15\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"10.0\"/>\n      <inertia ixx=\"0.1\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.2\" iyz=\"0.0\" izz=\"0.15\"/>\n    </inertial>\n  </link>\n\n  <!-- Torso -->\n  <joint name=\"torso_joint\" type=\"fixed\">\n    <parent link=\"base_link\"/>\n    <child link=\"torso_link\"/>\n    <origin xyz=\"0 0 0.1\" rpy=\"0 0 0\"/>\n  </joint>\n\n  <link name=\"torso_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.25 0.15 0.4\"/>\n      </geometry>\n      <material name=\"orange\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.25 0.15 0.4\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"8.0\"/>\n      <inertia ixx=\"0.2\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.3\" iyz=\"0.0\" izz=\"0.25\"/>\n    </inertial>\n  </link>\n\n  <!-- Head -->\n  <joint name=\"neck_joint\" type=\"revolute\">\n    <parent link=\"torso_link\"/>\n    <child link=\"head_link\"/>\n    <origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-0.5\" upper=\"0.5\" effort=\"10.0\" velocity=\"1.0\"/>\n  </joint>\n\n  <link name=\"head_link\">\n    <visual>\n      <geometry>\n        <sphere radius=\"0.1\"/>\n      </geometry>\n      <material name=\"skin\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <sphere radius=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"2.0\"/>\n      <inertia ixx=\"0.02\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.02\" iyz=\"0.0\" izz=\"0.02\"/>\n    </inertial>\n  </link>\n\n  <!-- Left Leg -->\n  <joint name=\"left_hip_joint\" type=\"revolute\">\n    <parent link=\"base_link\"/>\n    <child link=\"left_hip_link\"/>\n    <origin xyz=\"-0.1 0 -0.05\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 0 1\"/>\n    <limit lower=\"-1.0\" upper=\"1.0\" effort=\"50.0\" velocity=\"2.0\"/>\n  </joint>\n\n  <link name=\"left_hip_link\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.05\" length=\"0.1\"/>\n      </geometry>\n      <material name=\"blue\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius=\"0.05\" length=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1.5\"/>\n      <inertia ixx=\"0.005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.005\" iyz=\"0.0\" izz=\"0.0025\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"left_knee_joint\" type=\"revolute\">\n    <parent link=\"left_hip_link\"/>\n    <child link=\"left_knee_link\"/>\n    <origin xyz=\"0 0 -0.2\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"0\" upper=\"2.5\" effort=\"50.0\" velocity=\"2.0\"/>\n  </joint>\n\n  <link name=\"left_knee_link\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.05\" length=\"0.3\"/>\n      </geometry>\n      <material name=\"blue\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius=\"0.05\" length=\"0.3\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"2.0\"/>\n      <inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.01\" iyz=\"0.0\" izz=\"0.005\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"left_ankle_joint\" type=\"revolute\">\n    <parent link=\"left_knee_link\"/>\n    <child link=\"left_foot_link\"/>\n    <origin xyz=\"0 0 -0.3\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 0 1\"/>\n    <limit lower=\"-0.5\" upper=\"0.5\" effort=\"20.0\" velocity=\"1.0\"/>\n  </joint>\n\n  <link name=\"left_foot_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.2 0.1 0.05\"/>\n      </geometry>\n      <material name=\"black\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.2 0.1 0.05\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1.0\"/>\n      <inertia ixx=\"0.001\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.002\" iyz=\"0.0\" izz=\"0.0015\"/>\n    </inertial>\n  </link>\n\n  <!-- Right Leg (mirror of left leg) -->\n  <joint name=\"right_hip_joint\" type=\"revolute\">\n    <parent link=\"base_link\"/>\n    <child link=\"right_hip_link\"/>\n    <origin xyz=\"0.1 0 -0.05\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 0 1\"/>\n    <limit lower=\"-1.0\" upper=\"1.0\" effort=\"50.0\" velocity=\"2.0\"/>\n  </joint>\n\n  <link name=\"right_hip_link\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.05\" length=\"0.1\"/>\n      </geometry>\n      <material name=\"blue\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius=\"0.05\" length=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1.5\"/>\n      <inertia ixx=\"0.005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.005\" iyz=\"0.0\" izz=\"0.0025\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"right_knee_joint\" type=\"revolute\">\n    <parent link=\"right_hip_link\"/>\n    <child link=\"right_knee_link\"/>\n    <origin xyz=\"0 0 -0.2\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"0\" upper=\"2.5\" effort=\"50.0\" velocity=\"2.0\"/>\n  </joint>\n\n  <link name=\"right_knee_link\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.05\" length=\"0.3\"/>\n      </geometry>\n      <material name=\"blue\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius=\"0.05\" length=\"0.3\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"2.0\"/>\n      <inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.01\" iyz=\"0.0\" izz=\"0.005\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"right_ankle_joint\" type=\"revolute\">\n    <parent link=\"right_knee_link\"/>\n    <child link=\"right_foot_link\"/>\n    <origin xyz=\"0 0 -0.3\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 0 1\"/>\n    <limit lower=\"-0.5\" upper=\"0.5\" effort=\"20.0\" velocity=\"1.0\"/>\n  </joint>\n\n  <link name=\"right_foot_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.2 0.1 0.05\"/>\n      </geometry>\n      <material name=\"black\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.2 0.1 0.05\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1.0\"/>\n      <inertia ixx=\"0.001\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.002\" iyz=\"0.0\" izz=\"0.0015\"/>\n    </inertial>\n  </link>\n\n  <!-- Left Arm -->\n  <joint name=\"left_shoulder_joint\" type=\"revolute\">\n    <parent link=\"torso_link\"/>\n    <child link=\"left_shoulder_link\"/>\n    <origin xyz=\"-0.15 0 0.1\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-1.57\" upper=\"1.57\" effort=\"20.0\" velocity=\"1.5\"/>\n  </joint>\n\n  <link name=\"left_shoulder_link\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.04\" length=\"0.1\"/>\n      </geometry>\n      <material name=\"green\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius=\"0.04\" length=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1.0\"/>\n      <inertia ixx=\"0.002\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.002\" iyz=\"0.0\" izz=\"0.001\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"left_elbow_joint\" type=\"revolute\">\n    <parent link=\"left_shoulder_link\"/>\n    <child link=\"left_elbow_link\"/>\n    <origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n    <axis xyz=\"1 0 0\"/>\n    <limit lower=\"-2.0\" upper=\"2.0\" effort=\"20.0\" velocity=\"1.5\"/>\n  </joint>\n\n  <link name=\"left_elbow_link\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.03\" length=\"0.25\"/>\n      </geometry>\n      <material name=\"green\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius=\"0.03\" length=\"0.25\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1.5\"/>\n      <inertia ixx=\"0.005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.005\" iyz=\"0.0\" izz=\"0.001\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"left_wrist_joint\" type=\"revolute\">\n    <parent link=\"left_elbow_link\"/>\n    <child link=\"left_hand_link\"/>\n    <origin xyz=\"0 0 -0.25\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-1.57\" upper=\"1.57\" effort=\"10.0\" velocity=\"1.0\"/>\n  </joint>\n\n  <link name=\"left_hand_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.1 0.08 0.08\"/>\n      </geometry>\n      <material name=\"skin\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.1 0.08 0.08\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"0.5\"/>\n      <inertia ixx=\"0.0005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.0006\" iyz=\"0.0\" izz=\"0.0006\"/>\n    </inertial>\n  </link>\n\n  <!-- Right Arm (mirror of left arm) -->\n  <joint name=\"right_shoulder_joint\" type=\"revolute\">\n    <parent link=\"torso_link\"/>\n    <child link=\"right_shoulder_link\"/>\n    <origin xyz=\"0.15 0 0.1\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-1.57\" upper=\"1.57\" effort=\"20.0\" velocity=\"1.5\"/>\n  </joint>\n\n  <link name=\"right_shoulder_link\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.04\" length=\"0.1\"/>\n      </geometry>\n      <material name=\"green\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius=\"0.04\" length=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1.0\"/>\n      <inertia ixx=\"0.002\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.002\" iyz=\"0.0\" izz=\"0.001\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"right_elbow_joint\" type=\"revolute\">\n    <parent link=\"right_shoulder_link\"/>\n    <child link=\"right_elbow_link\"/>\n    <origin xyz=\"0 0 -0.15\" rpy=\"0 0 0\"/>\n    <axis xyz=\"1 0 0\"/>\n    <limit lower=\"-2.0\" upper=\"2.0\" effort=\"20.0\" velocity=\"1.5\"/>\n  </joint>\n\n  <link name=\"right_elbow_link\">\n    <visual>\n      <geometry>\n        <cylinder radius=\"0.03\" length=\"0.25\"/>\n      </geometry>\n      <material name=\"green\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius=\"0.03\" length=\"0.25\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"1.5\"/>\n      <inertia ixx=\"0.005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.005\" iyz=\"0.0\" izz=\"0.001\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"right_wrist_joint\" type=\"revolute\">\n    <parent link=\"right_elbow_link\"/>\n    <child link=\"right_hand_link\"/>\n    <origin xyz=\"0 0 -0.25\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-1.57\" upper=\"1.57\" effort=\"10.0\" velocity=\"1.0\"/>\n  </joint>\n\n  <link name=\"right_hand_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.1 0.08 0.08\"/>\n      </geometry>\n      <material name=\"skin\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.1 0.08 0.08\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"0.5\"/>\n      <inertia ixx=\"0.0005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.0006\" iyz=\"0.0\" izz=\"0.0006\"/>\n    </inertial>\n  </link>\n</robot>\n```\n\n## Xacro Macros for Parametric Design\n\nXacro allows for parameterized robot models that can be easily customized:\n\n```xml\n<?xml version=\"1.0\"?>\n<robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" name=\"humanoid_xacro\">\n\n  <!-- Properties -->\n  <xacro:property name=\"M_PI\" value=\"3.1415926535897931\" />\n  <xacro:property name=\"body_height\" value=\"0.8\" />\n  <xacro:property name=\"body_width\" value=\"0.25\" />\n  <xacro:property name=\"body_depth\" value=\"0.15\" />\n  <xacro:property name=\"leg_length\" value=\"0.7\" />\n  <xacro:property name=\"arm_length\" value=\"0.5\" />\n\n  <!-- Macro for humanoid link with standardized properties -->\n  <xacro:macro name=\"humanoid_link\" params=\"name mass visual_geometry collision_geometry *inertial_values\">\n    <link name=\"${name}\">\n      <visual>\n        <geometry>\n          <xacro:insert_block name=\"visual_geometry\"/>\n        </geometry>\n        <material name=\"grey\"/>\n      </visual>\n      <collision>\n        <geometry>\n          <xacro:insert_block name=\"collision_geometry\"/>\n        </geometry>\n      </collision>\n      <xacro:insert_block name=\"inertial_values\"/>\n    </link>\n  </xacro:macro>\n\n  <!-- Macro for humanoid joint -->\n  <xacro:macro name=\"humanoid_joint\" params=\"name type parent child origin_xyz origin_rpy axis_xyz lower upper effort velocity\">\n    <joint name=\"${name}\" type=\"${type}\">\n      <parent link=\"${parent}\"/>\n      <child link=\"${child}\"/>\n      <origin xyz=\"${origin_xyz}\" rpy=\"${origin_rpy}\"/>\n      <xacro:if value=\"${type != 'fixed'}\">\n        <axis xyz=\"${axis_xyz}\"/>\n        <limit lower=\"${lower}\" upper=\"${upper}\" effort=\"${effort}\" velocity=\"${velocity}\"/>\n      </xacro:if>\n    </joint>\n  </xacro:macro>\n\n  <!-- Macro for humanoid leg -->\n  <xacro:macro name=\"humanoid_leg\" params=\"side hip_xyz knee_xyz ankle_xyz\">\n    <xacro:humanoid_joint name=\"${side}_hip_joint\" type=\"revolute\"\n                          parent=\"base_link\" child=\"${side}_hip_link\"\n                          origin_xyz=\"${hip_xyz}\" origin_rpy=\"0 0 0\"\n                          axis_xyz=\"0 0 1\" lower=\"-1.0\" upper=\"1.0\" effort=\"50.0\" velocity=\"2.0\"/>\n\n    <xacro:humanoid_link name=\"${side}_hip_link\" mass=\"1.5\">\n      <visual_geometry>\n        <cylinder radius=\"0.05\" length=\"0.1\"/>\n      </visual_geometry>\n      <collision_geometry>\n        <cylinder radius=\"0.05\" length=\"0.1\"/>\n      </collision_geometry>\n      <inertial_values>\n        <inertial>\n          <mass value=\"1.5\"/>\n          <inertia ixx=\"0.005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.005\" iyz=\"0.0\" izz=\"0.0025\"/>\n        </inertial>\n      </inertial_values>\n    </xacro:humanoid_link>\n\n    <xacro:humanoid_joint name=\"${side}_knee_joint\" type=\"revolute\"\n                          parent=\"${side}_hip_link\" child=\"${side}_knee_link\"\n                          origin_xyz=\"${knee_xyz}\" origin_rpy=\"0 0 0\"\n                          axis_xyz=\"0 1 0\" lower=\"0\" upper=\"2.5\" effort=\"50.0\" velocity=\"2.0\"/>\n\n    <xacro:humanoid_link name=\"${side}_knee_link\" mass=\"2.0\">\n      <visual_geometry>\n        <cylinder radius=\"0.05\" length=\"0.3\"/>\n      </visual_geometry>\n      <collision_geometry>\n        <cylinder radius=\"0.05\" length=\"0.3\"/>\n      </collision_geometry>\n      <inertial_values>\n        <inertial>\n          <mass value=\"2.0\"/>\n          <inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.01\" iyz=\"0.0\" izz=\"0.005\"/>\n        </inertial>\n      </inertial_values>\n    </xacro:humanoid_link>\n\n    <xacro:humanoid_joint name=\"${side}_ankle_joint\" type=\"revolute\"\n                          parent=\"${side}_knee_link\" child=\"${side}_foot_link\"\n                          origin_xyz=\"${ankle_xyz}\" origin_rpy=\"0 0 0\"\n                          axis_xyz=\"0 0 1\" lower=\"-0.5\" upper=\"0.5\" effort=\"20.0\" velocity=\"1.0\"/>\n\n    <xacro:humanoid_link name=\"${side}_foot_link\" mass=\"1.0\">\n      <visual_geometry>\n        <box size=\"0.2 0.1 0.05\"/>\n      </visual_geometry>\n      <collision_geometry>\n        <box size=\"0.2 0.1 0.05\"/>\n      </collision_geometry>\n      <inertial_values>\n        <inertial>\n          <mass value=\"1.0\"/>\n          <inertia ixx=\"0.001\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.002\" iyz=\"0.0\" izz=\"0.0015\"/>\n        </inertial>\n      </inertial_values>\n    </xacro:humanoid_link>\n  </xacro:macro>\n\n  <!-- Macro for humanoid arm -->\n  <xacro:macro name=\"humanoid_arm\" params=\"side shoulder_xyz elbow_xyz wrist_xyz\">\n    <xacro:humanoid_joint name=\"${side}_shoulder_joint\" type=\"revolute\"\n                          parent=\"torso_link\" child=\"${side}_shoulder_link\"\n                          origin_xyz=\"${shoulder_xyz}\" origin_rpy=\"0 0 0\"\n                          axis_xyz=\"0 1 0\" lower=\"-1.57\" upper=\"1.57\" effort=\"20.0\" velocity=\"1.5\"/>\n\n    <xacro:humanoid_link name=\"${side}_shoulder_link\" mass=\"1.0\">\n      <visual_geometry>\n        <cylinder radius=\"0.04\" length=\"0.1\"/>\n      </visual_geometry>\n      <collision_geometry>\n        <cylinder radius=\"0.04\" length=\"0.1\"/>\n      </collision_geometry>\n      <inertial_values>\n        <inertial>\n          <mass value=\"1.0\"/>\n          <inertia ixx=\"0.002\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.002\" iyz=\"0.0\" izz=\"0.001\"/>\n        </inertial>\n      </inertial_values>\n    </xacro:humanoid_link>\n\n    <xacro:humanoid_joint name=\"${side}_elbow_joint\" type=\"revolute\"\n                          parent=\"${side}_shoulder_link\" child=\"${side}_elbow_link\"\n                          origin_xyz=\"${elbow_xyz}\" origin_rpy=\"0 0 0\"\n                          axis_xyz=\"1 0 0\" lower=\"-2.0\" upper=\"2.0\" effort=\"20.0\" velocity=\"1.5\"/>\n\n    <xacro:humanoid_link name=\"${side}_elbow_link\" mass=\"1.5\">\n      <visual_geometry>\n        <cylinder radius=\"0.03\" length=\"0.25\"/>\n      </visual_geometry>\n      <collision_geometry>\n        <cylinder radius=\"0.03\" length=\"0.25\"/>\n      </collision_geometry>\n      <inertial_values>\n        <inertial>\n          <mass value=\"1.5\"/>\n          <inertia ixx=\"0.005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.005\" iyz=\"0.0\" izz=\"0.001\"/>\n        </inertial>\n      </inertial_values>\n    </xacro:humanoid_link>\n\n    <xacro:humanoid_joint name=\"${side}_wrist_joint\" type=\"revolute\"\n                          parent=\"${side}_elbow_link\" child=\"${side}_hand_link\"\n                          origin_xyz=\"${wrist_xyz}\" origin_rpy=\"0 0 0\"\n                          axis_xyz=\"0 1 0\" lower=\"-1.57\" upper=\"1.57\" effort=\"10.0\" velocity=\"1.0\"/>\n\n    <xacro:humanoid_link name=\"${side}_hand_link\" mass=\"0.5\">\n      <visual_geometry>\n        <box size=\"0.1 0.08 0.08\"/>\n      </visual_geometry>\n      <collision_geometry>\n        <box size=\"0.1 0.08 0.08\"/>\n      </collision_geometry>\n      <inertial_values>\n        <inertial>\n          <mass value=\"0.5\"/>\n          <inertia ixx=\"0.0005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.0006\" iyz=\"0.0\" izz=\"0.0006\"/>\n        </inertial>\n      </inertial_values>\n    </xacro:humanoid_link>\n  </xacro:macro>\n\n  <!-- Base structure -->\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <box size=\"${body_width} 0.2 ${body_depth}\"/>\n      </geometry>\n      <material name=\"grey\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"${body_width} 0.2 ${body_depth}\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"10.0\"/>\n      <inertia ixx=\"0.1\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.2\" iyz=\"0.0\" izz=\"0.15\"/>\n    </inertial>\n  </link>\n\n  <xacro:humanoid_joint name=\"torso_joint\" type=\"fixed\"\n                        parent=\"base_link\" child=\"torso_link\"\n                        origin_xyz=\"0 0 0.1\" origin_rpy=\"0 0 0\"\n                        axis_xyz=\"0 0 0\" lower=\"0\" upper=\"0\" effort=\"0\" velocity=\"0\"/>\n\n  <link name=\"torso_link\">\n    <visual>\n      <geometry>\n        <box size=\"${body_width-0.05} 0.15 0.4\"/>\n      </geometry>\n      <material name=\"orange\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"${body_width-0.05} 0.15 0.4\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"8.0\"/>\n      <inertia ixx=\"0.2\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.3\" iyz=\"0.0\" izz=\"0.25\"/>\n    </inertial>\n  </link>\n\n  <!-- Use macros to create limbs -->\n  <xacro:humanoid_leg side=\"left\" hip_xyz=\"-0.1 0 -0.05\" knee_xyz=\"0 0 -0.2\" ankle_xyz=\"0 0 -0.3\"/>\n  <xacro:humanoid_leg side=\"right\" hip_xyz=\"0.1 0 -0.05\" knee_xyz=\"0 0 -0.2\" ankle_xyz=\"0 0 -0.3\"/>\n  <xacro:humanoid_arm side=\"left\" shoulder_xyz=\"-0.15 0 0.1\" elbow_xyz=\"0 0 -0.15\" wrist_xyz=\"0 0 -0.25\"/>\n  <xacro:humanoid_arm side=\"right\" shoulder_xyz=\"0.15 0 0.1\" elbow_xyz=\"0 0 -0.15\" wrist_xyz=\"0 0 -0.25\"/>\n\n</robot>\n```\n\n## Adding Sensors to Humanoid Models\n\nFor perception capabilities, sensors must be properly integrated into the URDF:\n\n```xml\n<!-- IMU Sensor in head -->\n<gazebo reference=\"head_link\">\n  <sensor name=\"imu_sensor\" type=\"imu\">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0000075</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0000075</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0000075</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name=\"imu_plugin\" filename=\"libgazebo_ros_imu_sensor.so\">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>~/out:=imu/data</remapping>\n      </ros>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n    </plugin>\n  </sensor>\n</gazebo>\n\n<!-- RGB-D Camera in head -->\n<gazebo reference=\"head_link\">\n  <sensor name=\"rgbd_camera\" type=\"depth\">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <pose>0.1 0 0.05 0 0 0</pose>\n    <camera name=\"head_camera\">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n    </camera>\n    <plugin name=\"camera_controller\" filename=\"libgazebo_ros_openni_kinect.so\">\n      <alwaysOn>true</alwaysOn>\n      <updateRate>30.0</updateRate>\n      <cameraName>head_camera</cameraName>\n      <imageTopicName>rgb/image_raw</imageTopicName>\n      <depthImageTopicName>depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>\n      <frameName>head_camera_depth_optical_frame</frameName>\n      <baseline>0.1</baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n      <pointCloudCutoff>0.5</pointCloudCutoff>\n      <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n      <CxPrime>0.0</CxPrime>\n      <Cx>320.5</Cx>\n      <Cy>240.5</Cy>\n      <focalLength>320.0</focalLength>\n    </plugin>\n  </sensor>\n</gazebo>\n\n<!-- LiDAR Sensor in torso -->\n<gazebo reference=\"torso_link\">\n  <sensor name=\"lidar\" type=\"ray\">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <pose>0.0 0.0 0.2 0 0 0</pose>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>360</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>10.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name=\"lidar_controller\" filename=\"libgazebo_ros_ray_sensor.so\">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n```\n\n## Validation and Testing\n\nProper validation of humanoid models is crucial:\n\n```bash\n# Validate URDF\ncheck_urdf /path/to/humanoid.urdf\n\n# Visualize in RViz\nros2 run rviz2 rviz2\n\n# Test kinematics\nros2 run moveit_ros_planners_ompl ompl_planning_pipeline_demo\n\n# Check joint limits\nros2 launch moveit_configs_utils moveit_rviz.launch.py\n```\n\n## Practical Exercise\n\n1. Create a humanoid model with 28+ degrees of freedom\n2. Add sensors (IMU, camera, LiDAR) to the model\n3. Validate the kinematic chain\n4. Test joint limits and range of motion\n\n## Summary\n\nURDF and Xacro are fundamental tools for creating humanoid robot models. Proper modeling requires attention to kinematic chains, inertial properties, joint limits, and sensor integration. Xacro macros enable parametric design for easier customization and maintenance of complex humanoid models.",
    "module": "Module 1: Robotic Nervous System (ROS 2)",
    "chapter": "humanoid modeling urdf xacro.md",
    "path": "/docs/module-1/1.4-humanoid-modeling-urdf-xacro"
  },
  {
    "id": "module-2/2.1-gazebo-physics-simulation",
    "title": "Chapter 2.1: Gazebo Physics Simulation",
    "content": "\n# Chapter 2.1: Gazebo Physics Simulation\n\n## Learning Objectives\n- Configure realistic physics for humanoid robots\n- Implement sensor simulation with proper noise models\n- Set up terrain and environment simulation\n- Validate simulation accuracy with real-world comparisons\n\n## Physics Engine Configuration\n\nGazebo provides multiple physics engines optimized for different simulation scenarios. For humanoid robots, we need to configure the physics engine to accurately simulate bipedal locomotion:\n\n```xml\n<!-- gazebo_world.physics -->\n<sdf version=\"1.7\">\n  <world name=\"humanoid_world\">\n    <!-- Physics engine configuration -->\n    <physics type=\"ode\" name=\"default_physics\">\n      <max_step_size>0.001</max_step_size>  <!-- 1ms time step for accuracy -->\n      <real_time_factor>1.0</real_time_factor>  <!-- Real-time simulation -->\n      <real_time_update_rate>1000</real_time_update_rate>  <!-- 1000 Hz update rate -->\n\n      <!-- ODE-specific parameters -->\n      <ode>\n        <solver>\n          <type>quick</type>  <!-- Quick solver for performance -->\n          <iters>100</iters>  <!-- Iterations for constraint solving -->\n          <sor>1.3</sor>  <!-- Successive Over-Relaxation parameter -->\n        </solver>\n        <constraints>\n          <cfm>0.0</cfm>  <!-- Constraint Force Mixing -->\n          <erp>0.2</erp>  <!-- Error Reduction Parameter -->\n          <contact_max_correcting_vel>100.0</contact_max_correcting_vel>\n          <contact_surface_layer>0.001</contact_surface_layer>  <!-- Penetration allowance -->\n        </constraints>\n      </ode>\n    </physics>\n\n    <!-- Gravity -->\n    <gravity>0 0 -9.8</gravity>\n\n    <!-- Wind effects (optional) -->\n    <wind>\n      <linear_velocity>0 0 0</linear_velocity>\n    </wind>\n  </world>\n</sdf>\n```\n\n### Humanoid-Specific Physics Parameters\n\nFor humanoid robots, special attention must be paid to parameters that affect balance and locomotion:\n\n```xml\n<!-- Physics parameters optimized for humanoid simulation -->\n<physics type=\"ode\" name=\"humanoid_physics\">\n  <max_step_size>0.001</max_step_size>\n  <real_time_factor>0.8</real_time_factor>  <!-- Slightly slower for stability -->\n  <real_time_update_rate>1000</real_time_update_rate>\n\n  <ode>\n    <solver>\n      <type>quick</type>\n      <iters>200</iters>  <!-- More iterations for humanoid stability -->\n      <sor>1.0</sor>  <!-- Conservative SOR for stability -->\n    </solver>\n    <constraints>\n      <cfm>1e-5</cfm>  <!-- Lower CFM for tighter constraints -->\n      <erp>0.1</erp>  <!-- Lower ERP for better error correction -->\n      <contact_max_correcting_vel>10.0</contact_max_correcting_vel>\n      <contact_surface_layer>0.002</contact_surface_layer>  <!-- Slightly higher for foot contacts -->\n    </constraints>\n  </ode>\n</physics>\n```\n\n## Contact Materials and Friction\n\nProper contact modeling is crucial for humanoid balance and walking:\n\n```xml\n<!-- Material properties for humanoid robot -->\n<gazebo reference=\"humanoid_robot\">\n  <!-- Define materials with appropriate friction coefficients -->\n  <material name=\"rubber_foot\">\n    <pbr>\n      <metal>\n        <albedo_map>file://materials/textures/rubber.png</albedo_map>\n        <roughness>0.8</roughness>\n        <metalness>0.0</metalness>\n      </metal>\n    </pbr>\n  </material>\n</gazebo>\n\n<!-- Foot link with high friction -->\n<gazebo reference=\"left_foot_link\">\n  <collision name=\"foot_collision\">\n    <surface>\n      <friction>\n        <ode>\n          <mu>0.8</mu>  <!-- High friction for stability -->\n          <mu2>0.8</mu2>\n          <slip1>0.0</slip1>  <!-- No slip for stable stance -->\n          <slip2>0.0</slip2>\n        </ode>\n        <torsional>\n          <coefficient>0.8</coefficient>\n          <use_patch_radius>false</use_patch_radius>\n          <surface_radius>0.02</surface_radius>\n        </torsional>\n      </friction>\n      <contact>\n        <ode>\n          <soft_cfm>0.001</soft_cfm>\n          <soft_erp>0.9</soft_erp>\n          <kp>1e5</kp>  <!-- High stiffness for foot contacts -->\n          <kd>100</kd>\n          <max_vel>100.0</max_vel>\n          <min_depth>0.001</min_depth>\n        </ode>\n      </contact>\n    </surface>\n  </collision>\n</gazebo>\n\n<!-- Body links with moderate friction -->\n<gazebo reference=\"torso_link\">\n  <collision name=\"torso_collision\">\n    <surface>\n      <friction>\n        <ode>\n          <mu>0.3</mu>  <!-- Moderate friction for body contacts -->\n          <mu2>0.3</mu2>\n        </ode>\n      </friction>\n    </surface>\n  </collision>\n</gazebo>\n```\n\n## Sensor Simulation with Noise Models\n\nAccurate sensor simulation with proper noise models is essential for robust controller development:\n\n```xml\n<!-- IMU sensor with realistic noise -->\n<gazebo reference=\"head_link\">\n  <sensor name=\"imu_sensor\" type=\"imu\">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <pose>0.05 0 0.05 0 0 0</pose>  <!-- Offset from link origin -->\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>  <!-- 0.2 mrad/s -->\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0000075</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0000075</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0</bias_mean>\n            <bias_stddev>0.0000075</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>  <!-- 17 mg -->\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name=\"imu_plugin\" filename=\"libgazebo_ros_imu.so\">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>~/out:=imu/data</remapping>\n      </ros>\n      <update_rate>100</update_rate>\n      <topic_name>imu/data</topic_name>\n      <frame_name>head_imu_link</frame_name>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n    </plugin>\n  </sensor>\n</gazebo>\n\n<!-- Depth camera with noise -->\n<gazebo reference=\"head_link\">\n  <sensor name=\"rgbd_camera\" type=\"depth\">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <pose>0.1 0 0.05 0 0 0</pose>\n    <camera name=\"head_camera\">\n      <horizontal_fov>1.047</horizontal_fov>  <!-- 60 degrees -->\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name=\"camera_controller\" filename=\"libgazebo_ros_openni_kinect.so\">\n      <alwaysOn>true</alwaysOn>\n      <updateRate>30.0</updateRate>\n      <cameraName>head_camera</cameraName>\n      <imageTopicName>rgb/image_raw</imageTopicName>\n      <depthImageTopicName>depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoName>\n      <frameName>head_camera_depth_optical_frame</frameName>\n      <baseline>0.1</baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n      <pointCloudCutoff>0.5</pointCloudCutoff>\n      <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n      <CxPrime>0.0</CxPrime>\n      <Cx>320.5</Cx>\n      <Cy>240.5</Cy>\n      <focalLength>320.0</focalLength>\n      <hack_baseline>0.07</hack_baseline>\n    </plugin>\n  </sensor>\n</gazebo>\n\n<!-- LiDAR sensor -->\n<gazebo reference=\"torso_link\">\n  <sensor name=\"lidar_2d\" type=\"ray\">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <pose>0.0 0.0 0.3 0 0 0</pose>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>  <!-- -π -->\n          <max_angle>3.14159</max_angle>   <!-- π -->\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>10.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name=\"lidar_controller\" filename=\"libgazebo_ros_laser.so\">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>scan:=scan_filtered</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n      <frame_name>lidar_link</frame_name>\n      <topic_name>scan</topic_name>\n    </plugin>\n  </sensor>\n</gazebo>\n```\n\n## Terrain and Environment Setup\n\nCreating realistic environments for humanoid testing:\n\n```xml\n<!-- world_with_terrain.world -->\n<sdf version=\"1.7\">\n  <world name=\"humanoid_terrain_world\">\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    <include>\n      <uri>model://sun</uri>\n    </include>\n\n    <!-- Physics -->\n    <physics type=\"ode\" name=\"terrain_physics\">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n      <real_time_update_rate>1000</real_time_update_rate>\n      <ode>\n        <solver>\n          <type>quick</type>\n          <iters>100</iters>\n          <sor>1.3</sor>\n        </solver>\n        <constraints>\n          <cfm>0.0</cfm>\n          <erp>0.2</erp>\n          <contact_max_correcting_vel>100.0</contact_max_correcting_vel>\n          <contact_surface_layer>0.001</contact_surface_layer>\n        </constraints>\n      </ode>\n    </physics>\n\n    <!-- Terrain models -->\n    <model name=\"uneven_terrain\">\n      <static>true</static>\n      <link name=\"terrain_link\">\n        <collision name=\"terrain_collision\">\n          <geometry>\n            <heightmap>\n              <uri>file://terrain/uneven_heightmap.png</uri>\n              <size>10 10 1</size>  <!-- width, depth, height -->\n              <pos>0 0 0</pos>\n            </heightmap>\n          </geometry>\n          <surface>\n            <friction>\n              <ode>\n                <mu>0.8</mu>\n                <mu2>0.8</mu2>\n              </ode>\n            </friction>\n          </surface>\n        </collision>\n        <visual name=\"terrain_visual\">\n          <geometry>\n            <heightmap>\n              <uri>file://terrain/uneven_heightmap.png</uri>\n              <size>10 10 1</size>\n            </heightmap>\n          </geometry>\n        </visual>\n        <inertial>\n          <mass>1e10</mass>\n          <inertia>\n            <ixx>1e10</ixx>\n            <ixy>0</ixy>\n            <ixz>0</ixz>\n            <iyy>1e10</iyy>\n            <iyz>0</iyz>\n            <izz>1e10</izz>\n          </inertia>\n        </inertial>\n      </link>\n    </model>\n\n    <!-- Stairs for testing locomotion -->\n    <model name=\"stairs\">\n      <static>true</static>\n      <link name=\"stairs_link\">\n        <collision name=\"stairs_collision\">\n          <geometry>\n            <mesh>\n              <uri>file://meshes/stairs.dae</uri>\n            </mesh>\n          </geometry>\n          <surface>\n            <friction>\n              <ode>\n                <mu>0.9</mu>\n                <mu2>0.9</mu2>\n              </ode>\n            </friction>\n          </surface>\n        </collision>\n        <visual name=\"stairs_visual\">\n          <geometry>\n            <mesh>\n              <uri>file://meshes/stairs.dae</uri>\n            </mesh>\n          </geometry>\n        </visual>\n        <inertial>\n          <mass>1e10</mass>\n          <inertia>\n            <ixx>1e10</ixx>\n            <ixy>0</ixy>\n            <ixz>0</ixz>\n            <iyy>1e10</iyy>\n            <iyz>0</iyz>\n            <izz>1e10</izz>\n          </inertia>\n        </inertial>\n      </link>\n    </model>\n\n    <!-- Obstacle course -->\n    <model name=\"obstacle_1\">\n      <pose>2 0 0.1 0 0 0</pose>\n      <link name=\"obstacle_link\">\n        <collision name=\"collision\">\n          <geometry>\n            <box>\n              <size>0.2 0.2 0.2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name=\"visual\">\n          <geometry>\n            <box>\n              <size>0.2 0.2 0.2</size>\n            </box>\n          </geometry>\n          <material>\n            <script>\n              <uri>file://media/materials/scripts/gazebo.material</uri>\n              <name>Gazebo/Blue</name>\n            </script>\n          </material>\n        </visual>\n        <inertial>\n          <mass>1.0</mass>\n          <inertia>\n            <ixx>0.0083</ixx>\n            <ixy>0</ixy>\n            <ixz>0</ixz>\n            <iyy>0.0083</iyy>\n            <iyz>0</iyz>\n            <izz>0.0083</izz>\n          </inertia>\n        </inertial>\n      </link>\n    </model>\n\n    <!-- Humanoid robot -->\n    <include>\n      <uri>model://humanoid_robot</uri>\n      <pose>0 0 0.8 0 0 0</pose>  <!-- Start above ground for stable placement -->\n    </include>\n  </world>\n</sdf>\n```\n\n## Simulation Validation\n\nValidating that the simulation accurately represents real-world physics:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState\nfrom geometry_msgs.msg import PointStamped\nfrom std_msgs.msg import Float64\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport matplotlib.pyplot as plt\n\nclass SimulationValidator(Node):\n    def __init__(self):\n        super().__init__('simulation_validator')\n\n        # Subscriptions for validation\n        self.imu_sub = self.create_subscription(Imu, '/humanoid/imu/data', self.imu_callback, 10)\n        self.joint_sub = self.create_subscription(JointState, '/joint_states', self.joint_callback, 10)\n\n        # Publishers for validation metrics\n        self.balance_error_pub = self.create_publisher(Float64, '/validation/balance_error', 10)\n        self.energy_pub = self.create_publisher(Float64, '/validation/energy_consumption', 10)\n\n        # Storage for validation data\n        self.imu_data = []\n        self.joint_data = []\n        self.validation_metrics = {\n            'balance_stability': [],\n            'energy_efficiency': [],\n            'tracking_accuracy': [],\n            'contact_stability': []\n        }\n\n        # Reference values from real robot (when available)\n        self.real_robot_reference = {\n            'walking_speed': 0.5,  # m/s\n            'step_frequency': 1.8,  # Hz\n            'energy_per_meter': 150.0,  # J/m\n            'balance_variance': 0.02  # rad^2\n        }\n\n        self.validation_timer = self.create_timer(0.1, self.validation_callback)\n\n    def imu_callback(self, msg: Imu):\n        \"\"\"Process IMU data for validation\"\"\"\n        # Convert quaternion to Euler angles\n        quat = [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]\n        r = R.from_quat(quat)\n        euler = r.as_euler('xyz')\n\n        # Calculate balance metrics\n        roll, pitch, yaw = euler\n\n        # Zero Moment Point (ZMP) approximation\n        com_height = 0.7  # Approximate CoM height for humanoid\n        zmp_x = com_height * np.tan(pitch)\n        zmp_y = com_height * np.tan(roll)\n\n        # Balance error (deviation from center)\n        balance_error = np.sqrt(zmp_x**2 + zmp_y**2)\n\n        # Publish balance error\n        error_msg = Float64()\n        error_msg.data = float(balance_error)\n        self.balance_error_pub.publish(error_msg)\n\n        # Store for validation analysis\n        self.imu_data.append({\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9,\n            'roll': roll,\n            'pitch': pitch,\n            'yaw': yaw,\n            'linear_acc': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z],\n            'angular_vel': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],\n            'balance_error': balance_error\n        })\n\n    def joint_callback(self, msg: JointState):\n        \"\"\"Process joint state data for validation\"\"\"\n        # Calculate energy consumption approximation\n        if hasattr(self, 'prev_joint_positions') and hasattr(self, 'prev_timestamp'):\n            dt = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9 - self.prev_timestamp\n\n            if dt > 0:\n                # Calculate joint velocities\n                velocities = []\n                for i, pos in enumerate(msg.position):\n                    if i < len(self.prev_joint_positions):\n                        vel = (pos - self.prev_joint_positions[i]) / dt\n                        velocities.append(vel)\n                    else:\n                        velocities.append(0.0)\n\n                # Approximate energy consumption (simplified model)\n                # Energy = sum of (torque * velocity) over time\n                # For now, use (velocity squared) as proxy\n                approx_energy = sum(v**2 for v in velocities) * dt\n\n                energy_msg = Float64()\n                energy_msg.data = float(approx_energy)\n                self.energy_pub.publish(energy_msg)\n\n        # Store for validation\n        self.joint_data.append({\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9,\n            'positions': msg.position,\n            'velocities': msg.velocity,\n            'efforts': msg.effort\n        })\n\n        # Update previous values\n        self.prev_joint_positions = msg.position[:]\n        self.prev_timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n\n    def validation_callback(self):\n        \"\"\"Perform ongoing validation\"\"\"\n        if len(self.imu_data) > 10:  # Need sufficient data\n            # Calculate validation metrics\n            balance_stability = self.calculate_balance_stability()\n            energy_efficiency = self.calculate_energy_efficiency()\n            contact_stability = self.calculate_contact_stability()\n\n            # Store metrics\n            self.validation_metrics['balance_stability'].append(balance_stability)\n            self.validation_metrics['energy_efficiency'].append(energy_efficiency)\n            self.validation_metrics['contact_stability'].append(contact_stability)\n\n            # Log validation results\n            self.get_logger().info(\n                f'Validation - Balance: {balance_stability:.3f}, '\n                f'Energy: {energy_efficiency:.3f}, '\n                f'Contact: {contact_stability:.3f}'\n            )\n\n            # Check for significant deviations from expected behavior\n            self.check_simulation_fidelity()\n\n    def calculate_balance_stability(self) -> float:\n        \"\"\"Calculate balance stability metric\"\"\"\n        if len(self.imu_data) < 10:\n            return 0.0\n\n        # Calculate variance of balance error (lower is better)\n        balance_errors = [d['balance_error'] for d in self.imu_data[-50:]]  # Last 50 readings\n        if len(balance_errors) > 1:\n            stability = 1.0 / (1.0 + np.var(balance_errors))  # Higher is more stable\n        else:\n            stability = 1.0\n\n        return stability\n\n    def calculate_energy_efficiency(self) -> float:\n        \"\"\"Calculate energy efficiency metric\"\"\"\n        if len(self.joint_data) < 10:\n            return 0.0\n\n        # Calculate average joint velocities (lower is more efficient)\n        if len(self.joint_data) > 1:\n            avg_velocities = []\n            for i in range(len(self.joint_data[-1]['velocities'])):\n                velocities = [jd['velocities'][i] for jd in self.joint_data[-20:]]\n                if velocities:\n                    avg_velocities.append(np.mean(np.abs(velocities)))\n\n            if avg_velocities:\n                avg_avg_velocity = np.mean(avg_velocities)\n                # Higher efficiency for lower average velocities\n                efficiency = 1.0 / (1.0 + avg_avg_velocity)\n            else:\n                efficiency = 1.0\n        else:\n            efficiency = 1.0\n\n        return efficiency\n\n    def calculate_contact_stability(self) -> float:\n        \"\"\"Calculate contact stability metric\"\"\"\n        # This would involve checking contact forces and foot contacts\n        # For simulation, we can approximate using IMU data\n        if len(self.imu_data) < 10:\n            return 0.0\n\n        # Look for high frequency oscillations that indicate instability\n        angular_velocities = []\n        for d in self.imu_data[-20:]:\n            ang_vel_mag = np.linalg.norm(d['angular_vel'])\n            angular_velocities.append(ang_vel_mag)\n\n        if len(angular_velocities) > 1:\n            stability = 1.0 / (1.0 + np.std(angular_velocities))  # Lower std dev = more stable\n        else:\n            stability = 1.0\n\n        return stability\n\n    def check_simulation_fidelity(self):\n        \"\"\"Check if simulation behavior matches expected real-world behavior\"\"\"\n        if len(self.validation_metrics['balance_stability']) < 100:\n            return  # Need more data\n\n        # Calculate recent averages\n        recent_balance = np.mean(self.validation_metrics['balance_stability'][-50:])\n        recent_energy = np.mean(self.validation_metrics['energy_efficiency'][-50:])\n        recent_contact = np.mean(self.validation_metrics['contact_stability'][-50:])\n\n        # Check for anomalies\n        if recent_balance < 0.3:  # Very unstable\n            self.get_logger().warn(f'High balance instability detected: {recent_balance:.3f}')\n        if recent_energy < 0.3:  # Very inefficient\n            self.get_logger().warn(f'Low energy efficiency detected: {recent_energy:.3f}')\n        if recent_contact < 0.3:  # Unstable contacts\n            self.get_logger().warn(f'Unstable contact dynamics detected: {recent_contact:.3f}')\n\n    def generate_validation_report(self) -> str:\n        \"\"\"Generate a comprehensive validation report\"\"\"\n        if not self.validation_metrics['balance_stability']:\n            return \"Insufficient data for validation report\"\n\n        report = \"SIMULATION VALIDATION REPORT\\n\"\n        report += \"=\" * 30 + \"\\n\\n\"\n\n        # Balance stability\n        avg_balance = np.mean(self.validation_metrics['balance_stability'])\n        report += f\"Balance Stability: {avg_balance:.3f}/1.0 (Higher is better)\\n\"\n\n        # Energy efficiency\n        avg_energy = np.mean(self.validation_metrics['energy_efficiency'])\n        report += f\"Energy Efficiency: {avg_energy:.3f}/1.0 (Higher is better)\\n\"\n\n        # Contact stability\n        avg_contact = np.mean(self.validation_metrics['contact_stability'])\n        report += f\"Contact Stability: {avg_contact:.3f}/1.0 (Higher is better)\\n\\n\"\n\n        # Recommendations\n        report += \"RECOMMENDATIONS:\\n\"\n        if avg_balance < 0.5:\n            report += \"- Improve balance controller parameters\\n\"\n        if avg_energy < 0.5:\n            report += \"- Optimize joint control for efficiency\\n\"\n        if avg_contact < 0.5:\n            report += \"- Review contact parameters and friction\\n\"\n\n        return report\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = SimulationValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        # Generate final report\n        report = validator.generate_validation_report()\n        print(report)\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Practical Exercise\n\n1. Create a humanoid model with appropriate physics properties\n2. Set up a challenging terrain environment\n3. Implement sensor noise models that match real hardware\n4. Validate simulation results against expected real-world behavior\n\n## Summary\n\nPhysics simulation in Gazebo requires careful tuning of parameters for humanoid robots. Proper friction, contact models, and sensor noise are crucial for realistic simulation. The validation process ensures that simulated behavior matches expected real-world performance, enabling confidence in simulation-based development and testing.",
    "module": "Module 2: Digital Twin (Gazebo + Unity)",
    "chapter": "gazebo physics simulation.md",
    "path": "/docs/module-2/2.1-gazebo-physics-simulation"
  },
  {
    "id": "module-2/2.2-sensors-in-simulation",
    "title": "Chapter 2.2 – Sensors in Simulation",
    "content": "\n# Chapter 2.2 – Sensors in Simulation\n\n## Learning Objectives\n- Implement LiDAR sensors for humanoid robots in Gazebo\n- Configure depth cameras with realistic parameters\n- Set up IMU sensors with appropriate noise models\n- Model sensor realism including noise and systematic errors\n\n## LiDAR Simulation\n\nLiDAR sensors are crucial for humanoid robots for navigation, mapping, and obstacle detection. In Gazebo, configure LiDAR with realistic parameters:\n\n```xml\n<sdf version=\"1.7\">\n  <sensor name=\"humanoid_lidar\" type=\"ray\">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <pose>0.1 0 0.5 0 0 0</pose>  <!-- Position on robot head/torso -->\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>360</samples>  <!-- 360 degree scan -->\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>  <!-- -π radians -->\n          <max_angle>3.14159</max_angle>   <!-- π radians -->\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>10.0</max>  <!-- 10 meter range -->\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name=\"lidar_controller\" filename=\"libgazebo_ros_ray_sensor.so\">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</sdf>\n```\n\n### Multi-line LiDAR for 3D Perception\n\nFor humanoid robots that need 3D perception:\n\n```xml\n<sensor name=\"3d_lidar\" type=\"ray\">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1024</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>64</samples>  <!-- 64 vertical beams -->\n        <resolution>1</resolution>\n        <min_angle>-0.2618</min_angle>  <!-- -15 degrees -->\n        <max_angle>0.2618</max_angle>    <!-- 15 degrees -->\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <!-- Noise model for realistic behavior -->\n  <noise type=\"gaussian\">\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>  <!-- 1cm accuracy -->\n  </noise>\n</sensor>\n```\n\n## Depth Camera Configuration\n\nDepth cameras provide crucial 3D information for humanoid manipulation and navigation:\n\n```xml\n<sensor name=\"depth_camera\" type=\"depth\">\n  <always_on>true</always_on>\n  <update_rate>30</update_rate>\n  <pose>0.15 0 0.4 0 0 0</pose>  <!-- Positioned like a head camera -->\n  <camera name=\"humanoid_camera\">\n    <horizontal_fov>1.047</horizontal_fov>  <!-- 60 degrees -->\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.007</stddev>\n    </noise>\n  </camera>\n  <plugin name=\"camera_controller\" filename=\"libgazebo_ros_openni_kinect.so\">\n    <alwaysOn>true</alwaysOn>\n    <updateRate>30.0</updateRate>\n    <cameraName>head_camera</cameraName>\n    <imageTopicName>rgb/image_raw</imageTopicName>\n    <depthImageTopicName>depth/image_raw</depthImageTopicName>\n    <pointCloudTopicName>depth/points</pointCloudTopicName>\n    <cameraInfoTopicName>rgb/camera_info</cameraInfoTopicName>\n    <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>\n    <frameName>head_camera_depth_optical_frame</frameName>\n    <baseline>0.1</baseline>\n    <distortion_k1>0.0</distortion_k1>\n    <distortion_k2>0.0</distortion_k2>\n    <distortion_k3>0.0</distortion_k3>\n    <distortion_t1>0.0</distortion_t1>\n    <distortion_t2>0.0</distortion_t2>\n    <pointCloudCutoff>0.5</pointCloudCutoff>\n    <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n    <CxPrime>0.0</CxPrime>\n    <Cx>320.5</Cx>\n    <Cy>240.5</Cy>\n    <focalLength>320.0</focalLength>\n  </plugin>\n</sensor>\n```\n\n### RGB-D Sensor Fusion\n\nCombine RGB and depth for enhanced perception:\n\n```cpp\n#include <sensor_msgs/msg/image.hpp>\n#include <sensor_msgs/msg/camera_info.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass PerceptionFusion {\npublic:\n  void imageCallback(const sensor_msgs::msg::Image::SharedPtr rgb_msg,\n                     const sensor_msgs::msg::Image::SharedPtr depth_msg) {\n\n    // Convert ROS images to OpenCV\n    cv_bridge::CvImagePtr rgb_cv = cv_bridge::toCvCopy(rgb_msg, \"bgr8\");\n    cv_bridge::CvImagePtr depth_cv = cv_bridge::toCvCopy(depth_msg, \"32FC1\");\n\n    // Create point cloud from RGB-D data\n    createPointCloud(rgb_cv->image, depth_cv->image);\n  }\n\nprivate:\n  void createPointCloud(const cv::Mat& rgb, const cv::Mat& depth) {\n    // Implementation to create 3D point cloud from 2D images\n    // Uses camera intrinsics and depth values\n  }\n};\n```\n\n## IMU Simulation\n\nIMU sensors are critical for humanoid balance and state estimation:\n\n```xml\n<sensor name=\"imu_sensor\" type=\"imu\">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>\n  <pose>0 0 0.2 0 0 0</pose>  <!-- Position in torso -->\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type=\"gaussian\">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>  <!-- 1 mrad/s accuracy -->\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0001</bias_stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type=\"gaussian\">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0001</bias_stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type=\"gaussian\">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0001</bias_stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type=\"gaussian\">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>  <!-- 17 mg accuracy -->\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0017</bias_stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type=\"gaussian\">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0017</bias_stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type=\"gaussian\">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0017</bias_stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n  <plugin name=\"imu_plugin\" filename=\"libgazebo_ros_imu_sensor.so\">\n    <ros>\n      <namespace>/humanoid</namespace>\n      <remapping>~/out:=imu/data</remapping>\n    </ros>\n    <initial_orientation_as_reference>false</initial_orientation_as_reference>\n  </plugin>\n</sensor>\n```\n\n### IMU-Based Balance Control\n\nUse IMU data for humanoid balance:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nimport numpy as np\n\nclass BalanceController(Node):\n    def __init__(self):\n        super().__init__('balance_controller')\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/humanoid/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        self.roll = 0.0\n        self.pitch = 0.0\n        self.yaw = 0.0\n\n    def imu_callback(self, msg: Imu):\n        # Extract orientation from quaternion\n        q = msg.orientation\n        self.roll, self.pitch, self.yaw = self.quaternion_to_euler([\n            q.x, q.y, q.z, q.w\n        ])\n\n        # Extract angular velocities\n        self.angular_vel = {\n            'x': msg.angular_velocity.x,\n            'y': msg.angular_velocity.y,\n            'z': msg.angular_velocity.z\n        }\n\n        # Extract linear accelerations\n        self.linear_acc = {\n            'x': msg.linear_acceleration.x,\n            'y': msg.linear_acceleration.y,\n            'z': msg.linear_acceleration.z\n        }\n\n        # Check balance and trigger corrections\n        self.check_balance()\n\n    def check_balance(self):\n        # Simple balance check\n        if abs(self.roll) > 0.3 or abs(self.pitch) > 0.3:\n            self.get_logger().warn('Balance error detected!')\n            # Implement balance recovery here\n            self.recover_balance()\n\n    def recover_balance(self):\n        # Send corrective joint commands based on IMU data\n        pass\n\n    def quaternion_to_euler(self, q):\n        # Convert quaternion to Euler angles\n        import math\n        sinr_cosp = 2 * (q[3] * q[0] + q[1] * q[2])\n        cosr_cosp = 1 - 2 * (q[0] * q[0] + q[1] * q[1])\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (q[3] * q[1] - q[2] * q[0])\n        pitch = math.asin(sinp)\n\n        siny_cosp = 2 * (q[3] * q[2] + q[0] * q[1])\n        cosy_cosp = 1 - 2 * (q[1] * q[1] + q[2] * q[2])\n        yaw = math.atan2(siny_cosp, cosy_cosp)\n\n        return roll, pitch, yaw\n```\n\n## Noise Modeling and Realism\n\nRealistic sensor simulation requires proper noise modeling:\n\n### LiDAR Noise Parameters\n\n```xml\n<sensor name=\"lidar_with_noise\" type=\"ray\">\n  <!-- ... other configuration ... -->\n  <noise type=\"gaussian\">\n    <mean>0.0</mean>\n    <stddev>0.02</stddev>  <!-- 2cm standard deviation -->\n    <bias_mean>0.01</bias_mean>  <!-- 1cm systematic bias -->\n    <bias_stddev>0.005</bias_stddev>  <!-- 0.5cm bias drift -->\n  </noise>\n</sensor>\n```\n\n### Camera Noise Models\n\n```xml\n<sensor name=\"camera_with_noise\" type=\"camera\">\n  <camera>\n    <!-- ... camera configuration ... -->\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.005</stddev>  <!-- 0.5% of pixel value -->\n    </noise>\n  </camera>\n</sensor>\n```\n\n## Practical Exercise\n\n1. Add LiDAR, depth camera, and IMU sensors to your humanoid model\n2. Configure realistic noise parameters for each sensor\n3. Create a ROS2 node to subscribe to sensor data\n4. Visualize sensor data in RViz and verify realistic behavior\n\n## Summary\n\nRealistic sensor simulation is crucial for humanoid robot development. LiDAR provides 2D/3D environmental information, depth cameras enable 3D perception, and IMUs are essential for balance control. Proper noise modeling ensures that algorithms developed in simulation will transfer to real robots. The combination of these sensors enables humanoid robots to perceive and interact with their environment effectively.",
    "module": "Module 2: Digital Twin (Gazebo + Unity)",
    "chapter": "sensors in simulation.md",
    "path": "/docs/module-2/2.2-sensors-in-simulation"
  },
  {
    "id": "module-2/2.3-human-robot-interaction-hri",
    "title": "2.3 Human-Robot Interaction (HRI)",
    "content": "# 2.3 Human-Robot Interaction (HRI)\n\n## Overview\n\nHuman-Robot Interaction (HRI) is a multidisciplinary field that focuses on the design, development, and evaluation of robots that interact with humans. In the context of digital twins and simulation environments, HRI encompasses the creation of intuitive interfaces, natural interaction modalities, and collaborative behaviors that enable effective human-robot teamwork. This chapter explores the principles, techniques, and technologies that enable meaningful interaction between humans and robots in both simulated and real-world environments.\n\n## Learning Objectives\n\nBy the end of this chapter, you will be able to:\n- Understand the fundamental principles of Human-Robot Interaction\n- Design intuitive interfaces for robot control and monitoring\n- Implement multimodal interaction systems (speech, gesture, vision)\n- Create collaborative behaviors for human-robot teams\n- Address safety and trust considerations in HRI\n- Evaluate HRI systems using appropriate metrics\n- Implement HRI systems in simulation environments\n\n## Foundations of Human-Robot Interaction\n\n### HRI Principles and Design Considerations\n\nEffective Human-Robot Interaction is built on several fundamental principles that ensure safe, intuitive, and productive collaboration:\n\n```python\nclass HRIDesignPrinciples:\n    \"\"\"\n    Core principles for effective Human-Robot Interaction design\n    \"\"\"\n    def __init__(self):\n        self.principles = {\n            'transparency': {\n                'description': 'Robots should clearly communicate their intentions and state',\n                'implementation': self.implement_transparency\n            },\n            'predictability': {\n                'description': 'Robot behavior should be consistent and predictable',\n                'implementation': self.implement_predictability\n            },\n            'appropriateness': {\n                'description': 'Robot behavior should be appropriate for the context',\n                'implementation': self.implement_appropriateness\n            },\n            'trust': {\n                'description': 'Build and maintain appropriate levels of human trust',\n                'implementation': self.implement_trust\n            },\n            'safety': {\n                'description': 'Ensure physical and psychological safety of humans',\n                'implementation': self.implement_safety\n            }\n        }\n\n    def implement_transparency(self, robot_state, human_operator):\n        \"\"\"\n        Implement transparency through clear communication of robot state and intentions\n        \"\"\"\n        # Provide clear feedback about robot's current task\n        current_task = robot_state.get_current_task()\n        human_operator.display_feedback(f\"Current task: {current_task}\")\n\n        # Communicate planned actions\n        planned_actions = robot_state.get_planned_actions()\n        human_operator.display_feedback(f\"Next actions: {planned_actions}\")\n\n        # Show confidence levels in decisions\n        confidence_levels = robot_state.get_confidence_levels()\n        human_operator.display_feedback(f\"Confidence: {confidence_levels}\")\n\n    def implement_predictability(self, robot_behavior):\n        \"\"\"\n        Ensure robot behavior is consistent and follows expected patterns\n        \"\"\"\n        # Use consistent interaction patterns\n        robot_behavior.set_interaction_style('consistent')\n\n        # Follow established protocols\n        robot_behavior.adhere_to_protocols = True\n\n        # Maintain temporal consistency\n        robot_behavior.response_time = 0.5  # seconds\n\n    def implement_appropriateness(self, context, robot_behavior):\n        \"\"\"\n        Adjust robot behavior based on context and social norms\n        \"\"\"\n        # Adjust behavior based on environment\n        if context.environment == 'industrial':\n            robot_behavior.intensity = 'high_efficiency'\n        elif context.environment == 'domestic':\n            robot_behavior.intensity = 'friendly'\n        elif context.environment == 'healthcare':\n            robot_behavior.intensity = 'careful'\n\n        # Adjust to human preferences\n        robot_behavior.personalize_to_human(context.human_preferences)\n\n    def implement_trust(self, human_state, robot_state):\n        \"\"\"\n        Build and maintain appropriate trust levels\n        \"\"\"\n        # Start with conservative behavior\n        if human_state.trust_level < 0.5:\n            robot_state.set_conservative_behavior(True)\n        else:\n            robot_state.set_conservative_behavior(False)\n\n        # Provide honest feedback about capabilities\n        robot_state.report_capabilities_realistically()\n\n    def implement_safety(self, safety_system, robot_state):\n        \"\"\"\n        Ensure safety in all interactions\n        \"\"\"\n        safety_system.enable_proximity_detection(True)\n        safety_system.set_safe_distance(0.5)  # meters\n        safety_system.enable_emergency_stop(True)\n\nclass InteractionModality:\n    \"\"\"\n    Base class for different interaction modalities\n    \"\"\"\n    def __init__(self, name, priority=1):\n        self.name = name\n        self.priority = priority\n        self.enabled = True\n        self.confidence_threshold = 0.7\n\n    def process_input(self, raw_input):\n        \"\"\"\n        Process raw input and return structured data\n        \"\"\"\n        raise NotImplementedError\n\n    def validate_input(self, processed_input):\n        \"\"\"\n        Validate processed input meets quality standards\n        \"\"\"\n        return processed_input.confidence > self.confidence_threshold\n\n    def generate_response(self, processed_input):\n        \"\"\"\n        Generate appropriate response to input\n        \"\"\"\n        raise NotImplementedError\n```\n\n### Multimodal Interaction Systems\n\nModern HRI systems leverage multiple interaction modalities to create more natural and robust communication:\n\n```python\nimport numpy as np\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional, Tuple\nimport threading\nimport time\n\nclass ModalityType(Enum):\n    SPEECH = \"speech\"\n    GESTURE = \"gesture\"\n    VISION = \"vision\"\n    HAPTIC = \"haptic\"\n    PROXIMITY = \"proximity\"\n\n@dataclass\nclass InteractionInput:\n    \"\"\"\n    Structured input from various modalities\n    \"\"\"\n    modality: ModalityType\n    data: any\n    timestamp: float\n    confidence: float\n    source_id: str\n    context: Dict = None\n\n@dataclass\nclass InteractionOutput:\n    \"\"\"\n    Structured output to various modalities\n    \"\"\"\n    modality: ModalityType\n    data: any\n    priority: int\n    target: str\n\nclass MultimodalFusion:\n    \"\"\"\n    Fuses inputs from multiple modalities to create coherent interaction\n    \"\"\"\n    def __init__(self):\n        self.modality_weights = {\n            ModalityType.SPEECH: 0.4,\n            ModalityType.GESTURE: 0.3,\n            ModalityType.VISION: 0.2,\n            ModalityType.HAPTIC: 0.05,\n            ModalityType.PROXIMITY: 0.05\n        }\n        self.temporal_window = 2.0  # seconds\n        self.interaction_buffer = []\n        self.fusion_lock = threading.Lock()\n\n    def add_input(self, input_data: InteractionInput):\n        \"\"\"\n        Add input from a modality to the fusion system\n        \"\"\"\n        with self.fusion_lock:\n            self.interaction_buffer.append(input_data)\n            # Remove old inputs outside temporal window\n            current_time = time.time()\n            self.interaction_buffer = [\n                inp for inp in self.interaction_buffer\n                if current_time - inp.timestamp < self.temporal_window\n            ]\n\n    def fuse_inputs(self) -> Optional[Dict]:\n        \"\"\"\n        Fuse recent inputs to create a coherent interaction interpretation\n        \"\"\"\n        with self.fusion_lock:\n            if not self.interaction_buffer:\n                return None\n\n            # Group inputs by temporal proximity\n            recent_inputs = self._group_temporally_proximate_inputs()\n\n            # Fuse inputs from different modalities\n            fused_interpretation = self._fuse_modalities(recent_inputs)\n\n            return fused_interpretation\n\n    def _group_temporally_proximate_inputs(self) -> List[List[InteractionInput]]:\n        \"\"\"\n        Group inputs that occurred close in time\n        \"\"\"\n        if not self.interaction_buffer:\n            return []\n\n        groups = []\n        current_group = [self.interaction_buffer[0]]\n\n        for input_data in self.interaction_buffer[1:]:\n            time_diff = abs(input_data.timestamp - current_group[-1].timestamp)\n            if time_diff < 0.5:  # 500ms window\n                current_group.append(input_data)\n            else:\n                if len(current_group) > 0:\n                    groups.append(current_group)\n                current_group = [input_data]\n\n        if len(current_group) > 0:\n            groups.append(current_group)\n\n        return groups\n\n    def _fuse_modalities(self, input_groups: List[List[InteractionInput]]) -> Dict:\n        \"\"\"\n        Fuse inputs from different modalities in the same temporal group\n        \"\"\"\n        if not input_groups:\n            return {}\n\n        # Take the most recent group\n        latest_group = input_groups[-1]\n\n        # Calculate weighted confidence for each modality in the group\n        modality_interpretations = {}\n        total_weight = 0\n\n        for input_data in latest_group:\n            weight = self.modality_weights.get(input_data.modality, 0.1)\n            modality_interpretations[input_data.modality] = {\n                'data': input_data.data,\n                'confidence': input_data.confidence,\n                'weight': weight,\n                'weighted_confidence': input_data.confidence * weight\n            }\n            total_weight += weight\n\n        # Calculate overall interpretation\n        if total_weight > 0:\n            overall_confidence = sum(\n                interp['weighted_confidence'] for interp in modality_interpretations.values()\n            ) / total_weight\n\n            return {\n                'interpretation': modality_interpretations,\n                'overall_confidence': overall_confidence,\n                'timestamp': time.time(),\n                'modalities_present': list(modality_interpretations.keys())\n            }\n\n        return {}\n\nclass SpeechModality(InteractionModality):\n    \"\"\"\n    Speech interaction modality\n    \"\"\"\n    def __init__(self):\n        super().__init__(ModalityType.SPEECH, priority=1)\n        self.speech_recognizer = None\n        self.language_model = None\n        self.confidence_threshold = 0.7\n\n    def process_input(self, raw_audio):\n        \"\"\"\n        Process raw audio input and convert to text/command\n        \"\"\"\n        # In practice, this would use a speech recognition system\n        recognized_text = self._recognize_speech(raw_audio)\n        intent = self._extract_intent(recognized_text)\n\n        return InteractionInput(\n            modality=ModalityType.SPEECH,\n            data={'text': recognized_text, 'intent': intent},\n            timestamp=time.time(),\n            confidence=self._calculate_confidence(recognized_text),\n            source_id='speech_recognizer'\n        )\n\n    def _recognize_speech(self, audio_data):\n        \"\"\"\n        Recognize speech from audio data\n        \"\"\"\n        # Placeholder for actual speech recognition\n        return \"recognized speech text\"\n\n    def _extract_intent(self, text):\n        \"\"\"\n        Extract intent from recognized text\n        \"\"\"\n        # Simple intent extraction - in practice, use NLP\n        if any(word in text.lower() for word in ['move', 'go', 'walk']):\n            return 'navigation'\n        elif any(word in text.lower() for word in ['pick', 'grasp', 'take']):\n            return 'manipulation'\n        elif any(word in text.lower() for word in ['stop', 'halt', 'pause']):\n            return 'stop'\n        else:\n            return 'unknown'\n\n    def _calculate_confidence(self, recognized_text):\n        \"\"\"\n        Calculate confidence in speech recognition\n        \"\"\"\n        # Placeholder confidence calculation\n        return 0.8 if recognized_text else 0.1\n\nclass GestureModality(InteractionModality):\n    \"\"\"\n    Gesture interaction modality\n    \"\"\"\n    def __init__(self):\n        super().__init__(ModalityType.GESTURE, priority=2)\n        self.gesture_classifier = None\n        self.tracking_system = None\n\n    def process_input(self, raw_gesture_data):\n        \"\"\"\n        Process raw gesture data and classify gesture\n        \"\"\"\n        gesture_type = self._classify_gesture(raw_gesture_data)\n        confidence = self._calculate_gesture_confidence(raw_gesture_data)\n\n        return InteractionInput(\n            modality=ModalityType.GESTURE,\n            data={'gesture_type': gesture_type, 'raw_data': raw_gesture_data},\n            timestamp=time.time(),\n            confidence=confidence,\n            source_id='gesture_tracker'\n        )\n\n    def _classify_gesture(self, gesture_data):\n        \"\"\"\n        Classify gesture from raw data\n        \"\"\"\n        # Placeholder gesture classification\n        # In practice, use computer vision and ML models\n        return \"wave\"\n\n    def _calculate_gesture_confidence(self, gesture_data):\n        \"\"\"\n        Calculate confidence in gesture classification\n        \"\"\"\n        # Placeholder confidence calculation\n        return 0.9\n```\n\n## Natural Language Processing for HRI\n\n### Intent Recognition and Natural Language Understanding\n\nNatural language processing is crucial for enabling human-like communication with robots:\n\n```python\nimport re\nfrom typing import Dict, List, Tuple\nimport spacy  # For advanced NLP (in practice)\n\nclass NaturalLanguageProcessor:\n    \"\"\"\n    Processes natural language input for HRI systems\n    \"\"\"\n    def __init__(self):\n        self.intent_patterns = {\n            'navigation': [\n                r'move to (.+)',\n                r'go to (.+)',\n                r'walk to (.+)',\n                r'go (.+)',\n                r'navigate to (.+)'\n            ],\n            'manipulation': [\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'take (.+)',\n                r'lift (.+)',\n                r'get (.+)'\n            ],\n            'stop': [\n                r'stop',\n                r'hold',\n                r'pause',\n                r'wait',\n                r'freeze'\n            ],\n            'follow': [\n                r'follow me',\n                r'come with me',\n                r'follow',\n                r'come after me'\n            ],\n            'greeting': [\n                r'hello',\n                r'hi',\n                r'hey',\n                r'good morning',\n                r'good afternoon'\n            ]\n        }\n\n        self.location_entities = [\n            'kitchen', 'bedroom', 'living room', 'office', 'bathroom',\n            'dining room', 'hallway', 'garage', 'garden', 'entrance'\n        ]\n\n        self.object_entities = [\n            'cup', 'book', 'phone', 'bottle', 'plate', 'fork', 'spoon',\n            'knife', 'glass', 'box', 'ball', 'toy', 'remote', 'keys'\n        ]\n\n    def parse_intent(self, text: str) -> Dict:\n        \"\"\"\n        Parse intent from natural language text\n        \"\"\"\n        text = text.lower().strip()\n        intent = 'unknown'\n        confidence = 0.0\n        entities = []\n\n        # Match against intent patterns\n        for intent_type, patterns in self.intent_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text)\n                if match:\n                    intent = intent_type\n                    confidence = 0.9  # High confidence for pattern match\n                    # Extract captured groups as entities\n                    entities.extend(list(match.groups()))\n                    break\n            if intent != 'unknown':\n                break\n\n        # Extract additional entities\n        additional_entities = self._extract_entities(text)\n        entities.extend(additional_entities)\n\n        return {\n            'intent': intent,\n            'confidence': confidence,\n            'entities': entities,\n            'original_text': text\n        }\n\n    def _extract_entities(self, text: str) -> List[str]:\n        \"\"\"\n        Extract named entities from text\n        \"\"\"\n        entities = []\n\n        # Extract location entities\n        for location in self.location_entities:\n            if location in text:\n                entities.append(location)\n\n        # Extract object entities\n        for obj in self.object_entities:\n            if obj in text:\n                entities.append(obj)\n\n        return entities\n\nclass DialogueManager:\n    \"\"\"\n    Manages conversation flow and context in HRI\n    \"\"\"\n    def __init__(self):\n        self.nlp_processor = NaturalLanguageProcessor()\n        self.conversation_context = {}\n        self.response_templates = {\n            'navigation': \"I will navigate to {location}.\",\n            'manipulation': \"I will pick up the {object}.\",\n            'stop': \"I have stopped.\",\n            'follow': \"I will follow you.\",\n            'greeting': \"Hello! How can I assist you today?\",\n            'unknown': \"I'm sorry, I didn't understand that.\"\n        }\n\n    def process_user_input(self, user_input: str) -> str:\n        \"\"\"\n        Process user input and generate appropriate response\n        \"\"\"\n        # Parse intent and entities\n        parsed = self.nlp_processor.parse_intent(user_input)\n\n        # Update conversation context\n        self._update_context(parsed)\n\n        # Generate response based on intent\n        response = self._generate_response(parsed)\n\n        return response\n\n    def _update_context(self, parsed_data: Dict):\n        \"\"\"\n        Update conversation context with new information\n        \"\"\"\n        self.conversation_context.update({\n            'last_intent': parsed_data['intent'],\n            'last_entities': parsed_data['entities'],\n            'last_confidence': parsed_data['confidence']\n        })\n\n    def _generate_response(self, parsed_data: Dict) -> str:\n        \"\"\"\n        Generate appropriate response based on parsed input\n        \"\"\"\n        intent = parsed_data['intent']\n        entities = parsed_data['entities']\n\n        if intent in self.response_templates:\n            template = self.response_templates[intent]\n\n            if entities and '{' in template:\n                # Fill in entities for parameterized templates\n                if intent == 'navigation' and entities:\n                    return template.format(location=entities[0])\n                elif intent == 'manipulation' and entities:\n                    return template.format(object=entities[0])\n                else:\n                    return template\n            else:\n                return template\n        else:\n            return self.response_templates['unknown']\n\n    def get_conversation_context(self) -> Dict:\n        \"\"\"\n        Get current conversation context\n        \"\"\"\n        return self.conversation_context\n```\n\n## Gesture Recognition and Computer Vision\n\n### Visual Interaction Systems\n\nVisual interaction enables robots to understand and respond to human gestures and visual cues:\n\n```python\nimport cv2\nimport numpy as np\nfrom typing import List, Tuple, Dict, Optional\nimport mediapipe as mp\n\nclass GestureRecognitionSystem:\n    \"\"\"\n    System for recognizing human gestures using computer vision\n    \"\"\"\n    def __init__(self):\n        # Initialize MediaPipe for hand and pose tracking\n        self.mp_hands = mp.solutions.hands\n        self.mp_pose = mp.solutions.pose\n        self.mp_face = mp.solutions.face_mesh\n\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.7,\n            min_tracking_confidence=0.7\n        )\n\n        self.pose = self.mp_pose.Pose(\n            static_image_mode=False,\n            model_complexity=1,\n            enable_segmentation=False,\n            min_detection_confidence=0.5\n        )\n\n        self.face = self.mp_face.FaceMesh(\n            static_image_mode=False,\n            max_num_faces=1,\n            min_detection_confidence=0.5\n        )\n\n        # Gesture recognition parameters\n        self.gesture_thresholds = {\n            'finger_count': 0.8,\n            'hand_distance': 0.5,\n            'movement_speed': 1.0\n        }\n\n    def process_frame(self, frame: np.ndarray) -> Dict:\n        \"\"\"\n        Process video frame for gesture recognition\n        \"\"\"\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Detect hands\n        hand_results = self.hands.process(rgb_frame)\n        hands_data = self._extract_hand_data(hand_results, frame.shape)\n\n        # Detect body pose\n        pose_results = self.pose.process(rgb_frame)\n        pose_data = self._extract_pose_data(pose_results)\n\n        # Detect face/micro-expressions\n        face_results = self.face.process(rgb_frame)\n        face_data = self._extract_face_data(face_results)\n\n        # Classify gestures\n        gestures = self._classify_gestures(hands_data, pose_data)\n\n        return {\n            'hands': hands_data,\n            'pose': pose_data,\n            'face': face_data,\n            'gestures': gestures,\n            'timestamp': time.time()\n        }\n\n    def _extract_hand_data(self, hand_results, frame_shape) -> List[Dict]:\n        \"\"\"\n        Extract hand landmark data from MediaPipe results\n        \"\"\"\n        if not hand_results.multi_hand_landmarks:\n            return []\n\n        hands_data = []\n        for i, hand_landmarks in enumerate(hand_results.multi_hand_landmarks):\n            # Convert landmarks to pixel coordinates\n            landmarks = []\n            for landmark in hand_landmarks.landmark:\n                x = int(landmark.x * frame_shape[1])\n                y = int(landmark.y * frame_shape[0])\n                z = landmark.z\n                landmarks.append((x, y, z))\n\n            # Determine hand type (left/right)\n            handedness = hand_results.multi_handedness[i].classification[0].label\n\n            hand_data = {\n                'landmarks': landmarks,\n                'handedness': handedness,\n                'confidence': hand_results.multi_handedness[i].classification[0].score\n            }\n\n            hands_data.append(hand_data)\n\n        return hands_data\n\n    def _extract_pose_data(self, pose_results) -> Optional[Dict]:\n        \"\"\"\n        Extract pose landmark data from MediaPipe results\n        \"\"\"\n        if not pose_results.pose_landmarks:\n            return None\n\n        landmarks = []\n        for landmark in pose_results.pose_landmarks.landmark:\n            landmarks.append((landmark.x, landmark.y, landmark.z))\n\n        return {\n            'landmarks': landmarks,\n            'confidence': pose_results.pose_landmarks.landmark[0].visibility\n        }\n\n    def _extract_face_data(self, face_results) -> Optional[Dict]:\n        \"\"\"\n        Extract face landmark data from MediaPipe results\n        \"\"\"\n        if not face_results.multi_face_landmarks:\n            return None\n\n        face_landmarks = face_results.multi_face_landmarks[0]\n        landmarks = []\n        for landmark in face_landmarks.landmark:\n            landmarks.append((landmark.x, landmark.y, landmark.z))\n\n        return {\n            'landmarks': landmarks,\n            'confidence': 0.8  # Placeholder\n        }\n\n    def _classify_gestures(self, hands_data: List[Dict], pose_data: Optional[Dict]) -> List[Dict]:\n        \"\"\"\n        Classify gestures from detected landmarks\n        \"\"\"\n        gestures = []\n\n        for hand_data in hands_data:\n            gesture = self._classify_single_hand_gesture(hand_data)\n            if gesture:\n                gestures.append(gesture)\n\n        if pose_data:\n            body_gestures = self._classify_body_gesture(pose_data)\n            gestures.extend(body_gestures)\n\n        return gestures\n\n    def _classify_single_hand_gesture(self, hand_data: Dict) -> Optional[Dict]:\n        \"\"\"\n        Classify gesture from single hand landmarks\n        \"\"\"\n        landmarks = hand_data['landmarks']\n\n        # Count extended fingers\n        extended_fingers = self._count_extended_fingers(landmarks)\n\n        # Classify based on finger count and configuration\n        if extended_fingers == 1:\n            return {\n                'type': 'pointing',\n                'confidence': 0.9,\n                'details': {'finger_count': 1}\n            }\n        elif extended_fingers == 2:\n            return {\n                'type': 'peace_sign',\n                'confidence': 0.85,\n                'details': {'finger_count': 2}\n            }\n        elif extended_fingers == 5:\n            return {\n                'type': 'open_hand',\n                'confidence': 0.9,\n                'details': {'finger_count': 5}\n            }\n        elif extended_fingers == 0:\n            return {\n                'type': 'fist',\n                'confidence': 0.8,\n                'details': {'finger_count': 0}\n            }\n        else:\n            return None\n\n    def _count_extended_fingers(self, landmarks: List[Tuple]) -> int:\n        \"\"\"\n        Count the number of extended fingers\n        \"\"\"\n        if len(landmarks) < 21:\n            return 0\n\n        # Thumb (landmarks 1-4), other fingers (tips at 8, 12, 16, 20)\n        extended_count = 0\n\n        # Check thumb (between wrist and tip)\n        wrist = np.array(landmarks[0])\n        thumb_tip = np.array(landmarks[4])\n        thumb_mcp = np.array(landmarks[2])\n\n        # Thumb is extended if tip is away from wrist relative to MCP joint\n        thumb_vector = thumb_tip - wrist\n        mcp_vector = thumb_mcp - wrist\n        if np.linalg.norm(thumb_vector) > np.linalg.norm(mcp_vector) * 0.8:\n            extended_count += 1\n\n        # Check other fingers\n        finger_tips = [8, 12, 16, 20]  # Index, middle, ring, pinky tips\n        finger_mcp = [5, 9, 13, 17]    # Corresponding MCP joints\n\n        for tip_idx, mcp_idx in zip(finger_tips, finger_mcp):\n            tip = np.array(landmarks[tip_idx])\n            mcp = np.array(landmarks[mcp_idx])\n            wrist_to_mcp = mcp - wrist\n\n            # Finger is extended if tip is away from wrist in the same direction as MCP\n            finger_extension = tip - mcp\n            if np.dot(finger_extension, wrist_to_mcp) > 0:\n                extended_count += 1\n\n        return extended_count\n\n    def _classify_body_gesture(self, pose_data: Dict) -> List[Dict]:\n        \"\"\"\n        Classify body-based gestures from pose landmarks\n        \"\"\"\n        gestures = []\n        landmarks = pose_data['landmarks']\n\n        # Check for waving gesture (arm movement)\n        if self._is_waving_gesture(landmarks):\n            gestures.append({\n                'type': 'waving',\n                'confidence': 0.8,\n                'details': {'body_part': 'arm'}\n            })\n\n        # Check for pointing gesture\n        if self._is_pointing_gesture(landmarks):\n            gestures.append({\n                'type': 'pointing',\n                'confidence': 0.75,\n                'details': {'body_part': 'arm'}\n            })\n\n        return gestures\n\n    def _is_waving_gesture(self, landmarks: List[Tuple]) -> bool:\n        \"\"\"\n        Detect waving gesture based on arm movement patterns\n        \"\"\"\n        # This would require tracking movement over time\n        # For static detection, look for arm position\n        # Placeholder implementation\n        return False\n\n    def _is_pointing_gesture(self, landmarks: List[Tuple]) -> bool:\n        \"\"\"\n        Detect pointing gesture based on arm and hand position\n        \"\"\"\n        # Placeholder implementation\n        return False\n\nclass VisualAttentionSystem:\n    \"\"\"\n    System for detecting and responding to visual attention cues\n    \"\"\"\n    def __init__(self):\n        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n        self.gaze_detection_enabled = True\n        self.attention_threshold = 0.5\n\n    def detect_attention(self, frame: np.ndarray) -> Dict:\n        \"\"\"\n        Detect where human is looking/attending\n        \"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)\n\n        attention_data = []\n\n        for (x, y, w, h) in faces:\n            face_roi = frame[y:y+h, x:x+w]\n\n            # Estimate gaze direction (simplified)\n            gaze_direction = self._estimate_gaze_direction(face_roi, (x, y, w, h))\n\n            attention_info = {\n                'face_bounding_box': (x, y, w, h),\n                'gaze_direction': gaze_direction,\n                'attention_score': self._calculate_attention_score(gaze_direction)\n            }\n\n            attention_data.append(attention_info)\n\n        return {\n            'attention_targets': attention_data,\n            'timestamp': time.time()\n        }\n\n    def _estimate_gaze_direction(self, face_roi, bounding_box) -> Tuple[float, float]:\n        \"\"\"\n        Estimate gaze direction from face ROI\n        \"\"\"\n        # In practice, this would use specialized gaze estimation models\n        # For now, return a placeholder\n        return (0.0, 0.0)  # Looking straight ahead\n\n    def _calculate_attention_score(self, gaze_direction) -> float:\n        \"\"\"\n        Calculate attention score based on gaze direction\n        \"\"\"\n        # Simplified attention scoring\n        # In practice, consider head pose, eye contact, etc.\n        gaze_x, gaze_y = gaze_direction\n        attention_score = max(0.0, 1.0 - abs(gaze_x) - abs(gaze_y))\n        return min(1.0, attention_score)\n```\n\n## Social Robotics and Collaborative Behaviors\n\n### Social Norms and Collaborative Robotics\n\nSocial robotics involves creating robots that can interact with humans in socially appropriate ways:\n\n```python\nimport math\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple\n\nclass SocialContext(Enum):\n    FORMAL = \"formal\"\n    INFORMAL = \"informal\"\n    COLLABORATIVE = \"collaborative\"\n    SERVICE = \"service\"\n\n@dataclass\nclass SocialState:\n    \"\"\"\n    Represents the social state in human-robot interaction\n    \"\"\"\n    context: SocialContext\n    social_distance: float  # Personal space in meters\n    eye_contact_frequency: float\n    turn_taking_behavior: str\n    politeness_level: float\n\nclass SocialBehaviorEngine:\n    \"\"\"\n    Engine for generating socially appropriate robot behaviors\n    \"\"\"\n    def __init__(self):\n        self.current_social_state = SocialState(\n            context=SocialContext.INFORMAL,\n            social_distance=1.0,\n            eye_contact_frequency=0.5,\n            turn_taking_behavior=\"responsive\",\n            politeness_level=0.8\n        )\n        self.social_norms = self._load_social_norms()\n\n    def _load_social_norms(self) -> Dict:\n        \"\"\"\n        Load social norms for different contexts\n        \"\"\"\n        return {\n            SocialContext.FORMAL: {\n                'personal_distance': 1.2,\n                'eye_contact_duration': 0.3,\n                'response_time': 1.0,\n                'formality_level': 0.9\n            },\n            SocialContext.INFORMAL: {\n                'personal_distance': 0.8,\n                'eye_contact_duration': 0.5,\n                'response_time': 0.5,\n                'formality_level': 0.3\n            },\n            SocialContext.COLLABORATIVE: {\n                'personal_distance': 0.5,\n                'eye_contact_duration': 0.7,\n                'response_time': 0.3,\n                'formality_level': 0.5\n            },\n            SocialContext.SERVICE: {\n                'personal_distance': 1.0,\n                'eye_contact_duration': 0.4,\n                'response_time': 0.8,\n                'formality_level': 0.7\n            }\n        }\n\n    def adapt_to_context(self, context: SocialContext):\n        \"\"\"\n        Adapt robot behavior to the current social context\n        \"\"\"\n        norms = self.social_norms[context]\n\n        self.current_social_state.context = context\n        self.current_social_state.social_distance = norms['personal_distance']\n        self.current_social_state.politeness_level = norms['formality_level']\n\n    def generate_social_response(self, human_action: str, context: Dict) -> str:\n        \"\"\"\n        Generate socially appropriate response to human action\n        \"\"\"\n        # Determine appropriate response based on social context and human action\n        if human_action == \"greeting\":\n            return self._generate_greeting_response(context)\n        elif human_action == \"request\":\n            return self._generate_request_response(context)\n        elif human_action == \"command\":\n            return self._generate_command_response(context)\n        elif human_action == \"question\":\n            return self._generate_question_response(context)\n        else:\n            return self._generate_default_response(context)\n\n    def _generate_greeting_response(self, context: Dict) -> str:\n        \"\"\"\n        Generate greeting response based on social context\n        \"\"\"\n        formality = self.current_social_state.politeness_level\n\n        if formality > 0.7:\n            return \"Good day! How may I assist you?\"\n        elif formality > 0.4:\n            return \"Hello! Nice to meet you.\"\n        else:\n            return \"Hi there! What's up?\"\n\n    def _generate_request_response(self, context: Dict) -> str:\n        \"\"\"\n        Generate response to request based on social context\n        \"\"\"\n        formality = self.current_social_state.politeness_level\n\n        if formality > 0.7:\n            return \"Certainly, I would be happy to help with that.\"\n        elif formality > 0.4:\n            return \"Sure, I can do that for you.\"\n        else:\n            return \"Yeah, I can help with that!\"\n\n    def _generate_command_response(self, context: Dict) -> str:\n        \"\"\"\n        Generate response to command based on social context\n        \"\"\"\n        formality = self.current_social_state.politeness_level\n\n        if formality > 0.7:\n            return \"Understood. I will proceed with the requested action.\"\n        elif formality > 0.4:\n            return \"Okay, I'll do that right away.\"\n        else:\n            return \"Got it! Doing that now.\"\n\n    def _generate_question_response(self, context: Dict) -> str:\n        \"\"\"\n        Generate response to question based on social context\n        \"\"\"\n        formality = self.current_social_state.politeness_level\n\n        if formality > 0.7:\n            return \"That's an excellent question. Let me provide you with the information.\"\n        elif formality > 0.4:\n            return \"Let me answer that for you.\"\n        else:\n            return \"Good question! Here's what I know about that.\"\n\n    def _generate_default_response(self, context: Dict) -> str:\n        \"\"\"\n        Generate default response when action type is unknown\n        \"\"\"\n        return \"I'm here to help. Could you please clarify what you need?\"\n\nclass CollaborativeTaskManager:\n    \"\"\"\n    Manages collaborative tasks between humans and robots\n    \"\"\"\n    def __init__(self):\n        self.active_tasks = []\n        self.human_capabilities = {}\n        self.robot_capabilities = {}\n        self.task_allocation_strategy = \"complementary\"\n\n    def assign_task(self, task_description: Dict, human_agent: str, robot_agent: str):\n        \"\"\"\n        Assign task to human and robot agents based on capabilities\n        \"\"\"\n        # Analyze task requirements\n        task_requirements = self._analyze_task_requirements(task_description)\n\n        # Evaluate agent capabilities\n        human_fit = self._evaluate_agent_fit(human_agent, task_requirements)\n        robot_fit = self._evaluate_agent_fit(robot_agent, task_requirements)\n\n        # Allocate subtasks based on complementary strengths\n        allocation = self._allocate_subtasks(task_description, human_fit, robot_fit)\n\n        # Create collaborative task\n        collaborative_task = {\n            'id': len(self.active_tasks),\n            'original_task': task_description,\n            'allocation': allocation,\n            'status': 'assigned',\n            'collaboration_type': 'complementary'\n        }\n\n        self.active_tasks.append(collaborative_task)\n        return collaborative_task\n\n    def _analyze_task_requirements(self, task_description: Dict) -> Dict:\n        \"\"\"\n        Analyze requirements of a collaborative task\n        \"\"\"\n        return {\n            'physical_manipulation': task_description.get('needs_manipulation', False),\n            'cognitive_reasoning': task_description.get('needs_reasoning', False),\n            'spatial_navigation': task_description.get('needs_navigation', False),\n            'social_interaction': task_description.get('needs_interaction', False),\n            'precision_required': task_description.get('precision_required', False),\n            'strength_required': task_description.get('strength_required', False)\n        }\n\n    def _evaluate_agent_fit(self, agent_id: str, task_requirements: Dict) -> Dict:\n        \"\"\"\n        Evaluate how well an agent fits the task requirements\n        \"\"\"\n        if agent_id.startswith('human'):\n            capabilities = {\n                'manipulation_skill': 0.8,\n                'cognitive_ability': 0.9,\n                'social_skills': 0.9,\n                'spatial_reasoning': 0.8,\n                'precision': 0.9,\n                'strength': 0.5\n            }\n        else:  # robot\n            capabilities = {\n                'manipulation_skill': 0.9,\n                'cognitive_ability': 0.7,\n                'social_skills': 0.3,\n                'spatial_reasoning': 0.9,\n                'precision': 0.95,\n                'strength': 0.8\n            }\n\n        # Calculate fit scores\n        fit_scores = {}\n        for requirement, needed in task_requirements.items():\n            if needed:\n                capability_key = requirement.replace('needs_', '').replace('required', '').replace(' ', '_')\n                if capability_key in capabilities:\n                    fit_scores[capability_key] = capabilities[capability_key]\n                else:\n                    fit_scores[requirement] = 0.5  # default medium fit\n\n        return fit_scores\n\n    def _allocate_subtasks(self, task_description: Dict, human_fit: Dict, robot_fit: Dict) -> Dict:\n        \"\"\"\n        Allocate subtasks based on agent capabilities\n        \"\"\"\n        allocation = {\n            'human_subtasks': [],\n            'robot_subtasks': [],\n            'joint_subtasks': []\n        }\n\n        # Simple allocation strategy: assign to best-fitting agent\n        requirements = self._analyze_task_requirements(task_description)\n\n        for requirement, needed in requirements.items():\n            if not needed:\n                continue\n\n            capability_key = requirement.replace('needs_', '').replace('required', '').replace(' ', '_')\n            human_capability = human_fit.get(capability_key, 0.5)\n            robot_capability = robot_fit.get(capability_key, 0.5)\n\n            if human_capability > robot_capability + 0.1:\n                allocation['human_subtasks'].append(requirement)\n            elif robot_capability > human_capability + 0.1:\n                allocation['robot_subtasks'].append(requirement)\n            else:\n                # Similar capabilities - consider joint execution\n                allocation['joint_subtasks'].append(requirement)\n\n        return allocation\n\n    def monitor_collaboration(self, task_id: int) -> Dict:\n        \"\"\"\n        Monitor the progress of a collaborative task\n        \"\"\"\n        task = next((t for t in self.active_tasks if t['id'] == task_id), None)\n        if not task:\n            return {'error': 'Task not found'}\n\n        # Monitor task progress\n        progress = self._evaluate_task_progress(task)\n\n        # Check for collaboration issues\n        collaboration_issues = self._detect_collaboration_issues(task)\n\n        return {\n            'task_id': task_id,\n            'progress': progress,\n            'issues': collaboration_issues,\n            'recommendations': self._generate_collaboration_recommendations(collaboration_issues)\n        }\n\n    def _evaluate_task_progress(self, task: Dict) -> float:\n        \"\"\"\n        Evaluate the progress of a collaborative task\n        \"\"\"\n        # Placeholder implementation\n        return 0.5  # 50% progress\n\n    def _detect_collaboration_issues(self, task: Dict) -> List[str]:\n        \"\"\"\n        Detect potential issues in human-robot collaboration\n        \"\"\"\n        issues = []\n\n        # Check for communication breakdown\n        if task.get('last_communication_time', 0) < time.time() - 30:  # 30 seconds\n            issues.append('communication_breakdown')\n\n        # Check for task conflicts\n        if task.get('conflicting_actions', 0) > 0:\n            issues.append('action_conflict')\n\n        # Check for safety violations\n        if task.get('safety_violations', 0) > 0:\n            issues.append('safety_violation')\n\n        return issues\n\n    def _generate_collaboration_recommendations(self, issues: List[str]) -> List[str]:\n        \"\"\"\n        Generate recommendations to improve collaboration\n        \"\"\"\n        recommendations = []\n\n        for issue in issues:\n            if issue == 'communication_breakdown':\n                recommendations.append('Initiate check-in communication')\n            elif issue == 'action_conflict':\n                recommendations.append('Clarify task responsibilities')\n            elif issue == 'safety_violation':\n                recommendations.append('Reassess safety protocols')\n\n        return recommendations\n```\n\n## Safety and Trust in HRI\n\n### Safety Protocols and Trust Building\n\nSafety and trust are fundamental to successful human-robot interaction:\n\n```python\nclass SafetyManager:\n    \"\"\"\n    Manages safety protocols in Human-Robot Interaction\n    \"\"\"\n    def __init__(self):\n        self.safety_zones = {\n            'dangerous': 0.3,    # meters - immediate danger\n            'caution': 0.8,      # meters - potential risk\n            'safe': 1.5          # meters - safe distance\n        }\n        self.emergency_stop_active = False\n        self.safety_violations = []\n        self.safety_thresholds = {\n            'velocity': 1.0,      # m/s\n            'acceleration': 2.0,  # m/s²\n            'force': 50.0,        # Newtons\n            'torque': 20.0        # Nm\n        }\n\n    def check_safety(self, human_position: Tuple[float, float, float],\n                    robot_position: Tuple[float, float, float]) -> Dict:\n        \"\"\"\n        Check safety conditions between human and robot\n        \"\"\"\n        distance = self._calculate_distance(human_position, robot_position)\n\n        safety_status = {\n            'distance': distance,\n            'zone': self._determine_safety_zone(distance),\n            'is_safe': distance > self.safety_zones['caution'],\n            'violations': [],\n            'actions_needed': []\n        }\n\n        # Check for immediate danger\n        if distance < self.safety_zones['dangerous']:\n            safety_status['is_safe'] = False\n            safety_status['violations'].append('TOO_CLOSE')\n            safety_status['actions_needed'].append('EMERGENCY_STOP')\n\n        # Check velocity and acceleration limits\n        # (would check actual robot velocities in real implementation)\n\n        return safety_status\n\n    def _calculate_distance(self, pos1: Tuple[float, float, float],\n                           pos2: Tuple[float, float, float]) -> float:\n        \"\"\"\n        Calculate Euclidean distance between two 3D positions\n        \"\"\"\n        dx = pos1[0] - pos2[0]\n        dy = pos1[1] - pos2[1]\n        dz = pos1[2] - pos2[2]\n        return math.sqrt(dx*dx + dy*dy + dz*dz)\n\n    def _determine_safety_zone(self, distance: float) -> str:\n        \"\"\"\n        Determine which safety zone a distance falls into\n        \"\"\"\n        if distance < self.safety_zones['dangerous']:\n            return 'dangerous'\n        elif distance < self.safety_zones['caution']:\n            return 'caution'\n        else:\n            return 'safe'\n\n    def trigger_emergency_stop(self, reason: str = \"Safety violation\"):\n        \"\"\"\n        Trigger emergency stop for safety\n        \"\"\"\n        self.emergency_stop_active = True\n        self.safety_violations.append({\n            'timestamp': time.time(),\n            'reason': reason,\n            'type': 'EMERGENCY_STOP'\n        })\n        print(f\"EMERGENCY STOP: {reason}\")\n\n    def clear_emergency_stop(self):\n        \"\"\"\n        Clear emergency stop condition\n        \"\"\"\n        self.emergency_stop_active = False\n\n    def log_safety_violation(self, violation_type: str, details: str):\n        \"\"\"\n        Log safety violation for analysis\n        \"\"\"\n        violation = {\n            'timestamp': time.time(),\n            'type': violation_type,\n            'details': details,\n            'severity': self._assess_violation_severity(violation_type)\n        }\n        self.safety_violations.append(violation)\n\n    def _assess_violation_severity(self, violation_type: str) -> str:\n        \"\"\"\n        Assess severity of safety violation\n        \"\"\"\n        high_severity = ['TOO_CLOSE', 'EMERGENCY_STOP', 'COLLISION']\n        medium_severity = ['VELOCITY_EXCEEDED', 'TORQUE_EXCEEDED']\n\n        if violation_type in high_severity:\n            return 'high'\n        elif violation_type in medium_severity:\n            return 'medium'\n        else:\n            return 'low'\n\nclass TrustManager:\n    \"\"\"\n    Manages trust between humans and robots\n    \"\"\"\n    def __init__(self):\n        self.trust_scores = {}  # Per human user\n        self.trust_decay_rate = 0.01  # Trust decreases over time without positive interactions\n        self.trust_update_weights = {\n            'success': 0.1,\n            'failure': -0.2,\n            'safety': 0.15,\n            'reliability': 0.05\n        }\n        self.trust_history = {}\n\n    def initialize_user_trust(self, user_id: str, initial_trust: float = 0.5):\n        \"\"\"\n        Initialize trust for a new user\n        \"\"\"\n        self.trust_scores[user_id] = initial_trust\n        self.trust_history[user_id] = []\n\n    def update_trust(self, user_id: str, interaction_type: str, outcome: str,\n                    confidence: float = 1.0):\n        \"\"\"\n        Update trust based on interaction outcome\n        \"\"\"\n        if user_id not in self.trust_scores:\n            self.initialize_user_trust(user_id)\n\n        # Determine trust update value\n        update_value = self._calculate_trust_update(interaction_type, outcome, confidence)\n\n        # Apply update with bounds checking\n        current_trust = self.trust_scores[user_id]\n        new_trust = max(0.0, min(1.0, current_trust + update_value))\n\n        # Log the update\n        self.trust_history[user_id].append({\n            'timestamp': time.time(),\n            'interaction_type': interaction_type,\n            'outcome': outcome,\n            'update_value': update_value,\n            'previous_trust': current_trust,\n            'new_trust': new_trust\n        })\n\n        # Keep history to reasonable size\n        if len(self.trust_history[user_id]) > 100:\n            self.trust_history[user_id] = self.trust_history[user_id][-50:]\n\n        self.trust_scores[user_id] = new_trust\n\n    def _calculate_trust_update(self, interaction_type: str, outcome: str,\n                              confidence: float) -> float:\n        \"\"\"\n        Calculate trust update based on interaction and outcome\n        \"\"\"\n        base_update = 0.0\n\n        if outcome == 'success':\n            base_update = self.trust_update_weights['success']\n        elif outcome == 'failure':\n            base_update = self.trust_update_weights['failure']\n        elif outcome == 'safe':\n            base_update = self.trust_update_weights['safety']\n\n        # Apply confidence weighting\n        return base_update * confidence\n\n    def get_trust_score(self, user_id: str) -> float:\n        \"\"\"\n        Get current trust score for a user\n        \"\"\"\n        if user_id not in self.trust_scores:\n            self.initialize_user_trust(user_id)\n\n        # Apply decay since last interaction\n        if user_id in self.trust_history and self.trust_history[user_id]:\n            last_interaction_time = self.trust_history[user_id][-1]['timestamp']\n            time_since = time.time() - last_interaction_time\n            decay_factor = self.trust_decay_rate * time_since\n            decayed_trust = max(0.0, self.trust_scores[user_id] - decay_factor)\n            return decayed_trust\n\n        return self.trust_scores[user_id]\n\n    def adapt_behavior_to_trust(self, user_id: str, base_behavior: Dict) -> Dict:\n        \"\"\"\n        Adapt robot behavior based on user's trust level\n        \"\"\"\n        trust_level = self.get_trust_score(user_id)\n\n        adapted_behavior = base_behavior.copy()\n\n        if trust_level < 0.3:\n            # Low trust: Be extra careful and transparent\n            adapted_behavior.update({\n                'explanation_frequency': 'high',\n                'autonomy_level': 'low',\n                'safety_margin': 'high',\n                'communication_style': 'formal'\n            })\n        elif trust_level < 0.7:\n            # Medium trust: Balanced approach\n            adapted_behavior.update({\n                'explanation_frequency': 'medium',\n                'autonomy_level': 'medium',\n                'safety_margin': 'medium',\n                'communication_style': 'polite'\n            })\n        else:\n            # High trust: More autonomous and natural\n            adapted_behavior.update({\n                'explanation_frequency': 'low',\n                'autonomy_level': 'high',\n                'safety_margin': 'standard',\n                'communication_style': 'friendly'\n            })\n\n        return adapted_behavior\n\n    def build_trust(self, user_id: str):\n        \"\"\"\n        Proactive trust-building measures\n        \"\"\"\n        # Provide clear explanations\n        self._provide_transparency(user_id)\n\n        # Demonstrate reliability\n        self._demonstrate_reliability(user_id)\n\n        # Ensure safety\n        self._ensure_safety(user_id)\n\n    def _provide_transparency(self, user_id: str):\n        \"\"\"\n        Provide transparency to build trust\n        \"\"\"\n        # This would involve explaining robot's intentions, actions, and decision-making\n        pass\n\n    def _demonstrate_reliability(self, user_id: str):\n        \"\"\"\n        Demonstrate consistent and reliable behavior\n        \"\"\"\n        # This would involve consistent performance and predictable behavior\n        pass\n\n    def _ensure_safety(self, user_id: str):\n        \"\"\"\n        Ensure safety to build trust\n        \"\"\"\n        # This would involve consistent safety behavior\n        pass\n```\n\n## HRI Evaluation and Metrics\n\n### Measuring HRI System Effectiveness\n\nEvaluating HRI systems requires comprehensive metrics that assess both technical performance and user experience:\n\n```python\nimport statistics\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple\n\nclass HRIEvaluator:\n    \"\"\"\n    Evaluates Human-Robot Interaction systems using various metrics\n    \"\"\"\n    def __init__(self):\n        self.interaction_logs = []\n        self.evaluation_metrics = {\n            'task_success_rate': [],\n            'interaction_time': [],\n            'user_satisfaction': [],\n            'trust_score': [],\n            'safety_incidents': [],\n            'communication_efficiency': []\n        }\n\n    def log_interaction(self, interaction_data: Dict):\n        \"\"\"\n        Log interaction data for evaluation\n        \"\"\"\n        interaction_data['timestamp'] = datetime.now().isoformat()\n        self.interaction_logs.append(interaction_data)\n\n        # Update metrics\n        self._update_metrics(interaction_data)\n\n    def _update_metrics(self, interaction_data: Dict):\n        \"\"\"\n        Update evaluation metrics based on interaction data\n        \"\"\"\n        if 'task_success' in interaction_data:\n            self.evaluation_metrics['task_success_rate'].append(\n                1.0 if interaction_data['task_success'] else 0.0\n            )\n\n        if 'interaction_duration' in interaction_data:\n            self.evaluation_metrics['interaction_time'].append(\n                interaction_data['interaction_duration']\n            )\n\n        if 'user_satisfaction' in interaction_data:\n            self.evaluation_metrics['user_satisfaction'].append(\n                interaction_data['user_satisfaction']\n            )\n\n        if 'trust_score' in interaction_data:\n            self.evaluation_metrics['trust_score'].append(\n                interaction_data['trust_score']\n            )\n\n        if 'safety_violation' in interaction_data:\n            self.evaluation_metrics['safety_incidents'].append(\n                1.0 if interaction_data['safety_violation'] else 0.0\n            )\n\n        if 'communication_success' in interaction_data:\n            self.evaluation_metrics['communication_efficiency'].append(\n                1.0 if interaction_data['communication_success'] else 0.0\n            )\n\n    def generate_evaluation_report(self) -> Dict:\n        \"\"\"\n        Generate comprehensive evaluation report\n        \"\"\"\n        report = {\n            'summary': self._calculate_summary_metrics(),\n            'detailed_metrics': self._calculate_detailed_metrics(),\n            'trends': self._analyze_trends(),\n            'recommendations': self._generate_recommendations()\n        }\n\n        return report\n\n    def _calculate_summary_metrics(self) -> Dict:\n        \"\"\"\n        Calculate summary metrics for the evaluation\n        \"\"\"\n        summary = {}\n\n        for metric_name, values in self.evaluation_metrics.items():\n            if values:\n                summary[metric_name] = {\n                    'mean': statistics.mean(values),\n                    'median': statistics.median(values) if values else 0,\n                    'std_dev': statistics.stdev(values) if len(values) > 1 else 0,\n                    'min': min(values),\n                    'max': max(values),\n                    'count': len(values)\n                }\n            else:\n                summary[metric_name] = {\n                    'mean': 0, 'median': 0, 'std_dev': 0, 'min': 0, 'max': 0, 'count': 0\n                }\n\n        return summary\n\n    def _calculate_detailed_metrics(self) -> Dict:\n        \"\"\"\n        Calculate detailed metrics including advanced HRI measures\n        \"\"\"\n        detailed_metrics = {}\n\n        # Task efficiency\n        if (self.evaluation_metrics['task_success_rate'] and\n            self.evaluation_metrics['interaction_time']):\n            success_rate = statistics.mean(self.evaluation_metrics['task_success_rate'])\n            avg_time = statistics.mean(self.evaluation_metrics['interaction_time'])\n            detailed_metrics['task_efficiency'] = success_rate / (avg_time + 1)  # +1 to avoid division by zero\n\n        # User experience score\n        if self.evaluation_metrics['user_satisfaction']:\n            detailed_metrics['user_experience_score'] = statistics.mean(self.evaluation_metrics['user_satisfaction'])\n\n        # Safety performance\n        if self.evaluation_metrics['safety_incidents']:\n            safety_rate = 1 - statistics.mean(self.evaluation_metrics['safety_incidents'])\n            detailed_metrics['safety_performance'] = safety_rate\n\n        # Communication effectiveness\n        if self.evaluation_metrics['communication_efficiency']:\n            detailed_metrics['communication_effectiveness'] = statistics.mean(self.evaluation_metrics['communication_efficiency'])\n\n        return detailed_metrics\n\n    def _analyze_trends(self) -> Dict:\n        \"\"\"\n        Analyze trends in the interaction data\n        \"\"\"\n        trends = {}\n\n        # Performance trend over time\n        if len(self.interaction_logs) > 10:  # Need sufficient data for trend analysis\n            recent_logs = self.interaction_logs[-20:]  # Last 20 interactions\n            older_logs = self.interaction_logs[:20]   # First 20 interactions (if available)\n\n            recent_success = [log.get('task_success', False) for log in recent_logs]\n            older_success = [log.get('task_success', False) for log in older_logs[:len(recent_success)]]\n\n            recent_rate = sum(recent_success) / len(recent_success) if recent_success else 0\n            older_rate = sum(older_success) / len(older_success) if older_success else 0\n\n            trends['performance_trend'] = recent_rate - older_rate\n\n        return trends\n\n    def _generate_recommendations(self) -> List[str]:\n        \"\"\"\n        Generate recommendations based on evaluation results\n        \"\"\"\n        recommendations = []\n\n        summary = self._calculate_summary_metrics()\n\n        # Task success recommendations\n        success_rate = summary.get('task_success_rate', {}).get('mean', 0)\n        if success_rate < 0.8:\n            recommendations.append(\"Task success rate is below 80%. Consider improving task planning or execution.\")\n\n        # User satisfaction recommendations\n        satisfaction = summary.get('user_satisfaction', {}).get('mean', 0)\n        if satisfaction < 0.7:\n            recommendations.append(\"User satisfaction is below 70%. Investigate interaction design issues.\")\n\n        # Safety recommendations\n        safety_incidents = summary.get('safety_incidents', {}).get('mean', 0)\n        if safety_incidents > 0.1:  # More than 10% safety incidents\n            recommendations.append(\"Safety incidents are above 10%. Review safety protocols.\")\n\n        # Communication efficiency\n        comm_efficiency = summary.get('communication_efficiency', {}).get('mean', 0)\n        if comm_efficiency < 0.8:\n            recommendations.append(\"Communication efficiency is below 80%. Improve natural language processing or gesture recognition.\")\n\n        return recommendations\n\n    def calculate_user_engagement(self, user_id: str) -> Dict:\n        \"\"\"\n        Calculate engagement metrics for a specific user\n        \"\"\"\n        user_interactions = [log for log in self.interaction_logs if log.get('user_id') == user_id]\n\n        if not user_interactions:\n            return {'error': 'No interactions found for user'}\n\n        engagement_metrics = {\n            'total_interactions': len(user_interactions),\n            'average_duration': statistics.mean([log.get('interaction_duration', 0) for log in user_interactions]) if user_interactions else 0,\n            'success_rate': sum([1 for log in user_interactions if log.get('task_success', False)]) / len(user_interactions) if user_interactions else 0,\n            'last_interaction': max([log.get('timestamp', '') for log in user_interactions]) if user_interactions else None\n        }\n\n        return engagement_metrics\n\nclass HRIUserStudyFramework:\n    \"\"\"\n    Framework for conducting user studies in HRI\n    \"\"\"\n    def __init__(self):\n        self.study_protocols = {}\n        self.participant_data = {}\n        self.ethical_considerations = {\n            'informed_consent': True,\n            'privacy_protection': True,\n            'right_to_withdraw': True,\n            'data_anonymization': True\n        }\n\n    def design_study(self, study_type: str, objectives: List[str],\n                    participant_criteria: Dict) -> str:\n        \"\"\"\n        Design a user study for HRI evaluation\n        \"\"\"\n        study_id = f\"study_{len(self.study_protocols) + 1}\"\n\n        protocol = {\n            'study_id': study_id,\n            'type': study_type,\n            'objectives': objectives,\n            'criteria': participant_criteria,\n            'tasks': self._generate_appropriate_tasks(study_type),\n            'metrics': self._determine_evaluation_metrics(study_type),\n            'duration': self._estimate_study_duration(study_type),\n            'ethical_approval': True  # Should be verified in practice\n        }\n\n        self.study_protocols[study_id] = protocol\n        return study_id\n\n    def _generate_appropriate_tasks(self, study_type: str) -> List[Dict]:\n        \"\"\"\n        Generate appropriate tasks for the study type\n        \"\"\"\n        if study_type == 'usability':\n            return [\n                {'name': 'navigation_task', 'description': 'Navigate robot to target location'},\n                {'name': 'manipulation_task', 'description': 'Command robot to pick up object'},\n                {'name': 'communication_task', 'description': 'Interact with robot using speech'}\n            ]\n        elif study_type == 'acceptance':\n            return [\n                {'name': 'long_term_interaction', 'description': 'Interact with robot over extended period'},\n                {'name': 'trust_building', 'description': 'Perform tasks requiring trust in robot'},\n                {'name': 'social_interaction', 'description': 'Engage in social behaviors with robot'}\n            ]\n        else:  # general\n            return [\n                {'name': 'basic_interaction', 'description': 'Perform basic robot commands'},\n                {'name': 'problem_solving', 'description': 'Solve problems with robot assistance'},\n                {'name': 'collaboration', 'description': 'Work together on tasks'}\n            ]\n\n    def _determine_evaluation_metrics(self, study_type: str) -> List[str]:\n        \"\"\"\n        Determine appropriate evaluation metrics for the study type\n        \"\"\"\n        base_metrics = ['task_success_rate', 'interaction_time', 'user_satisfaction']\n\n        if study_type == 'usability':\n            return base_metrics + ['error_rate', 'learning_curve', 'system_usability_scale']\n        elif study_type == 'acceptance':\n            return base_metrics + ['trust_score', 'usage_frequency', 'willingness_to_recommend']\n        else:  # general\n            return base_metrics + ['safety_incidents', 'communication_efficiency']\n\n    def _estimate_study_duration(self, study_type: str) -> int:\n        \"\"\"\n        Estimate study duration in minutes\n        \"\"\"\n        duration_map = {\n            'usability': 45,\n            'acceptance': 120,  # Longer for trust building\n            'general': 30\n        }\n        return duration_map.get(study_type, 30)\n\n    def conduct_study(self, study_id: str, participants: List[str]) -> Dict:\n        \"\"\"\n        Conduct the user study\n        \"\"\"\n        if study_id not in self.study_protocols:\n            return {'error': 'Study not found'}\n\n        study_protocol = self.study_protocols[study_id]\n        results = {\n            'study_id': study_id,\n            'protocol': study_protocol,\n            'participants': participants,\n            'interactions': [],\n            'results': {}\n        }\n\n        # In a real implementation, this would run the actual study\n        # For now, we'll simulate the process\n        for participant in participants:\n            participant_interactions = self._simulate_participant_interactions(\n                participant, study_protocol['tasks']\n            )\n            results['interactions'].extend(participant_interactions)\n\n        # Analyze results\n        evaluator = HRIEvaluator()\n        for interaction in results['interactions']:\n            evaluator.log_interaction(interaction)\n\n        results['results'] = evaluator.generate_evaluation_report()\n\n        return results\n\n    def _simulate_participant_interactions(self, participant_id: str, tasks: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Simulate participant interactions for study (placeholder)\n        \"\"\"\n        import random\n\n        interactions = []\n        for task in tasks:\n            interaction = {\n                'user_id': participant_id,\n                'task_name': task['name'],\n                'task_description': task['description'],\n                'interaction_duration': random.uniform(30, 300),  # 30 seconds to 5 minutes\n                'task_success': random.choice([True, False, True, True]),  # 75% success rate\n                'user_satisfaction': random.uniform(0.5, 1.0),\n                'trust_score': random.uniform(0.3, 1.0),\n                'safety_violation': random.choice([False, False, False, True]),  # 25% safety incidents\n                'communication_success': random.choice([True, True, False, True])  # 75% communication success\n            }\n            interactions.append(interaction)\n\n        return interactions\n```\n\n## Implementation Examples\n\n### Complete HRI System Integration\n\n```python\nclass CompleteHRIController:\n    \"\"\"\n    Complete Human-Robot Interaction system integrating all components\n    \"\"\"\n    def __init__(self):\n        # Core HRI components\n        self.multimodal_fusion = MultimodalFusion()\n        self.dialogue_manager = DialogueManager()\n        self.gesture_recognizer = GestureRecognitionSystem()\n        self.visual_attention = VisualAttentionSystem()\n        self.social_engine = SocialBehaviorEngine()\n        self.collaboration_manager = CollaborativeTaskManager()\n        self.safety_manager = SafetyManager()\n        self.trust_manager = TrustManager()\n        self.evaluator = HRIEvaluator()\n\n        # System state\n        self.current_user = None\n        self.interaction_mode = 'idle'\n        self.robot_state = 'ready'\n\n    def process_human_input(self, input_type: str, input_data: any, user_id: str = None):\n        \"\"\"\n        Process input from human user through multimodal system\n        \"\"\"\n        if user_id:\n            self.current_user = user_id\n\n        # Create interaction input based on type\n        if input_type == 'speech':\n            modality_input = InteractionInput(\n                modality=ModalityType.SPEECH,\n                data=input_data,\n                timestamp=time.time(),\n                confidence=0.8,\n                source_id='user_mic'\n            )\n        elif input_type == 'gesture':\n            modality_input = InteractionInput(\n                modality=ModalityType.GESTURE,\n                data=input_data,\n                timestamp=time.time(),\n                confidence=0.9,\n                source_id='camera'\n            )\n        elif input_type == 'proximity':\n            modality_input = InteractionInput(\n                modality=ModalityType.PROXIMITY,\n                data=input_data,\n                timestamp=time.time(),\n                confidence=1.0,\n                source_id='distance_sensor'\n            )\n        else:\n            return {'error': 'Unknown input type'}\n\n        # Add to fusion system\n        self.multimodal_fusion.add_input(modality_input)\n\n        # Fuse inputs to get interpretation\n        fused_interpretation = self.multimodal_fusion.fuse_inputs()\n\n        if fused_interpretation:\n            # Generate appropriate response\n            response = self._generate_response(fused_interpretation, user_id)\n\n            # Log interaction for evaluation\n            self.evaluator.log_interaction({\n                'user_id': user_id,\n                'input_type': input_type,\n                'interpretation': fused_interpretation,\n                'response': response,\n                'timestamp': time.time()\n            })\n\n            return response\n        else:\n            return {'error': 'Could not interpret input'}\n\n    def _generate_response(self, interpretation: Dict, user_id: str):\n        \"\"\"\n        Generate response based on fused interpretation\n        \"\"\"\n        # Check safety first\n        safety_check = self._check_safety(user_id)\n        if not safety_check['is_safe']:\n            return {'action': 'safety_protocols', 'details': safety_check}\n\n        # Process natural language if speech was involved\n        speech_data = interpretation['interpretation'].get(ModalityType.SPEECH)\n        if speech_data:\n            text = speech_data['data']['text']\n            dialogue_response = self.dialogue_manager.process_user_input(text)\n\n            # Update trust based on successful communication\n            self.trust_manager.update_trust(user_id, 'communication', 'success')\n\n            return {\n                'action': 'speak',\n                'content': dialogue_response,\n                'trust_score': self.trust_manager.get_trust_score(user_id)\n            }\n\n        # Process gestures\n        gesture_data = interpretation['interpretation'].get(ModalityType.GESTURE)\n        if gesture_data:\n            gesture_type = gesture_data['data']['gesture_type']\n            social_response = self.social_engine.generate_social_response(gesture_type, {})\n\n            return {\n                'action': 'gesture_response',\n                'content': social_response,\n                'gesture_recognized': gesture_type\n            }\n\n        # Default response\n        return {'action': 'acknowledge', 'content': 'Input received'}\n\n    def _check_safety(self, user_id: str = None):\n        \"\"\"\n        Check safety conditions before responding\n        \"\"\"\n        # In a real system, this would check actual distances and positions\n        # For simulation, we'll return a safe status\n        return {\n            'is_safe': True,\n            'distance': 2.0,  # meters\n            'zone': 'safe',\n            'violations': []\n        }\n\n    def set_collaboration_task(self, task_description: Dict, human_agent: str = None):\n        \"\"\"\n        Set up a collaborative task between human and robot\n        \"\"\"\n        robot_agent = 'robot_1'  # Default robot identifier\n        human_agent = human_agent or self.current_user or 'default_human'\n\n        task = self.collaboration_manager.assign_task(\n            task_description, human_agent, robot_agent\n        )\n\n        return task\n\n    def monitor_interaction(self):\n        \"\"\"\n        Monitor ongoing interaction for issues\n        \"\"\"\n        monitoring_data = {\n            'safety_status': self.safety_manager.check_safety(\n                (0, 0, 0), (1, 1, 1)  # Placeholder positions\n            ),\n            'trust_level': self.trust_manager.get_trust_score(self.current_user or 'default_user'),\n            'interaction_mode': self.interaction_mode,\n            'robot_state': self.robot_state\n        }\n\n        return monitoring_data\n\n    def run_hri_system(self):\n        \"\"\"\n        Run the complete HRI system (simulation)\n        \"\"\"\n        print(\"Starting Human-Robot Interaction System...\")\n        print(\"System components initialized:\")\n        print(f\"- Multimodal Fusion: {type(self.multimodal_fusion).__name__}\")\n        print(f\"- Dialogue Manager: {type(self.dialogue_manager).__name__}\")\n        print(f\"- Gesture Recognition: {type(self.gesture_recognizer).__name__}\")\n        print(f\"- Social Engine: {type(self.social_engine).__name__}\")\n        print(f\"- Safety Manager: {type(self.safety_manager).__name__}\")\n        print(f\"- Trust Manager: {type(self.trust_manager).__name__}\")\n\n        # Simulate some interactions\n        print(\"\\nSimulating HRI interactions...\")\n\n        # Example 1: Speech interaction\n        speech_response = self.process_human_input(\n            'speech',\n            'Hello robot, can you please go to the kitchen?',\n            'user_1'\n        )\n        print(f\"Speech response: {speech_response}\")\n\n        # Example 2: Gesture interaction\n        gesture_response = self.process_human_input(\n            'gesture',\n            {'type': 'pointing', 'location': (2, 3, 0)},\n            'user_1'\n        )\n        print(f\"Gesture response: {gesture_response}\")\n\n        # Example 3: Set up collaboration\n        task_desc = {\n            'task_name': 'object_retrieval',\n            'needs_navigation': True,\n            'needs_manipulation': True,\n            'target_object': 'water_bottle',\n            'destination': 'table'\n        }\n        collaboration_task = self.set_collaboration_task(task_desc)\n        print(f\"Collaboration task: {collaboration_task}\")\n\n        # Monitor interaction\n        monitoring = self.monitor_interaction()\n        print(f\"Monitoring: {monitoring}\")\n\n        # Generate evaluation report\n        report = self.evaluator.generate_evaluation_report()\n        print(f\"Evaluation summary: {report['summary']}\")\n\n        print(\"\\nHRI System simulation completed.\")\n\n# Example usage\ndef main():\n    \"\"\"\n    Example of using the complete HRI system\n    \"\"\"\n    hri_system = CompleteHRIController()\n    hri_system.run_hri_system()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Summary\n\nHuman-Robot Interaction (HRI) is a critical field that enables effective collaboration between humans and robots. Key concepts include:\n\n- **Multimodal Interaction**: Combining speech, gesture, vision, and other modalities for natural communication\n- **Social Robotics**: Creating robots that behave appropriately in social contexts\n- **Safety and Trust**: Ensuring physical safety and building appropriate levels of human trust\n- **Collaborative Behaviors**: Enabling effective human-robot teamwork\n- **Evaluation Metrics**: Measuring the effectiveness of HRI systems\n\nSuccessful HRI systems require careful integration of multiple technologies and consideration of human factors, social norms, and safety requirements.\n\n## Exercises\n\n1. Implement a multimodal input fusion system that combines speech and gesture recognition.\n\n2. Design a dialogue manager that can handle natural language commands for robot control.\n\n3. Create a safety system that monitors human-robot proximity and prevents collisions.\n\n4. Develop a trust management system that adapts robot behavior based on user trust levels.\n\n5. Implement an evaluation framework for measuring the effectiveness of HRI systems.",
    "module": "Module 2: Digital Twin (Gazebo + Unity)",
    "chapter": "human robot interaction hri.md",
    "path": "/docs/module-2/2.3-human-robot-interaction-hri"
  },
  {
    "id": "module-2/2.4-digital-twin-validation",
    "title": "Chapter 2.4 – Digital Twin Validation",
    "content": "\n# Chapter 2.4 – Digital Twin Validation\n\n## Learning Objectives\n- Understand sim-to-real gaps in humanoid robotics\n- Implement performance profiling methods for digital twins\n- Ensure determinism and repeatability in simulation\n- Establish simulation-driven development workflows\n\n## Sim vs Real Gaps\n\nDigital twins face several challenges when bridging simulation and reality. Understanding these gaps is crucial for humanoid robot development:\n\n### Physical Property Discrepancies\n\nReal robots have properties that are difficult to model perfectly:\n\n- **Friction**: Real-world friction varies with surface conditions, temperature, and wear\n- **Flexibility**: Real joints have compliance not captured in simulation\n- **Actuator Dynamics**: Real motors have different response characteristics\n- **Sensor Noise**: Real sensors have different noise patterns\n\n```python\n# Example: Compensating for sim-to-real gaps\nclass Sim2RealCompensation:\n    def __init__(self):\n        self.friction_compensation = 0.1  # Additional friction in sim\n        self.actuator_delay = 0.02  # 20ms delay\n        self.sensor_noise_multiplier = 1.5  # 1.5x more noise in sim\n\n    def apply_friction_compensation(self, joint_effort, joint_velocity):\n        # Add additional friction to simulate real-world behavior\n        compensated_effort = joint_effort - (joint_velocity * self.friction_compensation)\n        return max(-100, min(100, compensated_effort))  # Limit to realistic values\n\n    def add_sensor_delay(self, sensor_data, delay_buffer):\n        # Simulate sensor processing delay\n        delay_buffer.append(sensor_data)\n        if len(delay_buffer) > int(self.actuator_delay / 0.001):  # Assuming 1ms sim step\n            return delay_buffer.pop(0)\n        return sensor_data\n```\n\n### Environmental Differences\n\n- **Surface Properties**: Real surfaces have varying friction, compliance, and texture\n- **Lighting Conditions**: Affects camera-based perception differently\n- **Air Resistance**: Negligible in simulation but present in reality\n- **Electromagnetic Interference**: Can affect sensors in real environments\n\n### Control Algorithm Adaptation\n\n```cpp\n// Example: Adaptive control to handle sim-to-real gaps\nclass AdaptiveController {\nprivate:\n    double adaptation_rate = 0.01;\n    double model_error = 0.0;\n    double parameter_adjustment = 0.0;\n\npublic:\n    double computeControl(const State& desired, const State& actual) {\n        // Standard control computation\n        double control_output = standardController(desired, actual);\n\n        // Adapt based on model error\n        model_error = desired.position - actual.position;\n        parameter_adjustment += adaptation_rate * model_error * actual.velocity;\n\n        // Apply parameter adjustment\n        control_output += parameter_adjustment;\n\n        return control_output;\n    }\n};\n```\n\n## Performance Profiling\n\nComprehensive profiling ensures digital twins accurately represent real robot performance:\n\n### Computational Performance\n\n```python\nimport time\nimport psutil\nimport matplotlib.pyplot as plt\n\nclass PerformanceProfiler:\n    def __init__(self):\n        self.timestamps = []\n        self.cpu_usage = []\n        self.memory_usage = []\n        self.simulation_times = []\n\n    def start_profiling(self):\n        self.start_time = time.time()\n        self.process = psutil.Process()\n\n    def profile_step(self):\n        # Record timestamp\n        current_time = time.time()\n        self.timestamps.append(current_time - self.start_time)\n\n        # Record system metrics\n        self.cpu_usage.append(self.process.cpu_percent())\n        self.memory_usage.append(self.process.memory_info().rss / 1024 / 1024)  # MB\n\n        # Record simulation step time\n        self.simulation_times.append(time.time() - self.start_time)\n\n    def generate_report(self):\n        fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n\n        # CPU Usage\n        axes[0].plot(self.timestamps, self.cpu_usage)\n        axes[0].set_title('CPU Usage Over Time')\n        axes[0].set_ylabel('CPU %')\n\n        # Memory Usage\n        axes[1].plot(self.timestamps, self.memory_usage)\n        axes[1].set_title('Memory Usage Over Time')\n        axes[1].set_ylabel('Memory (MB)')\n\n        # Simulation Timing\n        step_times = [self.simulation_times[i+1] - self.simulation_times[i]\n                     for i in range(len(self.simulation_times)-1)]\n        axes[2].plot(self.timestamps[1:], step_times)\n        axes[2].set_title('Simulation Step Times')\n        axes[2].set_ylabel('Time (s)')\n        axes[2].set_xlabel('Time (s)')\n\n        plt.tight_layout()\n        plt.savefig('performance_profile.png')\n        plt.show()\n```\n\n### Real-time Factor Analysis\n\n```python\nclass RealTimeFactorAnalyzer:\n    def __init__(self, simulation_step=0.001):  # 1ms sim step\n        self.simulation_step = simulation_step\n        self.simulation_start_time = None\n        self.wall_clock_start_time = None\n        self.simulation_time = 0.0\n\n    def start_simulation(self):\n        import time\n        self.simulation_start_time = 0.0\n        self.wall_clock_start_time = time.time()\n\n    def step_simulation(self):\n        import time\n        self.simulation_time += self.simulation_step\n        current_wall_time = time.time()\n\n        # Calculate real-time factor\n        elapsed_wall_time = current_wall_time - self.wall_clock_start_time\n        real_time_factor = self.simulation_time / elapsed_wall_time if elapsed_wall_time > 0 else 0\n\n        if real_time_factor < 0.9:\n            print(f\"Warning: RTF too low: {real_time_factor:.2f}\")\n        elif real_time_factor > 1.1:\n            print(f\"Info: RTF above 1.0: {real_time_factor:.2f} (fast simulation)\")\n\n        return real_time_factor\n\n# Usage example\nanalyzer = RealTimeFactorAnalyzer(0.001)  # 1ms steps\nanalyzer.start_simulation()\n\nfor step in range(1000):\n    rt_factor = analyzer.step_simulation()\n    # Run simulation step here\n```\n\n## Determinism and Repeatability\n\nFor validation purposes, digital twins must be deterministic and repeatable:\n\n### Random Seed Management\n\n```python\nimport random\nimport numpy as np\nimport torch\n\nclass DeterministicSimulator:\n    def __init__(self, seed=42):\n        self.seed = seed\n        self.set_seed(seed)\n\n    def set_seed(self, seed):\n        \"\"\"Set all random seeds for reproducibility\"\"\"\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n\n        # Set Gazebo seed if applicable\n        # This would typically be done through Gazebo API\n        print(f\"Set random seed to {seed} for all components\")\n\n    def run_deterministic_experiment(self, experiment_name):\n        \"\"\"Run the same experiment multiple times to verify determinism\"\"\"\n        results = []\n\n        for run in range(3):  # Run 3 times to verify consistency\n            self.set_seed(self.seed)\n            result = self.execute_experiment(experiment_name)\n            results.append(result)\n\n            if run > 0 and result != results[0]:\n                print(f\"ERROR: Non-deterministic result on run {run+1}\")\n                print(f\"  Expected: {results[0]}\")\n                print(f\"  Got: {result}\")\n                return False\n\n        print(f\"SUCCESS: All runs produced identical results\")\n        return True\n\n    def execute_experiment(self, experiment_name):\n        # Execute the specific experiment\n        # This would contain the actual simulation logic\n        pass\n```\n\n### State Serialization for Reproducibility\n\n```python\nimport pickle\nimport json\n\nclass SimulationStateSaver:\n    def __init__(self):\n        self.state_history = []\n\n    def save_state(self, simulation_time, robot_state, environment_state):\n        \"\"\"Save complete simulation state\"\"\"\n        state = {\n            'timestamp': simulation_time,\n            'robot_state': robot_state.copy(),\n            'environment_state': environment_state.copy(),\n            'random_state': self.get_random_state()\n        }\n        self.state_history.append(state)\n\n    def get_random_state(self):\n        \"\"\"Capture current random state for reproducibility\"\"\"\n        return {\n            'random_state': random.getstate(),\n            'numpy_state': np.random.get_state(),\n            'torch_state': torch.get_rng_state().tolist() if torch.cuda.is_available()\n                          else torch.get_rng_state().tolist()\n        }\n\n    def restore_state(self, state_index):\n        \"\"\"Restore simulation to a specific state\"\"\"\n        if 0 <= state_index < len(self.state_history):\n            state = self.state_history[state_index]\n\n            # Restore random states\n            random.setstate(state['random_state']['random_state'])\n            np.random.set_state(state['random_state']['numpy_state'])\n            torch.set_rng_state(torch.tensor(state['random_state']['torch_state']))\n\n            return state['robot_state'], state['environment_state'], state['timestamp']\n\n        return None, None, None\n\n    def save_to_file(self, filename):\n        \"\"\"Save state history to file\"\"\"\n        with open(filename, 'wb') as f:\n            pickle.dump(self.state_history, f)\n\n    def load_from_file(self, filename):\n        \"\"\"Load state history from file\"\"\"\n        with open(filename, 'rb') as f:\n            self.state_history = pickle.load(f)\n```\n\n## Simulation-Driven Development Workflow\n\nA structured workflow ensures effective use of digital twins:\n\n### Development Pipeline\n\n```mermaid\ngraph TD\n    A[Requirements] --> B[Simulation Model]\n    B --> C[Algorithm Development]\n    C --> D[Validation in Sim]\n    D --> E{Performance Adequate?}\n    E -->|No| C\n    E -->|Yes| F[Hardware Testing]\n    F --> G{Matches Simulation?}\n    G -->|No| H[Model Refinement]\n    H --> B\n    G -->|Yes| I[Deployment]\n    I --> J[Maintenance]\n```\n\n### Continuous Integration for Robotics\n\n```yaml\n# .github/workflows/robotics-ci.yml\nname: Robotics CI\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  simulation-test:\n    runs-on: ubuntu-latest\n    container:\n      image: osrf/ros:galactic-desktop\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Setup ROS environment\n      run: |\n        source /opt/ros/galactic/setup.bash\n        cd /github/workspace\n        colcon build\n\n    - name: Run simulation tests\n      run: |\n        source /opt/ros/galactic/setup.bash\n        source install/setup.bash\n        ros2 launch humanoid_simulation test_launch.py\n\n    - name: Run validation checks\n      run: |\n        python3 validation_scripts/check_balance_stability.py\n        python3 validation_scripts/check_sensor_accuracy.py\n        python3 validation_scripts/check_trajectory_following.py\n```\n\n### Validation Metrics Framework\n\n```python\nclass ValidationMetrics:\n    def __init__(self):\n        self.metrics = {}\n\n    def add_metric(self, name, expected, actual, tolerance=0.01):\n        \"\"\"Add a validation metric with expected vs actual values\"\"\"\n        error = abs(expected - actual)\n        passed = error <= tolerance\n        self.metrics[name] = {\n            'expected': expected,\n            'actual': actual,\n            'error': error,\n            'tolerance': tolerance,\n            'passed': passed\n        }\n\n    def compute_balance_accuracy(self, sim_com, real_com, tolerance=0.05):\n        \"\"\"Validate balance control between sim and real\"\"\"\n        distance_error = np.linalg.norm(sim_com - real_com)\n        passed = distance_error <= tolerance\n        self.metrics['balance_accuracy'] = {\n            'sim_com': sim_com.tolist(),\n            'real_com': real_com.tolist(),\n            'distance_error': distance_error,\n            'tolerance': tolerance,\n            'passed': passed\n        }\n        return passed\n\n    def compute_trajectory_accuracy(self, sim_trajectory, real_trajectory, tolerance=0.02):\n        \"\"\"Validate trajectory following performance\"\"\"\n        if len(sim_trajectory) != len(real_trajectory):\n            print(\"Trajectory length mismatch!\")\n            return False\n\n        errors = [np.linalg.norm(s - r) for s, r in zip(sim_trajectory, real_trajectory)]\n        avg_error = sum(errors) / len(errors)\n        max_error = max(errors)\n\n        passed = avg_error <= tolerance\n        self.metrics['trajectory_accuracy'] = {\n            'avg_error': avg_error,\n            'max_error': max_error,\n            'tolerance': tolerance,\n            'passed': passed\n        }\n        return passed\n\n    def generate_validation_report(self):\n        \"\"\"Generate comprehensive validation report\"\"\"\n        total_metrics = len(self.metrics)\n        passed_metrics = sum(1 for m in self.metrics.values() if m['passed'])\n        success_rate = passed_metrics / total_metrics if total_metrics > 0 else 0\n\n        report = {\n            'total_metrics': total_metrics,\n            'passed_metrics': passed_metrics,\n            'success_rate': success_rate,\n            'detailed_results': self.metrics,\n            'overall_status': 'PASS' if success_rate >= 0.95 else 'FAIL'\n        }\n\n        return report\n```\n\n## Practical Exercise\n\n1. Create a validation script that compares simulation and real robot behavior\n2. Implement determinism checks for your humanoid simulation\n3. Set up performance profiling for your digital twin\n4. Create a validation report showing sim-to-real accuracy\n\n## Summary\n\nDigital twin validation is critical for ensuring that simulation results translate to real-world performance. Understanding sim-to-real gaps, implementing comprehensive performance profiling, ensuring determinism, and establishing structured workflows are essential for successful humanoid robot development using digital twins. The validation process bridges the gap between virtual and physical systems, ensuring that algorithms developed in simulation will perform reliably on real robots.",
    "module": "Module 2: Digital Twin (Gazebo + Unity)",
    "chapter": "digital twin validation.md",
    "path": "/docs/module-2/2.4-digital-twin-validation"
  },
  {
    "id": "module-3/3.1-introduction-to-whole-body-control",
    "title": "3.1 Introduction to Whole Body Control",
    "content": "# 3.1 Introduction to Whole Body Control\n\n## Overview\n\nWhole Body Control (WBC) represents a sophisticated approach to controlling multi-degree-of-freedom robotic systems, particularly humanoid robots. Unlike traditional joint-space control methods that address each joint independently, WBC considers the entire robot as an integrated system, optimizing the coordination of all actuators simultaneously to achieve multiple tasks with varying priorities. This chapter introduces the fundamental concepts, mathematical foundations, and practical implementations of whole body control for humanoid robotics.\n\n## Learning Objectives\n\nBy the end of this chapter, you will be able to:\n- Understand the fundamental principles of whole body control\n- Explain the differences between joint-space control and task-space control\n- Describe the mathematical foundations of optimization-based control\n- Identify the key components of a whole body control system\n- Analyze the advantages and challenges of whole body control approaches\n- Implement basic whole body control algorithms for simple robotic tasks\n\n## What is Whole Body Control?\n\nWhole Body Control is a control framework that addresses the coordination of all degrees of freedom in a robotic system to achieve multiple simultaneous tasks. In humanoid robotics, this typically involves coordinating the movement of arms, legs, torso, and head to perform complex behaviors while maintaining balance and respecting physical constraints.\n\n### Key Characteristics\n\n1. **Multi-Task Optimization**: WBC handles multiple tasks simultaneously, each with its own priority level\n2. **Constraint Handling**: Explicitly manages joint limits, torque limits, and contact constraints\n3. **Hierarchical Priorities**: Organizes tasks in a hierarchy to ensure critical tasks (like balance) are prioritized\n4. **Real-time Performance**: Designed for real-time control with fast computation requirements\n5. **Constraint Satisfaction**: Ensures all physical and operational constraints are satisfied\n\n### Comparison with Traditional Control Methods\n\nTraditional robotic control typically operates in joint space, where each joint is controlled independently or in simple coordinated groups. In contrast, whole body control operates in task space, where the robot's behavior is defined by tasks in Cartesian space (position, orientation) or other meaningful coordinate systems.\n\n```python\n# Example: Joint-space control vs Task-space control\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass JointSpaceController:\n    \"\"\"Traditional joint-space control approach\"\"\"\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.num_joints = robot_model.get_num_joints()\n\n    def control_step(self, target_joints, current_joints, dt):\n        \"\"\"Simple joint-space PD control\"\"\"\n        error = target_joints - current_joints\n        velocity = error / dt\n        # PD control: u = Kp * error + Kd * velocity\n        control_effort = 100 * error + 10 * velocity\n        return control_effort\n\nclass TaskSpaceController:\n    \"\"\"Task-space control approach\"\"\"\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n\n    def control_step(self, target_tasks, current_tasks, dt):\n        \"\"\"Control based on task-space errors\"\"\"\n        task_errors = []\n        for target_task, current_task in zip(target_tasks, current_tasks):\n            error = self.calculate_task_error(target_task, current_task)\n            task_errors.append(error)\n\n        # Map task-space errors to joint-space commands\n        joint_commands = self.task_to_joint_mapping(task_errors)\n        return joint_commands\n\n    def calculate_task_error(self, target, current):\n        \"\"\"Calculate error in task space\"\"\"\n        # For position tasks\n        if hasattr(target, 'position'):\n            pos_error = target.position - current.position\n            return pos_error\n        # For orientation tasks\n        elif hasattr(target, 'orientation'):\n            # Calculate orientation error using rotation matrices\n            target_rot = R.from_quat(target.orientation)\n            current_rot = R.from_quat(current.orientation)\n            error_rot = target_rot * current_rot.inv()\n            # Convert to axis-angle representation\n            axis_angle = error_rot.as_rotvec()\n            return axis_angle\n        else:\n            return target - current\n\n    def task_to_joint_mapping(self, task_errors):\n        \"\"\"Map task-space errors to joint commands using Jacobian\"\"\"\n        # This would involve computing the Jacobian matrix and its pseudo-inverse\n        # J#d = desired_task_velocity\n        # q#d = J# * x#d (where J# is pseudo-inverse of Jacobian)\n        pass\n```\n\n## Mathematical Foundations\n\n### Optimization Framework\n\nWhole body control is fundamentally an optimization problem where multiple tasks are formulated as constraints or objectives in an optimization framework. The general form can be expressed as:\n\n```\nminimize: ||Ax - b||²\nsubject to: Cx = d (equality constraints)\n           Ex ≤ f (inequality constraints)\n```\n\nWhere:\n- `x` is the vector of control variables (typically joint velocities or accelerations)\n- `A` and `b` define the quadratic cost function\n- `C` and `d` define equality constraints\n- `E` and `f` define inequality constraints\n\n### Hierarchical Optimization\n\nIn WBC, tasks are organized in a hierarchy of priorities. Higher priority tasks must be satisfied before lower priority tasks are considered. This is typically implemented using a lexicographic optimization approach:\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\nfrom typing import List, Tuple, Dict\n\nclass HierarchicalOptimizer:\n    \"\"\"Implements hierarchical optimization for whole body control\"\"\"\n\n    def __init__(self):\n        self.tasks = []\n        self.constraints = []\n\n    def add_task(self, task_matrix: np.ndarray, task_vector: np.ndarray,\n                 priority: int, weight: float = 1.0):\n        \"\"\"Add a task to the optimization hierarchy\"\"\"\n        task = {\n            'A': task_matrix,\n            'b': task_vector,\n            'priority': priority,\n            'weight': weight\n        }\n        self.tasks.append(task)\n        # Sort tasks by priority\n        self.tasks.sort(key=lambda x: x['priority'])\n\n    def add_constraint(self, constraint_matrix: np.ndarray,\n                      constraint_vector: np.ndarray, constraint_type: str = 'equality'):\n        \"\"\"Add constraint to the optimization problem\"\"\"\n        constraint = {\n            'A': constraint_matrix,\n            'b': constraint_vector,\n            'type': constraint_type\n        }\n        self.constraints.append(constraint)\n\n    def solve_hierarchical(self, num_variables: int) -> np.ndarray:\n        \"\"\"Solve the hierarchical optimization problem\"\"\"\n        # Start with highest priority task\n        solution = np.zeros(num_variables)\n\n        for task in self.tasks:\n            # Formulate optimization problem for current priority level\n            A = task['A']\n            b = task['b']\n            weight = task['weight']\n\n            # Define objective function: minimize ||Ax - b||²\n            def objective(x):\n                return weight * np.sum((A @ x - b)**2)\n\n            # Define constraints from higher priority tasks\n            constraints = []\n            for i, higher_task in enumerate(self.tasks[:self.tasks.index(task)]):\n                # Add constraint that higher priority tasks must be satisfied\n                higher_A = higher_task['A']\n                higher_b = higher_task['b']\n                # Constraint: ||higher_A @ x - higher_b||² ≈ 0\n                constraints.append({\n                    'type': 'eq',\n                    'fun': lambda x, A=higher_A, b=higher_b: A @ x - b\n                })\n\n            # Add system constraints\n            for constraint in self.constraints:\n                if constraint['type'] == 'equality':\n                    constraints.append({\n                        'type': 'eq',\n                        'fun': lambda x, A=constraint['A'], b=constraint['b']: A @ x - b\n                    })\n\n            # Solve optimization problem\n            result = minimize(objective, solution, method='SLSQP', constraints=constraints)\n            solution = result.x if result.success else solution\n\n        return solution\n\n# Example usage of hierarchical optimizer\ndef example_hierarchical_control():\n    \"\"\"Example of hierarchical whole body control\"\"\"\n    optimizer = HierarchicalOptimizer()\n\n    # Define system with 10 joints\n    n_joints = 10\n\n    # Task 1: Balance (highest priority)\n    # Minimize deviation from center of mass\n    balance_A = np.zeros((3, n_joints))  # 3D position of CoM\n    balance_b = np.array([0.0, 0.0, 0.8])  # Desired CoM height and position\n    optimizer.add_task(balance_A, balance_b, priority=1, weight=10.0)\n\n    # Task 2: Arm reaching\n    arm_A = np.zeros((6, n_joints))  # 6D pose of end-effector (position + orientation)\n    arm_b = np.array([0.5, 0.2, 0.8, 0, 0, 0])  # Desired pose\n    optimizer.add_task(arm_A, arm_b, priority=2, weight=5.0)\n\n    # Task 3: Posture maintenance (lowest priority)\n    posture_A = np.eye(n_joints)  # Identity matrix for joint positions\n    posture_b = np.zeros(n_joints)  # Desired joint positions\n    optimizer.add_task(posture_A, posture_b, priority=3, weight=1.0)\n\n    # Solve the hierarchical optimization\n    solution = optimizer.solve_hierarchical(n_joints)\n    return solution\n```\n\n### Task Prioritization\n\nTasks in WBC are typically organized with the following priority structure:\n\n1. **Level 1 (Highest)**: Equality constraints (kinematic constraints, contact constraints)\n2. **Level 2**: Primary tasks (balance, safety)\n3. **Level 3**: Secondary tasks (motion objectives)\n4. **Level 4**: Optimization objectives (energy, smoothness)\n\n## Core Components of Whole Body Control\n\n### 1. Kinematic Model\n\nThe kinematic model describes the relationship between joint positions and end-effector positions. For whole body control, this includes forward and inverse kinematics for all kinematic chains.\n\n```python\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass KinematicModel:\n    \"\"\"Kinematic model for whole body control\"\"\"\n\n    def __init__(self, urdf_path: str):\n        # In practice, this would load from URDF\n        self.urdf_path = urdf_path\n        self.joint_names = []\n        self.links = []\n        self.transforms = {}\n\n    def forward_kinematics(self, joint_angles: np.ndarray,\n                          end_effector_link: str) -> Dict[str, np.ndarray]:\n        \"\"\"Compute forward kinematics for end-effector\"\"\"\n        # This would traverse the kinematic chain\n        # For simplicity, we'll implement a basic version\n\n        # Initialize transformation\n        transform = np.eye(4)\n\n        # Apply transformations for each joint\n        for i, joint_angle in enumerate(joint_angles):\n            # Create rotation matrix for revolute joint\n            joint_transform = self._create_joint_transform(joint_angle, i)\n            transform = transform @ joint_transform\n\n        # Extract position and orientation\n        position = transform[:3, 3]\n        orientation_matrix = transform[:3, :3]\n        orientation_quat = R.from_matrix(orientation_matrix).as_quat()\n\n        return {\n            'position': position,\n            'orientation': orientation_quat,\n            'transform': transform\n        }\n\n    def jacobian(self, joint_angles: np.ndarray,\n                end_effector_link: str) -> np.ndarray:\n        \"\"\"Compute geometric Jacobian for end-effector\"\"\"\n        # Geometric Jacobian maps joint velocities to end-effector twist\n        # J = [J_v; J_w] where J_v is linear velocity Jacobian, J_w is angular velocity Jacobian\n\n        n_joints = len(joint_angles)\n        jacobian = np.zeros((6, n_joints))  # 6D twist (linear + angular)\n\n        # Compute Jacobian columns\n        for i in range(n_joints):\n            # Get position of joint i\n            joint_pos = self._get_joint_position(joint_angles[:i+1], i)\n\n            # Get joint axis in world frame\n            joint_axis = self._get_joint_axis(joint_angles[:i+1], i)\n\n            # Linear velocity component: J_v_i = axis × (ee_pos - joint_pos)\n            if self._is_revolute_joint(i):\n                jacobian[:3, i] = np.cross(joint_axis, ee_pos - joint_pos)\n                # Angular velocity component: J_w_i = axis\n                jacobian[3:, i] = joint_axis\n            else:  # Prismatic joint\n                jacobian[:3, i] = joint_axis\n                jacobian[3:, i] = np.zeros(3)\n\n        return jacobian\n\n    def _create_joint_transform(self, angle: float, joint_idx: int) -> np.ndarray:\n        \"\"\"Create transformation matrix for a joint\"\"\"\n        # For a revolute joint around z-axis\n        transform = np.eye(4)\n        transform[0, 0] = np.cos(angle)\n        transform[0, 1] = -np.sin(angle)\n        transform[1, 0] = np.sin(angle)\n        transform[1, 1] = np.cos(angle)\n        return transform\n\n    def _get_joint_position(self, joint_angles: np.ndarray, joint_idx: int) -> np.ndarray:\n        \"\"\"Get world position of a joint\"\"\"\n        # Simplified implementation\n        return np.array([0.0, 0.0, 0.0])\n\n    def _get_joint_axis(self, joint_angles: np.ndarray, joint_idx: int) -> np.ndarray:\n        \"\"\"Get joint axis in world frame\"\"\"\n        # Simplified implementation\n        return np.array([0.0, 0.0, 1.0])\n\n    def _is_revolute_joint(self, joint_idx: int) -> bool:\n        \"\"\"Check if joint is revolute\"\"\"\n        return True\n```\n\n### 2. Dynamics Model\n\nThe dynamics model accounts for forces, torques, and inertial properties of the robot system.\n\n```python\nclass DynamicsModel:\n    \"\"\"Dynamics model for whole body control\"\"\"\n\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.mass_matrix = None\n        self.coriolis_matrix = None\n        self.gravity_vector = None\n\n    def compute_mass_matrix(self, joint_positions: np.ndarray) -> np.ndarray:\n        \"\"\"Compute mass matrix (inertia matrix)\"\"\"\n        # M(q) in the equation: M(q)q## + C(q,q#)q# + g(q) = τ\n        # This would use the composite rigid body algorithm or other methods\n        n_joints = len(joint_positions)\n        mass_matrix = np.eye(n_joints) * 1.0  # Simplified diagonal matrix\n        return mass_matrix\n\n    def compute_coriolis_matrix(self, joint_positions: np.ndarray,\n                               joint_velocities: np.ndarray) -> np.ndarray:\n        \"\"\"Compute Coriolis and centrifugal forces matrix\"\"\"\n        # C(q,q#) in the dynamics equation\n        n_joints = len(joint_positions)\n        coriolis_matrix = np.zeros((n_joints, n_joints))\n        # Simplified implementation - in practice, this involves Christoffel symbols\n        return coriolis_matrix\n\n    def compute_gravity_vector(self, joint_positions: np.ndarray) -> np.ndarray:\n        \"\"\"Compute gravity vector\"\"\"\n        # g(q) in the dynamics equation\n        n_joints = len(joint_positions)\n        gravity_vector = np.zeros(n_joints)\n        # Simplified implementation\n        for i in range(n_joints):\n            gravity_vector[i] = 9.81 * 0.1  # Simplified gravity effect\n        return gravity_vector\n\n    def inverse_dynamics(self, joint_positions: np.ndarray,\n                        joint_velocities: np.ndarray,\n                        joint_accelerations: np.ndarray) -> np.ndarray:\n        \"\"\"Compute required joint torques for desired accelerations\"\"\"\n        # τ = M(q)q## + C(q,q#)q# + g(q)\n        M = self.compute_mass_matrix(joint_positions)\n        C = self.compute_coriolis_matrix(joint_positions, joint_velocities)\n        g = self.compute_gravity_vector(joint_positions)\n\n        torques = M @ joint_accelerations + C @ joint_velocities + g\n        return torques\n\n    def forward_dynamics(self, joint_positions: np.ndarray,\n                        joint_velocities: np.ndarray,\n                        applied_torques: np.ndarray) -> np.ndarray:\n        \"\"\"Compute joint accelerations from applied torques\"\"\"\n        # q## = M#(q)[τ - C(q,q#)q# - g(q)]\n        M = self.compute_mass_matrix(joint_positions)\n        C = self.compute_coriolis_matrix(joint_positions, joint_velocities)\n        g = self.compute_gravity_vector(joint_positions)\n\n        # Compute accelerations\n        M_inv = np.linalg.inv(M)\n        accelerations = M_inv @ (applied_torques - C @ joint_velocities - g)\n        return accelerations\n```\n\n### 3. Contact Model\n\nFor humanoid robots, contact with the environment (floor, objects) is crucial and must be properly modeled.\n\n```python\nclass ContactModel:\n    \"\"\"Contact model for whole body control\"\"\"\n\n    def __init__(self):\n        self.contact_points = []\n        self.contact_forces = []\n        self.friction_coefficients = []\n\n    def add_contact_point(self, position: np.ndarray, normal: np.ndarray,\n                         friction_coeff: float = 0.8):\n        \"\"\"Add a contact point to the model\"\"\"\n        contact = {\n            'position': position,\n            'normal': normal / np.linalg.norm(normal),  # Normalize\n            'friction_coeff': friction_coeff,\n            'active': True\n        }\n        self.contact_points.append(contact)\n        self.friction_coefficients.append(friction_coeff)\n        self.contact_forces.append(np.zeros(3))  # Initialize with zero force\n\n    def compute_contact_jacobian(self, robot_model, contact_link: str) -> np.ndarray:\n        \"\"\"Compute Jacobian for contact point velocities\"\"\"\n        # This maps joint velocities to contact point velocities\n        # J_contact * q# = v_contact\n        pass\n\n    def check_contact_stability(self, com_position: np.ndarray,\n                              contact_forces: np.ndarray) -> bool:\n        \"\"\"Check if contact configuration is stable\"\"\"\n        # Check Zero Moment Point (ZMP) or support polygon\n        if len(self.contact_points) == 0:\n            return False\n\n        # Calculate center of pressure\n        total_force = np.sum(contact_forces, axis=0)\n        if np.linalg.norm(total_force[:2]) < 1e-6:  # Very small horizontal force\n            return True\n\n        # Calculate moment about contact points\n        moments = []\n        for contact, force in zip(self.contact_points, contact_forces):\n            # Moment = r × F\n            r = contact['position'] - com_position\n            moment = np.cross(r, force)\n            moments.append(moment)\n\n        # Check if net moment indicates stability\n        net_moment = np.sum(moments, axis=0)\n        # Simplified stability check\n        return abs(net_moment[1]) < 0.1  # Small moment about y-axis (roll)\n\n    def generate_contact_constraints(self) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Generate linear constraints for contact forces\"\"\"\n        # For each contact point, generate friction cone constraints\n        # These ensure contact forces lie within friction cones\n        A_ineq = []\n        b_ineq = []\n\n        for i, contact in enumerate(self.contact_points):\n            normal = contact['normal']\n            mu = contact['friction_coeff']\n\n            # Friction cone constraints: ||F_tangent|| ≤ μ * F_normal\n            # This creates multiple linear inequality constraints\n            # For 3D contact with 2D friction cone approximation\n            # We'll create constraints for x and y components\n            normal_force = normal[2]  # Assuming z is up\n\n            # Constraint 1: F_x ≤ μ * F_normal\n            row1 = np.zeros(len(self.contact_points) * 3)\n            row1[i*3 + 0] = 1  # F_x coefficient\n            row1[i*3 + 2] = -mu  # -μ * F_z coefficient\n            A_ineq.append(row1)\n            b_ineq.append(0)\n\n            # Constraint 2: -F_x ≤ μ * F_normal (equivalently: F_x ≥ -μ * F_normal)\n            row2 = np.zeros(len(self.contact_points) * 3)\n            row2[i*3 + 0] = -1  # -F_x coefficient\n            row2[i*3 + 2] = -mu  # -μ * F_z coefficient\n            A_ineq.append(row2)\n            b_ineq.append(0)\n\n            # Constraint 3: F_y ≤ μ * F_normal\n            row3 = np.zeros(len(self.contact_points) * 3)\n            row3[i*3 + 1] = 1  # F_y coefficient\n            row3[i*3 + 2] = -mu  # -μ * F_z coefficient\n            A_ineq.append(row3)\n            b_ineq.append(0)\n\n            # Constraint 4: -F_y ≤ μ * F_normal\n            row4 = np.zeros(len(self.contact_points) * 3)\n            row4[i*3 + 1] = -1  # -F_y coefficient\n            row4[i*3 + 2] = -mu  # -μ * F_z coefficient\n            A_ineq.append(row4)\n            b_ineq.append(0)\n\n        return np.array(A_ineq), np.array(b_ineq)\n```\n\n## Implementation Example: Simple Whole Body Controller\n\nLet's implement a simple but functional whole body controller that demonstrates the core concepts:\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom typing import List, Dict, Tuple\nimport time\n\nclass SimpleWholeBodyController:\n    \"\"\"A simple implementation of whole body control\"\"\"\n\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.kinematic_model = KinematicModel(\"dummy.urdf\")\n        self.dynamics_model = DynamicsModel(robot_model)\n        self.contact_model = ContactModel()\n\n        # Control parameters\n        self.control_frequency = 200  # Hz\n        self.integration_dt = 1.0 / self.control_frequency\n\n        # Task weights\n        self.balance_weight = 100.0\n        self.motion_weight = 10.0\n        self.posture_weight = 1.0\n\n        # Initialize joint positions\n        self.current_joint_positions = np.zeros(robot_model.get_num_joints())\n        self.current_joint_velocities = np.zeros(robot_model.get_num_joints())\n\n        # Desired tasks\n        self.desired_tasks = {\n            'balance': {'position': np.array([0.0, 0.0, 0.8]), 'weight': 100.0},\n            'left_arm': {'position': np.array([0.3, 0.2, 0.8]), 'weight': 10.0},\n            'right_arm': {'position': np.array([0.3, -0.2, 0.8]), 'weight': 10.0}\n        }\n\n    def update(self, dt: float) -> np.ndarray:\n        \"\"\"Main control update function\"\"\"\n        # Get current robot state\n        current_positions = self.current_joint_positions\n        current_velocities = self.current_joint_velocities\n\n        # Solve whole body control optimization\n        joint_accelerations = self._solve_control_problem(\n            current_positions, current_velocities\n        )\n\n        # Integrate to get new velocities and positions\n        new_velocities = current_velocities + joint_accelerations * dt\n        new_positions = current_positions + new_velocities * dt\n\n        # Update internal state\n        self.current_joint_positions = new_positions\n        self.current_joint_velocities = new_velocities\n\n        return new_positions, new_velocities, joint_accelerations\n\n    def _solve_control_problem(self, joint_positions: np.ndarray,\n                             joint_velocities: np.ndarray) -> np.ndarray:\n        \"\"\"Solve the whole body control optimization problem\"\"\"\n        n_joints = len(joint_positions)\n\n        # Define the optimization problem\n        def objective(joint_acc):\n            \"\"\"Objective function combining all tasks\"\"\"\n            total_cost = 0.0\n\n            # 1. Balance task - maintain center of mass\n            com_pos = self._compute_com_position(joint_positions)\n            com_vel = self._compute_com_velocity(joint_positions, joint_velocities)\n            com_acc = self._compute_com_acceleration(joint_positions, joint_velocities, joint_acc)\n\n            desired_com = self.desired_tasks['balance']['position']\n            balance_error = com_pos - desired_com\n            total_cost += self.balance_weight * np.sum(balance_error**2)\n\n            # 2. End-effector tasks\n            for effector_name in ['left_arm', 'right_arm']:\n                if effector_name in self.desired_tasks:\n                    ee_pos = self._compute_end_effector_position(joint_positions, effector_name)\n                    desired_ee_pos = self.desired_tasks[effector_name]['position']\n                    ee_error = ee_pos - desired_ee_pos\n                    total_cost += self.motion_weight * np.sum(ee_error**2)\n\n            # 3. Posture task - maintain desired joint configuration\n            desired_posture = np.zeros(n_joints)  # Default posture\n            posture_error = joint_positions - desired_posture\n            total_cost += self.posture_weight * np.sum(posture_error**2)\n\n            # 4. Smoothness term - penalize large accelerations\n            total_cost += 0.1 * np.sum(joint_acc**2)\n\n            return total_cost\n\n        # Define constraints\n        constraints = []\n\n        # Joint limits constraints\n        joint_limits = self.robot_model.get_joint_limits()\n        for i in range(n_joints):\n            # Lower bound: joint_acc[i] >= -max_acc\n            constraints.append({\n                'type': 'ineq',\n                'fun': lambda acc, idx=i, max_acc=100: acc[idx] + max_acc\n            })\n            # Upper bound: joint_acc[i] <= max_acc\n            constraints.append({\n                'type': 'ineq',\n                'fun': lambda acc, idx=i, max_acc=100: max_acc - acc[idx]\n            })\n\n        # Solve optimization problem\n        initial_guess = np.zeros(n_joints)\n        result = minimize(objective, initial_guess, method='SLSQP', constraints=constraints)\n\n        if result.success:\n            return result.x\n        else:\n            # Return zero acceleration if optimization fails\n            return np.zeros(n_joints)\n\n    def _compute_com_position(self, joint_positions: np.ndarray) -> np.ndarray:\n        \"\"\"Compute center of mass position\"\"\"\n        # Simplified implementation\n        # In practice, this would use the robot's inertial properties\n        return np.array([0.0, 0.0, 0.8])  # Fixed CoM for demonstration\n\n    def _compute_com_velocity(self, joint_positions: np.ndarray,\n                            joint_velocities: np.ndarray) -> np.ndarray:\n        \"\"\"Compute center of mass velocity\"\"\"\n        # Simplified implementation\n        return np.array([0.0, 0.0, 0.0])\n\n    def _compute_com_acceleration(self, joint_positions: np.ndarray,\n                                joint_velocities: np.ndarray,\n                                joint_accelerations: np.ndarray) -> np.ndarray:\n        \"\"\"Compute center of mass acceleration\"\"\"\n        # Simplified implementation\n        return np.array([0.0, 0.0, 0.0])\n\n    def _compute_end_effector_position(self, joint_positions: np.ndarray,\n                                     effector_name: str) -> np.ndarray:\n        \"\"\"Compute end-effector position using forward kinematics\"\"\"\n        # This would call the kinematic model\n        # For demonstration, return a fixed position\n        if effector_name == 'left_arm':\n            return np.array([0.3, 0.2, 0.8])\n        elif effector_name == 'right_arm':\n            return np.array([0.3, -0.2, 0.8])\n        else:\n            return np.array([0.0, 0.0, 0.0])\n\n    def set_balance_task(self, desired_com_position: np.ndarray):\n        \"\"\"Set desired center of mass position for balance\"\"\"\n        self.desired_tasks['balance']['position'] = desired_com_position\n\n    def set_end_effector_task(self, effector_name: str, desired_position: np.ndarray):\n        \"\"\"Set desired end-effector position\"\"\"\n        if effector_name in self.desired_tasks:\n            self.desired_tasks[effector_name]['position'] = desired_position\n        else:\n            self.desired_tasks[effector_name] = {'position': desired_position, 'weight': 10.0}\n\n    def add_contact(self, position: np.ndarray, normal: np.ndarray, friction_coeff: float = 0.8):\n        \"\"\"Add contact point to the controller\"\"\"\n        self.contact_model.add_contact_point(position, normal, friction_coeff)\n\n# Example usage and demonstration\ndef demonstrate_wbc():\n    \"\"\"Demonstrate whole body control concepts\"\"\"\n\n    # Create a dummy robot model (in practice, this would interface with a real robot)\n    class DummyRobotModel:\n        def __init__(self):\n            self.num_joints = 28  # Typical humanoid robot\n\n        def get_num_joints(self):\n            return self.num_joints\n\n        def get_joint_limits(self):\n            return {'lower': np.full(self.num_joints, -2.0),\n                   'upper': np.full(self.num_joints, 2.0)}\n\n    # Initialize the controller\n    robot_model = DummyRobotModel()\n    wbc_controller = SimpleWholeBodyController(robot_model)\n\n    print(\"Whole Body Controller initialized\")\n    print(f\"Robot has {robot_model.get_num_joints()} joints\")\n    print(f\"Control frequency: {wbc_controller.control_frequency} Hz\")\n\n    # Set some tasks\n    wbc_controller.set_balance_task(np.array([0.0, 0.0, 0.85]))\n    wbc_controller.set_end_effector_task('left_arm', np.array([0.4, 0.3, 0.8]))\n    wbc_controller.set_end_effector_task('right_arm', np.array([0.4, -0.3, 0.8]))\n\n    # Add contact points (feet)\n    wbc_controller.add_contact(np.array([0.1, 0.05, 0.0]), np.array([0, 0, 1]), 0.8)\n    wbc_controller.add_contact(np.array([0.1, -0.05, 0.0]), np.array([0, 0, 1]), 0.8)\n\n    print(\"Tasks and contacts set up\")\n    print(\"Balance task: maintain CoM at [0.0, 0.0, 0.85]\")\n    print(\"Left arm task: reach to [0.4, 0.3, 0.8]\")\n    print(\"Right arm task: reach to [0.4, -0.3, 0.8]\")\n\n    # Simulate control loop\n    dt = 1.0 / wbc_controller.control_frequency\n    simulation_steps = 100\n\n    print(f\"\\nRunning simulation for {simulation_steps} steps...\")\n\n    for i in range(simulation_steps):\n        # Update controller\n        positions, velocities, accelerations = wbc_controller.update(dt)\n\n        # Print progress every 20 steps\n        if i % 20 == 0:\n            print(f\"Step {i}: Joint position norms = {np.linalg.norm(positions):.3f}\")\n\n    print(f\"Simulation completed after {simulation_steps} steps\")\n    print(f\"Final joint positions norm: {np.linalg.norm(wbc_controller.current_joint_positions):.3f}\")\n    print(f\"Final joint velocities norm: {np.linalg.norm(wbc_controller.current_joint_velocities):.3f}\")\n\nif __name__ == \"__main__\":\n    demonstrate_wbc()\n```\n\n## Advantages of Whole Body Control\n\n### 1. Task Integration\nWBC naturally integrates multiple tasks that would be difficult to coordinate with traditional control methods. This includes balancing while manipulating objects, walking while avoiding obstacles, or any combination of simultaneous behaviors.\n\n### 2. Constraint Satisfaction\nThe optimization framework ensures that all physical constraints (joint limits, torque limits, contact constraints) are satisfied simultaneously, preventing the robot from commanding impossible or dangerous motions.\n\n### 3. Graceful Degradation\nWhen conflicts arise between tasks, WBC provides a principled way to handle them through hierarchical optimization, allowing the robot to maintain critical functions (like balance) while sacrificing less important ones.\n\n### 4. Optimal Coordination\nWBC finds the optimal coordination between all degrees of freedom to achieve the best possible performance across all tasks, rather than suboptimal sequential task execution.\n\n## Challenges and Limitations\n\n### 1. Computational Complexity\nSolving optimization problems in real-time requires significant computational resources, especially for high-DOF systems with many constraints.\n\n### 2. Model Accuracy\nWBC relies heavily on accurate models of robot kinematics, dynamics, and environmental interactions. Model errors can lead to poor performance.\n\n### 3. Real-time Requirements\nThe optimization must be solved faster than the control loop frequency, typically 200-1000 Hz for humanoid robots.\n\n### 4. Tuning Complexity\nSetting appropriate weights, priorities, and constraints requires significant expertise and tuning.\n\n## Applications of Whole Body Control\n\n### 1. Humanoid Locomotion\nWBC is essential for stable walking, running, and other locomotion patterns in humanoid robots, managing balance, foot placement, and momentum simultaneously.\n\n### 2. Manipulation\nCoordinating arm movements while maintaining balance and respecting contact constraints for manipulation tasks.\n\n### 3. Human-Robot Interaction\nManaging robot posture and motion during close interaction with humans while maintaining safety.\n\n### 4. Multi-Task Behaviors\nPerforming complex behaviors that require coordination of multiple body parts simultaneously.\n\n## Summary\n\nWhole Body Control represents a paradigm shift from traditional decoupled control approaches to a unified optimization framework that coordinates all degrees of freedom simultaneously. By formulating robot control as an optimization problem with hierarchical task priorities and constraint satisfaction, WBC enables sophisticated multi-task behaviors that would be extremely difficult to achieve with traditional methods.\n\nThe mathematical foundation of WBC, based on optimization theory and hierarchical control, provides a principled approach to handling the complexity of high-DOF robotic systems. While computationally demanding, modern advances in optimization algorithms and computational hardware have made WBC practical for real-time control of humanoid robots.\n\nAs humanoid robotics continues to advance, whole body control will remain a critical technology for achieving the complex, coordinated behaviors necessary for robots to operate effectively in human environments.\n\n## Exercises\n\n1. Implement a simple whole body controller for a 6-DOF manipulator that can track a Cartesian trajectory while maintaining a specific joint configuration as a secondary task.\n\n2. Extend the contact model implementation to handle multiple contact points and compute the Zero Moment Point (ZMP) for balance control.\n\n3. Create a hierarchical task structure for a humanoid robot that prioritizes balance, then arm reaching, then posture maintenance.\n\n4. Implement the Jacobian computation for a simple planar manipulator and verify its correctness through numerical differentiation.\n\n5. Design an optimization-based controller that can handle task conflicts by dynamically adjusting task priorities based on the robot's state.",
    "module": "Module 3: The AI-Robot Brain (NVIDIA Isaac)",
    "chapter": "introduction to whole body control.md",
    "path": "/docs/module-3/3.1-introduction-to-whole-body-control"
  },
  {
    "id": "module-3/3.2-kinematic-control-and-inverse-kinematics",
    "title": "3.2 Kinematic Control and Inverse Kinematics",
    "content": "# 3.2 Kinematic Control and Inverse Kinematics\n\n## Overview\n\nKinematic control forms the foundation of robot motion, enabling precise positioning and orientation of end-effectors and other points of interest on a robotic system. Inverse Kinematics (IK) is a critical component of kinematic control that determines the joint angles required to achieve desired end-effector poses. This chapter explores the mathematical foundations, algorithms, and practical implementations of kinematic control and inverse kinematics for humanoid robots and other multi-degree-of-freedom systems.\n\n## Learning Objectives\n\nBy the end of this chapter, you will be able to:\n- Understand the mathematical foundations of forward and inverse kinematics\n- Implement various inverse kinematics algorithms for robotic manipulators\n- Analyze the properties of different kinematic chains and their Jacobians\n- Handle kinematic singularities and joint limit constraints\n- Implement multi-task kinematic control for redundant manipulators\n- Design kinematic controllers for complex humanoid robot tasks\n\n## Forward Kinematics\n\nForward kinematics is the process of computing the position and orientation of a robot's end-effector given the joint angles. For a robotic manipulator with n joints, forward kinematics maps joint space coordinates to task space coordinates.\n\n### Homogeneous Transformation Matrices\n\nThe pose (position and orientation) of each link in a kinematic chain can be represented using homogeneous transformation matrices. A 4x4 homogeneous transformation matrix has the form:\n\n```\nT = [R  p]\n    [0  1]\n```\n\nWhere R is a 3x3 rotation matrix and p is a 3x1 position vector.\n\n```python\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport math\n\ndef dh_transform(a, alpha, d, theta):\n    \"\"\"\n    Denavit-Hartenberg transformation matrix\n    a: link length\n    alpha: link twist\n    d: link offset\n    theta: joint angle\n    \"\"\"\n    T = np.array([\n        [np.cos(theta), -np.sin(theta)*np.cos(alpha), np.sin(theta)*np.sin(alpha), a*np.cos(theta)],\n        [np.sin(theta), np.cos(theta)*np.cos(alpha), -np.cos(theta)*np.sin(alpha), a*np.sin(theta)],\n        [0, np.sin(alpha), np.cos(alpha), d],\n        [0, 0, 0, 1]\n    ])\n    return T\n\ndef forward_kinematics(joint_angles, dh_params):\n    \"\"\"\n    Compute forward kinematics using Denavit-Hartenberg parameters\n    joint_angles: array of joint angles\n    dh_params: list of tuples (a, alpha, d, theta_offset)\n    \"\"\"\n    T_total = np.eye(4)  # Start with identity matrix\n\n    for i, (a, alpha, d, theta_offset) in enumerate(dh_params):\n        theta = joint_angles[i] + theta_offset\n        T_link = dh_transform(a, alpha, d, theta)\n        T_total = T_total @ T_link\n\n    return T_total\n\ndef extract_position_orientation(T):\n    \"\"\"\n    Extract position and orientation from homogeneous transformation\n    \"\"\"\n    position = T[:3, 3]\n    orientation_matrix = T[:3, :3]\n    orientation_quat = R.from_matrix(orientation_matrix).as_quat()\n\n    return position, orientation_quat\n```\n\n### Kinematic Trees and Chain Traversal\n\nFor complex robots like humanoid systems, multiple kinematic chains exist (arms, legs, spine). Efficient traversal algorithms are needed to compute forward kinematics for all parts of the robot.\n\n```python\nclass KinematicChain:\n    \"\"\"Represents a kinematic chain for a robot\"\"\"\n\n    def __init__(self, chain_name, joint_names, dh_parameters):\n        self.name = chain_name\n        self.joint_names = joint_names\n        self.dh_params = dh_parameters  # List of (a, alpha, d, theta_offset) tuples\n        self.transforms = []  # Computed transforms for each joint\n\n    def compute_forward_kinematics(self, joint_angles):\n        \"\"\"Compute forward kinematics for the entire chain\"\"\"\n        if len(joint_angles) != len(self.dh_params):\n            raise ValueError(\"Joint angles length must match DH parameters length\")\n\n        transforms = [np.eye(4)]  # Base transform\n\n        for i, (a, alpha, d, theta_offset) in enumerate(self.dh_params):\n            theta = joint_angles[i] + theta_offset\n            T_link = dh_transform(a, alpha, d, theta)\n            T_cumulative = transforms[-1] @ T_link\n            transforms.append(T_cumulative)\n\n        self.transforms = transforms[1:]  # Exclude base transform\n        return self.transforms[-1]  # Return end-effector transform\n\n    def get_link_transform(self, link_index):\n        \"\"\"Get transform for a specific link in the chain\"\"\"\n        if link_index < 0 or link_index >= len(self.transforms):\n            raise IndexError(\"Link index out of range\")\n        return self.transforms[link_index]\n\n    def get_end_effector_pose(self):\n        \"\"\"Get the end-effector pose\"\"\"\n        if not self.transforms:\n            raise ValueError(\"Forward kinematics not computed yet\")\n        return extract_position_orientation(self.transforms[-1])\n\nclass RobotModel:\n    \"\"\"Complete robot model with multiple kinematic chains\"\"\"\n\n    def __init__(self):\n        self.chains = {}\n        self.joint_limits = {}\n        self.all_joint_names = []\n\n    def add_chain(self, name, joint_names, dh_params):\n        \"\"\"Add a kinematic chain to the robot\"\"\"\n        chain = KinematicChain(name, joint_names, dh_params)\n        self.chains[name] = chain\n        self.all_joint_names.extend(joint_names)\n\n    def set_joint_limits(self, joint_name, lower_limit, upper_limit):\n        \"\"\"Set joint limits\"\"\"\n        self.joint_limits[joint_name] = (lower_limit, upper_limit)\n\n    def compute_all_kinematics(self, joint_angles_dict):\n        \"\"\"Compute forward kinematics for all chains\"\"\"\n        results = {}\n\n        for chain_name, chain in self.chains.items():\n            # Extract joint angles for this chain\n            chain_angles = np.array([joint_angles_dict[name] for name in chain.joint_names])\n            end_effector_transform = chain.compute_forward_kinematics(chain_angles)\n            results[chain_name] = end_effector_transform\n\n        return results\n\n    def get_jacobian(self, chain_name, joint_angles, link_index=-1):\n        \"\"\"Compute geometric Jacobian for a chain\"\"\"\n        chain = self.chains[chain_name]\n        n_joints = len(joint_angles)\n\n        # Compute all transforms first\n        chain.compute_forward_kinematics(joint_angles)\n\n        # Get end-effector transform if link_index is -1 (default)\n        if link_index == -1:\n            link_transform = chain.transforms[-1]\n        else:\n            link_transform = chain.transforms[link_index]\n\n        ee_pos = link_transform[:3, 3]\n\n        # Initialize Jacobian\n        J = np.zeros((6, n_joints))  # 6D (linear + angular)\n\n        for i in range(n_joints):\n            # Get the transform of the i-th joint\n            if i == 0:\n                joint_pos = np.zeros(3)\n                joint_axis = np.array([0, 0, 1])  # Assuming z-axis rotation\n            else:\n                joint_transform = chain.transforms[i-1]\n                joint_pos = joint_transform[:3, 3]\n                # For revolute joints, axis is z-axis in joint frame\n                joint_axis = joint_transform[:3, 2]  # z-axis of joint frame\n\n            # Linear velocity component: Jv = z × (pe - pi)\n            J[:3, i] = np.cross(joint_axis, ee_pos - joint_pos)\n            # Angular velocity component: Jw = z\n            J[3:, i] = joint_axis\n\n        return J\n```\n\n## Inverse Kinematics Algorithms\n\n### Analytical Inverse Kinematics\n\nFor simple robots with specific geometries, analytical solutions to inverse kinematics can be derived. These solutions are computationally efficient and provide all possible solutions.\n\n```python\ndef inverse_kinematics_2dof_2r(l1, l2, x, y):\n    \"\"\"\n    Analytical inverse kinematics for 2-DOF planar manipulator\n    l1, l2: link lengths\n    x, y: desired end-effector position\n    \"\"\"\n    # Check if position is reachable\n    distance = np.sqrt(x**2 + y**2)\n    if distance > l1 + l2:\n        raise ValueError(\"Position is out of reach\")\n    if distance < abs(l1 - l2):\n        raise ValueError(\"Position is inside workspace\")\n\n    # Compute elbow-up solution\n    cos_theta2 = (x**2 + y**2 - l1**2 - l2**2) / (2 * l1 * l2)\n    sin_theta2 = np.sqrt(1 - cos_theta2**2)  # Take positive square root for elbow-up\n\n    theta2 = np.arctan2(sin_theta2, cos_theta2)\n\n    k1 = l1 + l2 * cos_theta2\n    k2 = l2 * sin_theta2\n\n    theta1 = np.arctan2(y, x) - np.arctan2(k2, k1)\n\n    return np.array([theta1, theta2])\n\ndef inverse_kinematics_3dof_spherical_wrist(l1, l2, l3, pos, orientation):\n    \"\"\"\n    Analytical IK for 6-DOF manipulator with spherical wrist\n    This is a simplified example for educational purposes\n    \"\"\"\n    # Position inverse kinematics (first 3 joints)\n    x, y, z = pos\n\n    # Wrist center position\n    wrist_center_x = x - l3 * orientation[0, 2]  # orientation[2] is the approach vector\n    wrist_center_y = y - l3 * orientation[1, 2]\n    wrist_center_z = z - l3 * orientation[2, 2]\n\n    # Compute first 3 joint angles\n    theta1 = np.arctan2(wrist_center_y, wrist_center_x)\n\n    r = np.sqrt(wrist_center_x**2 + wrist_center_y**2)\n    s = wrist_center_z - l1  # l1 is shoulder height\n\n    D = (r**2 + s**2 - l2**2 - l3**2) / (2 * l2 * l3)\n    theta3 = np.arctan2(np.sqrt(1 - D**2), D)  # Elbow up solution\n\n    theta2 = np.arctan2(s, r) - np.arctan2(l3 * np.sin(theta3), l2 + l3 * np.cos(theta3))\n\n    # Orientation inverse kinematics (last 3 joints) would follow\n    # This is simplified and would require the full spherical wrist solution\n\n    return np.array([theta1, theta2, theta3, 0, 0, 0])  # Placeholder for last 3 joints\n```\n\n### Numerical Inverse Kinematics\n\nFor complex robots without analytical solutions, numerical methods are used to solve inverse kinematics iteratively.\n\n```python\ndef jacobian_inverse_kinematics(chain, target_pose, initial_joints,\n                              max_iterations=100, tolerance=1e-6, alpha=0.1):\n    \"\"\"\n    Jacobian-based inverse kinematics\n    \"\"\"\n    joints = initial_joints.copy()\n\n    for i in range(max_iterations):\n        # Compute forward kinematics\n        current_transform = chain.compute_forward_kinematics(joints)\n        current_pos, current_quat = extract_position_orientation(current_transform)\n\n        # Compute pose error\n        pos_error = target_pose[:3] - current_pos\n\n        # For orientation error, we'll use a simplified approach\n        # In practice, you'd want to compute proper rotation error\n        current_rot = R.from_quat(current_quat)\n        target_rot = R.from_quat(target_pose[3:])  # Assuming target is [pos, quat]\n        rot_error = (target_rot.inv() * current_rot).as_rotvec()\n\n        # Combine position and rotation errors\n        error = np.concatenate([pos_error, rot_error])\n\n        # Check if error is within tolerance\n        if np.linalg.norm(error) < tolerance:\n            break\n\n        # Compute Jacobian\n        J = chain.get_jacobian(joints)\n\n        # Compute joint velocity\n        # Use damped least squares to handle singularities\n        damping = 0.01\n        JJT = J @ J.T\n        damping_matrix = damping**2 * np.eye(JJT.shape[0])\n        J_pseudo = J.T @ np.linalg.inv(JJT + damping_matrix)\n\n        joint_velocity = J_pseudo @ error\n\n        # Update joints\n        joints = joints + alpha * joint_velocity\n\n    return joints, np.linalg.norm(error) < tolerance\n\ndef jacobian_transpose_ik(chain, target_pose, initial_joints,\n                         max_iterations=100, tolerance=1e-6, alpha=0.1):\n    \"\"\"\n    Jacobian transpose method for inverse kinematics\n    \"\"\"\n    joints = initial_joints.copy()\n\n    for i in range(max_iterations):\n        # Compute forward kinematics\n        current_transform = chain.compute_forward_kinematics(joints)\n        current_pos, current_quat = extract_position_orientation(current_transform)\n\n        # Compute pose error\n        pos_error = target_pose[:3] - current_pos\n        current_rot = R.from_quat(current_quat)\n        target_rot = R.from_quat(target_pose[3:])\n        rot_error = (target_rot.inv() * current_rot).as_rotvec()\n\n        error = np.concatenate([pos_error, rot_error])\n\n        if np.linalg.norm(error) < tolerance:\n            break\n\n        # Compute Jacobian\n        J = chain.get_jacobian(joints)\n\n        # Use transpose of Jacobian\n        joint_velocity = alpha * J.T @ error\n        joints = joints + joint_velocity\n\n    return joints, np.linalg.norm(error) < tolerance\n```\n\n### Optimization-Based Inverse Kinematics\n\nFor complex multi-task scenarios, inverse kinematics can be formulated as an optimization problem:\n\n```python\nfrom scipy.optimize import minimize\n\nclass OptimizationBasedIK:\n    \"\"\"\n    Inverse kinematics formulated as an optimization problem\n    \"\"\"\n\n    def __init__(self, robot_model, chain_name):\n        self.robot_model = robot_model\n        self.chain_name = chain_name\n        self.chain = robot_model.chains[chain_name]\n\n        # Default weights for different terms\n        self.position_weight = 1.0\n        self.orientation_weight = 1.0\n        self.joint_center_weight = 0.1\n        self.smoothness_weight = 0.01\n\n    def solve(self, target_pose, current_joints, additional_objectives=None,\n              max_iterations=1000, tolerance=1e-6):\n        \"\"\"\n        Solve inverse kinematics as optimization problem\n        \"\"\"\n        def objective_function(joint_angles):\n            total_cost = 0.0\n\n            # Compute forward kinematics\n            transform = self.chain.compute_forward_kinematics(joint_angles)\n            pos, quat = extract_position_orientation(transform)\n\n            # Position error cost\n            pos_error = target_pose[:3] - pos\n            total_cost += self.position_weight * np.sum(pos_error**2)\n\n            # Orientation error cost\n            # Convert quaternions to rotation vectors for error computation\n            target_rot = R.from_quat(target_pose[3:])\n            current_rot = R.from_quat(quat)\n            rot_error = (target_rot.inv() * current_rot).as_rotvec()\n            total_cost += self.orientation_weight * np.sum(rot_error**2)\n\n            # Joint centering cost (keep joints near center of range)\n            for i, joint_name in enumerate(self.chain.joint_names):\n                if joint_name in self.robot_model.joint_limits:\n                    lower, upper = self.robot_model.joint_limits[joint_name]\n                    center = (lower + upper) / 2\n                    joint_error = joint_angles[i] - center\n                    total_cost += self.joint_center_weight * joint_error**2\n\n            # Smoothness cost (minimize joint velocity if previous position given)\n            if hasattr(self, 'previous_joints') and self.previous_joints is not None:\n                joint_smoothness = joint_angles - self.previous_joints\n                total_cost += self.smoothness_weight * np.sum(joint_smoothness**2)\n\n            # Additional objectives if provided\n            if additional_objectives:\n                for obj_func, weight in additional_objectives:\n                    total_cost += weight * obj_func(joint_angles)\n\n            return total_cost\n\n        # Define constraints for joint limits\n        constraints = []\n        for i, joint_name in enumerate(self.chain.joint_names):\n            if joint_name in self.robot_model.joint_limits:\n                lower, upper = self.robot_model.joint_limits[joint_name]\n\n                # Lower bound constraint: joint[i] >= lower\n                constraints.append({\n                    'type': 'ineq',\n                    'fun': lambda x, idx=i, lb=lower: x[idx] - lb\n                })\n\n                # Upper bound constraint: joint[i] <= upper\n                constraints.append({\n                    'type': 'ineq',\n                    'fun': lambda x, idx=i, ub=upper: ub - x[idx]\n                })\n\n        # Solve optimization problem\n        result = minimize(\n            objective_function,\n            current_joints,\n            method='SLSQP',\n            constraints=constraints,\n            options={'maxiter': max_iterations}\n        )\n\n        success = result.success and result.fun < tolerance\n\n        # Store for next iteration (for smoothness)\n        self.previous_joints = result.x.copy()\n\n        return result.x, success, result.fun\n\ndef example_optimization_ik():\n    \"\"\"Example usage of optimization-based IK\"\"\"\n    # Create a simple robot model\n    robot = RobotModel()\n\n    # Add a simple planar arm\n    dh_params = [\n        (1.0, 0, 0, 0),  # Link 1: a=1.0, alpha=0, d=0, theta_offset=0\n        (1.0, 0, 0, 0),  # Link 2: a=1.0, alpha=0, d=0, theta_offset=0\n        (0.5, 0, 0, 0)   # Link 3: a=0.5, alpha=0, d=0, theta_offset=0\n    ]\n\n    joint_names = ['joint1', 'joint2', 'joint3']\n    robot.add_chain('arm', joint_names, dh_params)\n\n    # Set joint limits\n    for joint in joint_names:\n        robot.set_joint_limits(joint, -np.pi, np.pi)\n\n    # Target pose [x, y, z, qw, qx, qy, qz]\n    target_pose = np.array([1.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0])  # Unit quaternion (no rotation)\n\n    # Initial joint configuration\n    initial_joints = np.array([0.0, 0.0, 0.0])\n\n    # Create optimization-based IK solver\n    ik_solver = OptimizationBasedIK(robot, 'arm')\n\n    # Solve\n    solution, success, final_cost = ik_solver.solve(target_pose, initial_joints)\n\n    print(f\"Optimization IK Result:\")\n    print(f\"Success: {success}\")\n    print(f\"Final cost: {final_cost}\")\n    print(f\"Joint solution: {solution}\")\n\n    # Verify solution\n    chain = robot.chains['arm']\n    final_transform = chain.compute_forward_kinematics(solution)\n    final_pos, final_quat = extract_position_orientation(final_transform)\n    print(f\"Final position: {final_pos}\")\n    print(f\"Target position: {target_pose[:3]}\")\n    print(f\"Position error: {np.linalg.norm(target_pose[:3] - final_pos)}\")\n\n    return solution, success\n```\n\n## Handling Kinematic Constraints\n\n### Joint Limits\n\nJoint limits are crucial for safe robot operation and must be considered in inverse kinematics:\n\n```python\nclass JointLimitHandler:\n    \"\"\"\n    Class to handle joint limit constraints in IK\n    \"\"\"\n\n    def __init__(self, joint_limits):\n        \"\"\"\n        joint_limits: dict with joint names as keys and (lower, upper) tuples as values\n        \"\"\"\n        self.limits = joint_limits\n\n    def apply_joint_limits(self, joint_angles, joint_names):\n        \"\"\"\n        Apply joint limits to joint angles\n        \"\"\"\n        limited_angles = joint_angles.copy()\n\n        for i, joint_name in enumerate(joint_names):\n            if joint_name in self.limits:\n                lower, upper = self.limits[joint_name]\n                limited_angles[i] = np.clip(limited_angles[i], lower, upper)\n\n        return limited_angles\n\n    def check_joint_limits(self, joint_angles, joint_names, tolerance=1e-6):\n        \"\"\"\n        Check if joint angles are within limits\n        \"\"\"\n        for i, joint_name in enumerate(joint_names):\n            if joint_name in self.limits:\n                lower, upper = self.limits[joint_name]\n                angle = joint_angles[i]\n\n                if angle < lower - tolerance or angle > upper + tolerance:\n                    return False, f\"Joint {joint_name} out of limits: {angle} not in [{lower}, {upper}]\"\n\n        return True, \"All joints within limits\"\n\n    def get_joint_limit_constraints(self, joint_names):\n        \"\"\"\n        Get constraint functions for optimization\n        \"\"\"\n        constraints = []\n\n        for i, joint_name in enumerate(joint_names):\n            if joint_name in self.limits:\n                lower, upper = self.limits[joint_name]\n\n                # Lower bound: x[i] >= lower\n                constraints.append({\n                    'type': 'ineq',\n                    'fun': lambda x, idx=i, lb=lower: x[idx] - lb\n                })\n\n                # Upper bound: x[i] <= upper\n                constraints.append({\n                    'type': 'ineq',\n                    'fun': lambda x, idx=i, ub=upper: ub - x[idx]\n                })\n\n        return constraints\n```\n\n### Singularity Handling\n\nKinematic singularities occur when the Jacobian loses rank, making it impossible to achieve certain motions:\n\n```python\nclass SingularityHandler:\n    \"\"\"\n    Class to handle kinematic singularities\n    \"\"\"\n\n    def __init__(self, threshold=1e-6):\n        self.threshold = threshold\n\n    def detect_singularity(self, jacobian, method='svd'):\n        \"\"\"\n        Detect singularity in the Jacobian matrix\n        \"\"\"\n        if method == 'svd':\n            # Use SVD to check condition number\n            U, s, Vh = np.linalg.svd(jacobian)\n            condition_number = s[0] / (s[-1] + 1e-12)  # Add small value to avoid division by zero\n\n            is_singular = condition_number > 1.0 / self.threshold\n            return is_singular, condition_number\n        elif method == 'determinant':\n            # For square matrices, check determinant\n            if jacobian.shape[0] == jacobian.shape[1]:\n                det = np.linalg.det(jacobian @ jacobian.T)\n                is_singular = abs(det) < self.threshold\n                return is_singular, abs(det)\n            else:\n                return self.detect_singularity(jacobian, 'svd')\n\n    def regularize_jacobian(self, jacobian):\n        \"\"\"\n        Regularize Jacobian using damped least squares to handle singularities\n        \"\"\"\n        J = jacobian\n        damping = 0.01  # Damping factor\n\n        # Damped least squares: J# = J.T @ inv(J @ J.T + damping^2 * I)\n        JJT = J @ J.T\n        damping_matrix = damping**2 * np.eye(JJT.shape[0])\n        regularized_pseudo_inverse = J.T @ np.linalg.inv(JJT + damping_matrix)\n\n        return regularized_pseudo_inverse\n\n    def get_nullspace_projection(self, jacobian):\n        \"\"\"\n        Get nullspace projection matrix for redundancy resolution\n        \"\"\"\n        # J# is the pseudoinverse of J\n        J_pinv = self.regularize_jacobian(jacobian)\n\n        # Nullspace projection: N = I - J# @ J\n        n_joints = jacobian.shape[1]\n        nullspace_proj = np.eye(n_joints) - J_pinv @ jacobian\n\n        return nullspace_proj\n```\n\n## Multi-Task Kinematic Control\n\nFor redundant robots, multiple tasks can be achieved simultaneously using nullspace optimization:\n\n```python\nclass MultiTaskKinematicController:\n    \"\"\"\n    Controller for multi-task kinematic control of redundant robots\n    \"\"\"\n\n    def __init__(self, robot_model, chain_name):\n        self.robot_model = robot_model\n        self.chain_name = chain_name\n        self.chain = robot_model.chains[chain_name]\n        self.singularity_handler = SingularityHandler()\n        self.joint_limit_handler = JointLimitHandler(robot_model.joint_limits)\n\n    def solve_multi_task(self, primary_tasks, secondary_task=None,\n                        current_joints=None, weights=None):\n        \"\"\"\n        Solve multi-task inverse kinematics\n\n        Args:\n            primary_tasks: List of (task_jacobian, task_error) tuples\n            secondary_task: Optional secondary task for redundancy resolution\n            current_joints: Current joint configuration\n            weights: Weights for each primary task\n        \"\"\"\n        if current_joints is None:\n            current_joints = np.zeros(len(self.chain.joint_names))\n\n        if weights is None:\n            weights = [1.0] * len(primary_tasks)\n\n        # Stack primary task equations\n        A_primary = []\n        b_primary = []\n\n        for i, (task_jac, task_err) in enumerate(primary_tasks):\n            weighted_jac = weights[i] * task_jac\n            A_primary.append(weighted_jac)\n            b_primary.append(weights[i] * task_err)\n\n        A_primary = np.vstack(A_primary)\n        b_primary = np.concatenate(b_primary)\n\n        # Solve primary task using damped least squares\n        damping = 0.01\n        JJT = A_primary @ A_primary.T\n        damping_matrix = damping**2 * np.eye(JJT.shape[0])\n\n        try:\n            A_primary_pinv = A_primary.T @ np.linalg.inv(JJT + damping_matrix)\n        except np.linalg.LinAlgError:\n            # If matrix is singular, use SVD-based pseudoinverse\n            U, s, Vh = np.linalg.svd(A_primary, full_matrices=False)\n            s_inv = np.where(s > self.singularity_handler.threshold, 1./s, 0)\n            A_primary_pinv = Vh.T @ np.diag(s_inv) @ U.T\n\n        # Primary solution\n        joint_change_primary = A_primary_pinv @ b_primary\n\n        # If there's a secondary task, resolve redundancy\n        joint_solution = joint_change_primary.copy()\n\n        if secondary_task is not None:\n            # Get nullspace of primary tasks\n            I = np.eye(len(current_joints))\n            nullspace_proj = I - A_primary_pinv @ A_primary\n\n            # Apply secondary task in nullspace\n            secondary_jac, secondary_error, secondary_weight = secondary_task\n            secondary_target = secondary_weight * secondary_error\n\n            # Project secondary Jacobian into nullspace\n            secondary_jac_null = secondary_jac @ nullspace_proj\n\n            # Solve secondary task in nullspace\n            if secondary_jac_null.shape[0] > 0:\n                secondary_damping = 0.01\n                JJT_secondary = secondary_jac_null @ secondary_jac_null.T\n                damping_matrix_secondary = secondary_damping**2 * np.eye(JJT_secondary.shape[0])\n\n                try:\n                    secondary_pinv = secondary_jac_null.T @ np.linalg.inv(JJT_secondary + damping_matrix_secondary)\n                except np.linalg.LinAlgError:\n                    U, s, Vh = np.linalg.svd(secondary_jac_null, full_matrices=False)\n                    s_inv = np.where(s > self.singularity_handler.threshold, 1./s, 0)\n                    secondary_pinv = Vh.T @ np.diag(s_inv) @ U.T\n\n                joint_change_secondary = secondary_pinv @ secondary_target\n\n                # Combine primary and secondary solutions\n                joint_solution = joint_change_primary + nullspace_proj @ joint_change_secondary\n\n        # Apply joint limits\n        final_joints = current_joints + joint_solution\n        final_joints = self.joint_limit_handler.apply_joint_limits(final_joints, self.chain.joint_names)\n\n        return final_joints, joint_solution\n\ndef example_multi_task_ik():\n    \"\"\"\n    Example of multi-task inverse kinematics\n    \"\"\"\n    # Create a simple robot model\n    robot = RobotModel()\n\n    # Add a 7-DOF arm (redundant for 6D end-effector control)\n    dh_params = [(1.0, 0, 0, 0) for _ in range(7)]\n    joint_names = [f'joint{i+1}' for i in range(7)]\n    robot.add_chain('arm', joint_names, dh_params)\n\n    for joint in joint_names:\n        robot.set_joint_limits(joint, -np.pi, np.pi)\n\n    # Create multi-task controller\n    controller = MultiTaskKinematicController(robot, 'arm')\n\n    # Current joint configuration\n    current_joints = np.zeros(7)\n\n    # Primary task: end-effector position and orientation\n    # In practice, we'd compute this using the robot's kinematic model\n    end_effector_jac = np.random.randn(6, 7)  # 6D task space, 7D joint space\n    end_effector_error = np.random.randn(6) * 0.1  # Small error\n\n    primary_tasks = [(end_effector_jac, end_effector_error)]\n\n    # Secondary task: maintain joint center configuration\n    joint_center_jac = np.eye(7)  # Identity for joint space\n    current_center_error = -current_joints  # Move toward center\n    secondary_task = (joint_center_jac, current_center_error, 0.01)  # Low weight\n\n    # Solve multi-task IK\n    final_joints, joint_changes = controller.solve_multi_task(\n        primary_tasks,\n        secondary_task=secondary_task,\n        current_joints=current_joints\n    )\n\n    print(f\"Multi-task IK Result:\")\n    print(f\"Current joints: {current_joints}\")\n    print(f\"Final joints: {final_joints}\")\n    print(f\"Joint changes: {joint_changes}\")\n\n    return final_joints\n```\n\n## Real-time Kinematic Control\n\nFor real-time applications, efficient algorithms are needed:\n\n```python\nclass RealTimeKinematicController:\n    \"\"\"\n    Efficient real-time kinematic controller\n    \"\"\"\n\n    def __init__(self, robot_model, control_frequency=200):\n        self.robot_model = robot_model\n        self.control_frequency = control_frequency\n        self.dt = 1.0 / control_frequency\n\n        # Caching for efficiency\n        self.last_joints = {}\n        self.jacobian_cache = {}\n\n        # Controllers for different chains\n        self.chain_controllers = {}\n        for chain_name in robot_model.chains.keys():\n            self.chain_controllers[chain_name] = ChainController(\n                robot_model.chains[chain_name],\n                robot_model.joint_limits\n            )\n\n    def update(self, target_poses, current_joints, dt=None):\n        \"\"\"\n        Update kinematic control\n\n        Args:\n            target_poses: Dict mapping chain names to target poses\n            current_joints: Dict mapping joint names to current positions\n            dt: Time step (defaults to controller's dt)\n        \"\"\"\n        if dt is None:\n            dt = self.dt\n\n        new_joints = current_joints.copy()\n\n        # Update each chain separately\n        for chain_name, target_pose in target_poses.items():\n            if chain_name in self.chain_controllers:\n                chain_controller = self.chain_controllers[chain_name]\n\n                # Extract chain-specific joints\n                chain_joint_names = self.robot_model.chains[chain_name].joint_names\n                chain_current_joints = np.array([current_joints[name] for name in chain_joint_names])\n\n                # Compute new joints for this chain\n                chain_new_joints = chain_controller.update(\n                    target_pose, chain_current_joints, dt\n                )\n\n                # Update main joint dictionary\n                for i, joint_name in enumerate(chain_joint_names):\n                    new_joints[joint_name] = chain_new_joints[i]\n\n        return new_joints\n\nclass ChainController:\n    \"\"\"\n    Controller for a single kinematic chain\n    \"\"\"\n\n    def __init__(self, chain_model, joint_limits):\n        self.chain = chain_model\n        self.joint_limits = joint_limits\n        self.singularity_handler = SingularityHandler()\n        self.last_joints = None\n        self.integrator_state = None\n\n    def update(self, target_pose, current_joints, dt):\n        \"\"\"\n        Update control for single chain\n        \"\"\"\n        # Compute error in task space\n        current_transform = self.chain.compute_forward_kinematics(current_joints)\n        current_pos, current_quat = extract_position_orientation(current_transform)\n\n        # Position error\n        pos_error = target_pose[:3] - current_pos\n\n        # Orientation error (simplified)\n        current_rot = R.from_quat(current_quat)\n        target_rot = R.from_quat(target_pose[3:])\n        rot_error = (target_rot.inv() * current_rot).as_rotvec()\n\n        task_error = np.concatenate([pos_error, rot_error])\n\n        # Compute Jacobian\n        jacobian = self.chain.get_jacobian(current_joints)\n\n        # Check for singularity\n        is_singular, condition_num = self.singularity_handler.detect_singularity(jacobian)\n\n        if is_singular:\n            # Use damped least squares\n            jacobian_pinv = self.singularity_handler.regularize_jacobian(jacobian)\n        else:\n            # Use standard pseudoinverse\n            jacobian_pinv = np.linalg.pinv(jacobian)\n\n        # Compute joint velocity\n        joint_vel = jacobian_pinv @ task_error / dt\n\n        # Integrate to get new joint positions\n        new_joints = current_joints + joint_vel * dt\n\n        # Apply joint limits\n        for i, joint_name in enumerate(self.chain.joint_names):\n            if joint_name in self.joint_limits:\n                lower, upper = self.joint_limits[joint_name]\n                new_joints[i] = np.clip(new_joints[i], lower, upper)\n\n        self.last_joints = new_joints.copy()\n\n        return new_joints\n\n# Example of complete kinematic control system\ndef demonstrate_kinematic_control():\n    \"\"\"\n    Demonstrate the complete kinematic control system\n    \"\"\"\n    print(\"=== Kinematic Control Demonstration ===\")\n\n    # Create robot model\n    robot = RobotModel()\n\n    # Add arm and leg chains\n    arm_dh = [(0.3, 0, 0, 0), (0.3, 0, 0, 0), (0.2, 0, 0, 0), (0.1, 0, 0, 0)]\n    arm_joints = ['shoulder', 'elbow', 'wrist1', 'wrist2']\n    robot.add_chain('right_arm', arm_joints, arm_dh)\n\n    leg_dh = [(0.4, 0, 0, 0), (0.4, 0, 0, 0), (0.1, 0, 0, 0)]\n    leg_joints = ['hip', 'knee', 'ankle']\n    robot.add_chain('left_leg', leg_joints, leg_dh)\n\n    # Set joint limits\n    for joint in arm_joints + leg_joints:\n        robot.set_joint_limits(joint, -np.pi/2, np.pi/2)\n\n    # Create real-time controller\n    controller = RealTimeKinematicController(robot, control_frequency=200)\n\n    # Initial joint configuration\n    initial_joints = {joint: 0.0 for joint in robot.all_joint_names}\n\n    # Target poses\n    target_poses = {\n        'right_arm': np.array([0.6, 0.2, 0.2, 1.0, 0.0, 0.0, 0.0]),  # [pos, quat]\n        'left_leg': np.array([0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0])\n    }\n\n    # Simulate control loop\n    print(\"Starting kinematic control simulation...\")\n\n    for step in range(100):\n        # Update control\n        new_joints = controller.update(target_poses, initial_joints)\n\n        # Update for next iteration\n        initial_joints = new_joints\n\n        # Print progress every 20 steps\n        if step % 20 == 0:\n            arm_joints = [new_joints[name] for name in arm_joints]\n            leg_joints = [new_joints[name] for name in leg_joints]\n            print(f\"Step {step}: Arm joints = {np.round(arm_joints, 3)}, Leg joints = {np.round(leg_joints, 3)}\")\n\n    print(\"Kinematic control simulation completed.\")\n\n    return new_joints\n\nif __name__ == \"__main__\":\n    # Run examples\n    solution, success = example_optimization_ik()\n    multi_task_solution = example_multi_task_ik()\n    final_joints = demonstrate_kinematic_control()\n\n    print(\"\\n=== Summary ===\")\n    print(f\"Optimization IK: Success = {success}\")\n    print(f\"Multi-task solution computed\")\n    print(f\"Real-time control simulation completed\")",
    "module": "Module 3: The AI-Robot Brain (NVIDIA Isaac)",
    "chapter": "kinematic control and inverse kinematics.md",
    "path": "/docs/module-3/3.2-kinematic-control-and-inverse-kinematics"
  },
  {
    "id": "module-3/3.3-dynamics-and-force-control",
    "title": "3.3 Dynamics and Force Control",
    "content": "# 3.3 Dynamics and Force Control\n\n## Overview\n\nRobot dynamics is the study of forces and torques that cause motion in robotic systems. Understanding and controlling the dynamic behavior of robots is crucial for achieving precise, stable, and efficient motion, especially in tasks involving interaction with the environment. This chapter covers the fundamental principles of robot dynamics, including the derivation of dynamic equations, force control strategies, and practical implementation of dynamic controllers for humanoid robots.\n\n## Learning Objectives\n\nBy the end of this chapter, you will be able to:\n- Derive and understand the equations of motion for robotic manipulators\n- Implement dynamic models for multi-body robotic systems\n- Design force and impedance controllers for robot-environment interaction\n- Analyze the stability and performance of dynamic control systems\n- Implement hybrid position/force control strategies\n- Handle dynamic constraints and contacts in robotic systems\n\n## Robot Dynamics Fundamentals\n\n### Equations of Motion\n\nThe dynamics of a robotic manipulator are described by the following equation:\n\n**M(q)q## + C(q,q#)q# + g(q) + J^T(q)F_ext = τ**\n\nWhere:\n- M(q): Inertia matrix (depends on joint configuration q)\n- C(q,q#): Coriolis and centrifugal forces matrix\n- g(q): Gravity vector\n- J(q): Jacobian matrix\n- F_ext: External forces/torques\n- τ: Joint torques\n- q, q#, q##: Joint positions, velocities, and accelerations\n\n```python\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport math\n\nclass RobotDynamics:\n    \"\"\"\n    Class to compute robot dynamics properties\n    \"\"\"\n\n    def __init__(self, robot_model):\n        \"\"\"\n        robot_model: Contains kinematic and inertial properties\n        \"\"\"\n        self.model = robot_model\n        self.n_dof = robot_model.get_num_joints()\n\n    def compute_mass_matrix(self, q):\n        \"\"\"\n        Compute the mass matrix M(q) using the Composite Rigid Body Algorithm\n        This is a simplified implementation; in practice, more sophisticated methods are used\n        \"\"\"\n        # For a simple n-DOF manipulator, we can approximate the mass matrix\n        # In reality, this would use the robot's link inertias and kinematic structure\n        M = np.zeros((self.n_dof, self.n_dof))\n\n        # This is a placeholder implementation - in practice, use RNEA or CRBA\n        for i in range(self.n_dof):\n            # Diagonal elements represent link masses\n            M[i, i] = self.model.get_link_mass(i) + 0.1  # Add some coupling\n\n        # Add some off-diagonal coupling terms\n        for i in range(self.n_dof):\n            for j in range(i+1, self.n_dof):\n                # Coupling based on geometric relationship\n                coupling = 0.1 * np.cos(q[i] - q[j])\n                M[i, j] = M[j, i] = coupling\n\n        return M\n\n    def compute_coriolis_matrix(self, q, q_dot):\n        \"\"\"\n        Compute the Coriolis and centrifugal forces matrix C(q,q#)\n        \"\"\"\n        C = np.zeros((self.n_dof, self.n_dof))\n\n        # Simplified computation - in practice, use more accurate methods\n        for i in range(self.n_dof):\n            for j in range(self.n_dof):\n                # Coriolis terms are typically computed using Christoffel symbols\n                # This is a simplified approximation\n                if i != j:\n                    C[i, j] = 0.5 * np.sin(q[i] - q[j]) * q_dot[j]\n                else:\n                    C[i, j] = np.cos(q[i]) * q_dot[i]\n\n        return C\n\n    def compute_gravity_vector(self, q, gravity=[0, 0, -9.81]):\n        \"\"\"\n        Compute the gravity vector g(q)\n        \"\"\"\n        g = np.zeros(self.n_dof)\n\n        # For each joint, compute gravity effect based on orientation\n        for i in range(self.n_dof):\n            # Simplified gravity effect\n            link_mass = self.model.get_link_mass(i)\n            g[i] = link_mass * gravity[2] * np.cos(q[i])\n\n        return g\n\n    def compute_coriolis_gravity(self, q, q_dot):\n        \"\"\"\n        Compute Coriolis and gravity terms: C(q,q#)q# + g(q)\n        \"\"\"\n        C = self.compute_coriolis_matrix(q, q_dot)\n        g = self.compute_gravity_vector(q)\n\n        # C(q,q#)q# + g(q)\n        return C @ q_dot + g\n\n    def inverse_dynamics(self, q, q_dot, q_ddot):\n        \"\"\"\n        Compute required joint torques for desired motion (inverse dynamics)\n        \"\"\"\n        M = self.compute_mass_matrix(q)\n        Cg = self.compute_coriolis_gravity(q, q_dot)\n\n        # τ = M(q)q## + C(q,q#)q# + g(q)\n        tau = M @ q_ddot + Cg\n\n        return tau\n\n    def forward_dynamics(self, q, q_dot, tau):\n        \"\"\"\n        Compute joint accelerations from applied torques (forward dynamics)\n        \"\"\"\n        M = self.compute_mass_matrix(q)\n        Cg = self.compute_coriolis_gravity(q, q_dot)\n\n        # q## = M#(q)[τ - C(q,q#)q# - g(q)]\n        M_inv = np.linalg.inv(M)\n        q_ddot = M_inv @ (tau - Cg)\n\n        return q_ddot\n\nclass SimpleRobotModel:\n    \"\"\"\n    Simple robot model for demonstration\n    \"\"\"\n\n    def __init__(self, n_joints=6):\n        self.n_joints = n_joints\n        # Simple link masses (in kg)\n        self.link_masses = [1.0 + 0.5*i for i in range(n_joints)]\n\n    def get_num_joints(self):\n        return self.n_joints\n\n    def get_link_mass(self, link_index):\n        if link_index < len(self.link_masses):\n            return self.link_masses[link_index]\n        else:\n            return 0.1  # Default small mass\n```\n\n### The Recursive Newton-Euler Algorithm (RNEA)\n\nThe RNEA is an efficient algorithm for computing inverse dynamics:\n\n```python\nclass RecursiveNewtonEuler:\n    \"\"\"\n    Recursive Newton-Euler Algorithm for inverse dynamics\n    \"\"\"\n\n    def __init__(self, robot_model):\n        self.model = robot_model\n        self.n_dof = robot_model.get_num_joints()\n\n    def inverse_dynamics_rnea(self, q, q_dot, q_ddot, gravity=np.array([0, 0, -9.81])):\n        \"\"\"\n        Compute inverse dynamics using RNEA\n        \"\"\"\n        n = self.n_dof\n\n        # Forward pass: compute velocities and accelerations\n        v = [np.zeros(3) for _ in range(n)]  # Linear velocities\n        w = [np.zeros(3) for _ in range(n)]  # Angular velocities\n        v_dot = [np.zeros(3) for _ in range(n)]  # Linear accelerations\n        w_dot = [np.zeros(3) for _ in range(n)]  # Angular accelerations\n\n        # Initialize base conditions\n        v[0] = np.zeros(3)\n        w[0] = np.zeros(3)\n        v_dot[0] = gravity\n        w_dot[0] = np.zeros(3)\n\n        # Forward recursion\n        for i in range(n):\n            # For revolute joints (simplified)\n            z_axis = np.array([0, 0, 1])  # Joint axis in local frame\n\n            # Transform to world frame (simplified)\n            # In practice, this would use the actual transformation matrices\n            z_world = z_axis  # Placeholder\n\n            # Compute velocities\n            w[i] = w[i-1] if i > 0 else np.zeros(3)  # Simplified\n            w[i] += q_dot[i] * z_world\n\n            # Compute accelerations\n            w_dot[i] = w_dot[i-1] if i > 0 else np.zeros(3)  # Simplified\n            w_dot[i] += q_ddot[i] * z_world + np.cross(w[i], q_dot[i] * z_world)\n\n            # Compute linear velocity and acceleration\n            # Simplified - in practice, this would use link transformations\n            v[i] = v[i-1] if i > 0 else np.zeros(3)\n            v_dot[i] = v_dot[i-1] if i > 0 else np.zeros(3)\n\n        # Backward pass: compute forces and torques\n        f = [np.zeros(3) for _ in range(n)]  # Link forces\n        n_tau = [np.zeros(3) for _ in range(n)]  # Link torques\n        tau = np.zeros(n)  # Joint torques\n\n        for i in range(n-1, -1, -1):\n            # Link mass and inertia (simplified)\n            m_i = self.model.get_link_mass(i)\n            I_i = np.eye(3) * (m_i * 0.1)  # Simplified inertia tensor\n\n            # Compute link forces and torques\n            f[i] = m_i * v_dot[i]\n            n_tau[i] = I_i @ w_dot[i] + np.cross(w[i], I_i @ w[i])\n\n            # Compute joint torque\n            z_axis = np.array([0, 0, 1])  # Joint axis\n            tau[i] = np.dot(z_axis, n_tau[i])\n\n        return tau\n\ndef example_rnea():\n    \"\"\"\n    Example of using RNEA for inverse dynamics\n    \"\"\"\n    robot_model = SimpleRobotModel(n_joints=6)\n    rnea = RecursiveNewtonEuler(robot_model)\n\n    # Joint states\n    q = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])  # Joint positions\n    q_dot = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])  # Joint velocities\n    q_ddot = np.array([0.01, 0.01, 0.01, 0.01, 0.01, 0.01])  # Joint accelerations\n\n    # Compute inverse dynamics\n    torques = rnea.inverse_dynamics_rnea(q, q_dot, q_ddot)\n\n    print(f\"Joint positions: {q}\")\n    print(f\"Joint velocities: {q_dot}\")\n    print(f\"Joint accelerations: {q_ddot}\")\n    print(f\"Required joint torques: {torques}\")\n\n    return torques\n```\n\n## Force Control Fundamentals\n\n### Impedance Control\n\nImpedance control makes the robot behave like a mechanical system with specified dynamic properties (mass, damping, stiffness):\n\n```python\nclass ImpedanceController:\n    \"\"\"\n    Impedance controller for force control\n    \"\"\"\n\n    def __init__(self, M_d, D_d, K_d, dt=0.005):\n        \"\"\"\n        M_d: Desired mass matrix\n        D_d: Desired damping matrix\n        K_d: Desired stiffness matrix\n        dt: Control time step\n        \"\"\"\n        self.M_d = M_d\n        self.D_d = D_d\n # Desired stiffness matrix\n        self.K_d = K_d\n        self.dt = dt\n\n        # Check matrix dimensions\n        assert M_d.shape == D_d.shape == K_d.shape, \"Matrix dimensions must match\"\n\n    def compute_impedance_force(self, x_error, x_dot_error, x_ddot_error):\n        \"\"\"\n        Compute impedance control force: F = M_d * x_ddot_error + D_d * x_dot_error + K_d * x_error\n        \"\"\"\n        F_impedance = self.M_d @ x_ddot_error + self.D_d @ x_dot_error + self.K_d @ x_error\n        return F_impedance\n\n    def update(self, desired_state, current_state):\n        \"\"\"\n        Update impedance controller\n        desired_state: dict with 'position', 'velocity', 'acceleration'\n        current_state: dict with 'position', 'velocity', 'acceleration'\n        \"\"\"\n        # Compute errors\n        x_error = desired_state['position'] - current_state['position']\n        x_dot_error = desired_state['velocity'] - current_state['velocity']\n        x_ddot_error = desired_state['acceleration'] - current_state['acceleration']\n\n        # Compute impedance force\n        F_impedance = self.compute_impedance_force(x_error, x_dot_error, x_ddot_error)\n\n        return F_impedance\n\nclass CartesianImpedanceController:\n    \"\"\"\n    Cartesian impedance controller for end-effector control\n    \"\"\"\n\n    def __init__(self, robot_model, chain_name, M_cart, D_cart, K_cart, dt=0.005):\n        self.robot_model = robot_model\n        self.chain_name = chain_name\n        self.chain = robot_model.chains[chain_name]\n\n        # Cartesian space impedance parameters\n        self.M_cart = M_cart  # 6x6 (x, y, z, rx, ry, rz)\n        self.D_cart = D_cart\n        self.K_cart = K_cart\n        self.dt = dt\n\n    def update(self, q, q_dot, desired_pose, current_pose, desired_twist=None, current_twist=None):\n        \"\"\"\n        Update Cartesian impedance controller\n        q: Current joint positions\n        q_dot: Current joint velocities\n        desired_pose: [x, y, z, qx, qy, qz, qw] (position + quaternion)\n        current_pose: [x, y, z, qx, qy, qz, qw] (position + quaternion)\n        \"\"\"\n        # Extract position and orientation errors\n        desired_pos = desired_pose[:3]\n        current_pos = current_pose[:3]\n        pos_error = desired_pos - current_pos\n\n        # Compute orientation error (simplified)\n        desired_rot = R.from_quat(desired_pose[3:])  # [qx, qy, qz, qw]\n        current_rot = R.from_quat(current_pose[3:])\n        rot_error_quat = (desired_rot.inv() * current_rot).as_quat()\n        # Convert to axis-angle representation (simplified)\n        rot_error = 2 * rot_error_quat[:3] * np.sign(rot_error_quat[3])\n\n        # Combine position and orientation errors\n        x_error = np.concatenate([pos_error, rot_error])\n\n        # Compute velocity error\n        if desired_twist is not None and current_twist is not None:\n            x_dot_error = desired_twist - current_twist\n        else:\n            # Estimate from position differences (simplified)\n            x_dot_error = np.zeros(6)\n\n        # Compute acceleration error (simplified)\n        x_ddot_error = np.zeros(6)\n\n        # Compute Cartesian impedance force\n        F_cart = self.M_cart @ x_ddot_error + self.D_cart @ x_dot_error + self.K_cart @ x_error\n\n        # Get Jacobian for mapping Cartesian forces to joint torques\n        J = self.chain.get_jacobian(q)\n\n        # Map Cartesian forces to joint torques: τ = J^T * F\n        tau_impedance = J.T @ F_cart\n\n        return tau_impedance, F_cart\n\ndef example_impedance_control():\n    \"\"\"\n    Example of impedance control\n    \"\"\"\n    # Define desired impedance parameters (6 DOF: x, y, z, rx, ry, rz)\n    M_cart = np.eye(6) * 10.0      # Mass matrix\n    D_cart = np.eye(6) * 50.0      # Damping matrix\n    K_cart = np.eye(6) * 1000.0    # Stiffness matrix\n\n    # Create robot model and controller\n    robot_model = SimpleRobotModel(n_joints=6)\n\n    # Add a simple chain for demonstration\n    class SimpleChain:\n        def get_jacobian(self, q):\n            # Simplified Jacobian (6x6 for 6 DOF)\n            return np.eye(6)\n\n    robot_model.chains = {'end_effector': SimpleChain()}\n\n    controller = CartesianImpedanceController(\n        robot_model, 'end_effector', M_cart, D_cart, K_cart\n    )\n\n    # Current and desired states\n    current_pose = np.array([0.5, 0.3, 0.8, 0, 0, 0, 1])  # [pos, quat]\n    desired_pose = np.array([0.6, 0.4, 0.8, 0, 0, 0, 1])  # [pos, quat]\n\n    current_joints = np.zeros(6)\n    current_joints_vel = np.zeros(6)\n\n    # Compute impedance control torques\n    tau_impedance, F_cart = controller.update(\n        current_joints, current_joints_vel, desired_pose, current_pose\n    )\n\n    print(f\"Cartesian impedance force: {F_cart}\")\n    print(f\"Joint torques: {tau_impedance}\")\n\n    return tau_impedance\n```\n\n### Admittance Control\n\nAdmittance control is the dual of impedance control, where motion is generated in response to applied forces:\n\n```python\nclass AdmittanceController:\n    \"\"\"\n    Admittance controller - motion is generated in response to forces\n    \"\"\"\n\n    def __init__(self, M_a, D_a, K_a, dt=0.005):\n        \"\"\"\n        M_a: Admittance mass matrix\n        D_a: Admittance damping matrix\n        K_a: Admittance stiffness matrix\n        dt: Control time step\n        \"\"\"\n        self.M_a = M_a\n        self.D_a = D_a\n        self.K_a = K_a\n        self.dt = dt\n\n        # State variables for integration\n        self.x = np.zeros(M_a.shape[0])  # Position\n        self.x_dot = np.zeros(M_a.shape[0])  # Velocity\n\n    def update(self, applied_force, desired_pose=None):\n        \"\"\"\n        Update admittance controller\n        applied_force: External force applied to the system\n        desired_pose: Optional desired pose for reference\n        \"\"\"\n        # Compute acceleration: M_a * x_ddot + D_a * x_dot + K_a * x = F\n        # x_ddot = M_a# * (F - D_a * x_dot - K_a * x)\n\n        if desired_pose is not None:\n            # Add stiffness to desired pose\n            pos_error = desired_pose - self.x\n            force_feedback = self.K_a @ pos_error\n            applied_force = applied_force + force_feedback\n\n        # Compute acceleration\n        M_inv = np.linalg.inv(self.M_a)\n        x_ddot = M_inv @ (applied_force - self.D_a @ self.x_dot - self.K_a @ self.x)\n\n        # Integrate to get new velocity and position\n        self.x_dot = self.x_dot + x_ddot * self.dt\n        self.x = self.x + self.x_dot * self.dt\n\n        return self.x, self.x_dot, x_ddot\n\nclass HybridPositionForceController:\n    \"\"\"\n    Hybrid position/force controller that can control different DOFs independently\n    \"\"\"\n\n    def __init__(self, robot_model, chain_name, dt=0.005):\n        self.robot_model = robot_model\n        self.chain_name = chain_name\n        self.dt = dt\n\n        # Selection matrix (identity for full 6D control)\n        self.Sigma = np.eye(6)  # Controls which DOFs are position-controlled vs force-controlled\n\n        # Position controller gains\n        self.Kp_pos = np.diag([1000, 1000, 1000, 100, 100, 100])  # Higher for translation\n        self.Kd_pos = np.diag([100, 100, 100, 50, 50, 50])\n\n        # Force controller gains\n        self.Kp_force = np.diag([10, 10, 10, 5, 5, 5])\n        self.Kd_force = np.diag([1, 1, 1, 0.5, 0.5, 0.5])\n\n    def update(self, q, q_dot, desired_pose, current_pose, desired_force, current_force,\n               force_frame='world'):\n        \"\"\"\n        Update hybrid controller\n        desired_pose: Desired Cartesian pose [x, y, z, qx, qy, qz, qw]\n        current_pose: Current Cartesian pose\n        desired_force: Desired force/torque in selected DOFs\n        current_force: Measured force/torque\n        \"\"\"\n        # Compute pose error\n        pos_error = desired_pose[:3] - current_pose[:3]\n\n        # Compute orientation error (simplified)\n        desired_rot = R.from_quat(desired_pose[3:])\n        current_rot = R.from_quat(current_pose[3:])\n        rot_error_quat = (desired_rot.inv() * current_rot).as_quat()\n        rot_error = 2 * rot_error_quat[:3] * np.sign(rot_error_quat[3])\n\n        pose_error = np.concatenate([pos_error, rot_error])\n\n        # Compute force error\n        force_error = desired_force - current_force\n\n        # Get Jacobian\n        chain = self.robot_model.chains[self.chain_name]\n        J = chain.get_jacobian(q)\n\n        # Compute selection matrix for active constraints\n        # This would typically be computed based on contact constraints\n        S = self.Sigma  # For now, use the full selection matrix\n\n        # Compute position control component\n        x_ddot_pos = -self.Kp_pos @ pose_error - self.Kd_pos @ np.zeros(6)  # Assuming zero velocity error for simplicity\n\n        # Compute force control component\n        F_error_integrated = force_error  # In practice, integrate force error\n        x_ddot_force = self.Kp_force @ F_error_integrated + self.Kd_force @ np.zeros(6)\n\n        # Combine position and force control\n        # This is a simplified combination - in practice, more sophisticated methods are used\n        x_ddot_cmd = S @ x_ddot_pos + (np.eye(6) - S) @ x_ddot_force\n\n        # Map Cartesian acceleration to joint torques using inverse dynamics\n        # This requires more complex treatment in practice\n        tau_cmd = J.T @ (np.eye(6) @ x_ddot_cmd)  # Simplified\n\n        return tau_cmd, x_ddot_cmd\n\ndef example_hybrid_control():\n    \"\"\"\n    Example of hybrid position/force control\n    \"\"\"\n    # Create robot model\n    robot_model = SimpleRobotModel(n_joints=6)\n\n    # Add a simple chain for demonstration\n    class SimpleChain:\n        def get_jacobian(self, q):\n            return np.eye(6)\n\n    robot_model.chains = {'end_effector': SimpleChain()}\n\n    controller = HybridPositionForceController(robot_model, 'end_effector')\n\n    # Current and desired states\n    current_pose = np.array([0.5, 0.3, 0.8, 0, 0, 0, 1])\n    desired_pose = np.array([0.6, 0.4, 0.8, 0, 0, 0, 1])\n\n    current_force = np.array([5.0, 2.0, 0.0, 0.1, 0.05, 0.0])\n    desired_force = np.array([10.0, 0.0, 0.0, 0.0, 0.0, 0.0])  # Force control in x direction\n\n    current_joints = np.zeros(6)\n    current_joints_vel = np.zeros(6)\n\n    # Compute hybrid control torques\n    tau_cmd, x_ddot_cmd = controller.update(\n        current_joints, current_joints_vel,\n        desired_pose, current_pose,\n        desired_force, current_force\n    )\n\n    print(f\"Hybrid control torques: {tau_cmd}\")\n    print(f\"Cartesian acceleration command: {x_ddot_cmd}\")\n\n    return tau_cmd\n```\n\n## Advanced Dynamic Control Techniques\n\n### Operational Space Control\n\nOperational space control allows direct control of task-space variables while maintaining dynamic consistency:\n\n```python\nclass OperationalSpaceController:\n    \"\"\"\n    Operational Space Controller (OSC)\n    \"\"\"\n\n    def __init__(self, robot_model, chain_name, dt=0.005):\n        self.robot_model = robot_model\n        self.chain_name = chain_name\n        self.dt = dt\n\n        # Task space controller gains\n        self.Kp_task = np.diag([1000, 1000, 1000, 100, 100, 100])  # Position gains\n        self.Kd_task = np.diag([100, 100, 100, 50, 50, 50])        # Velocity gains\n\n        # Null space controller gains (for secondary tasks)\n        self.Kp_null = np.diag([10, 10, 10, 10, 10, 10])\n\n    def compute_operational_space_control(self, q, q_dot, desired_x, current_x,\n                                        desired_x_dot=None, current_x_dot=None,\n                                        secondary_task=None):\n        \"\"\"\n        Compute operational space control\n        \"\"\"\n        chain = self.robot_model.chains[self.chain_name]\n        J = chain.get_jacobian(q)\n\n        # Compute task space error\n        x_error = desired_x - current_x\n\n        if desired_x_dot is not None and current_x_dot is not None:\n            x_dot_error = desired_x_dot - current_x_dot\n        else:\n            # Estimate from position error\n            x_dot_error = x_error / self.dt  # Simplified\n\n        # Compute task space acceleration command\n        x_ddot_cmd = -self.Kp_task @ x_error - self.Kd_task @ x_dot_error\n\n        # Compute operational space inertia\n        M = self.robot_model.compute_mass_matrix(q)\n        M_inv = np.linalg.inv(M)\n\n        # Lambda = (J * M# * J^T)# (task space inertia)\n        JMJt = J @ M_inv @ J.T\n        Lambda = np.linalg.inv(JMJt)\n\n        # Compute operational space forces\n        F_task = Lambda @ x_ddot_cmd\n\n        # Compute joint torques for primary task\n        tau_task = J.T @ F_task\n\n        # Compute null space torques for secondary tasks\n        tau_null = np.zeros(len(q))\n\n        if secondary_task is not None:\n            # Compute null space projection: N = I - M# * J^T * Lambda * J\n            N = np.eye(len(q)) - M_inv @ J.T @ Lambda @ J\n\n            # Apply secondary task in null space\n            tau_null_task = self.Kp_null @ (secondary_task - q)  # Simple joint centering\n            tau_null = N @ tau_null_task\n\n        # Add gravity compensation\n        g = self.robot_model.compute_gravity_vector(q)\n        tau_gravity = g\n\n        # Total torque\n        tau_total = tau_task + tau_null + tau_gravity\n\n        return tau_total, F_task\n\n    def compute_jacobian_derivative(self, q, q_dot):\n        \"\"\"\n        Compute Jacobian derivative J#\n        This is needed for more accurate operational space control\n        \"\"\"\n        # Simplified computation - in practice, this involves more complex derivatives\n        J_dot = np.zeros_like(self.robot_model.chains[self.chain_name].get_jacobian(q))\n        return J_dot\n\ndef example_operational_space_control():\n    \"\"\"\n    Example of operational space control\n    \"\"\"\n    # Create robot model\n    robot_model = SimpleRobotModel(n_joints=6)\n\n    # Add a simple chain for demonstration\n    class SimpleChain:\n        def get_jacobian(self, q):\n            return np.eye(6)\n\n        def compute_mass_matrix(self, q):\n            return np.eye(6) * 2.0  # Simplified mass matrix\n\n        def compute_gravity_vector(self, q):\n            return np.zeros(6)  # Simplified gravity\n\n    robot_model.chains = {'end_effector': SimpleChain()}\n    robot_model.compute_mass_matrix = lambda q: np.eye(6) * 2.0\n    robot_model.compute_gravity_vector = lambda q: np.zeros(6)\n\n    controller = OperationalSpaceController(robot_model, 'end_effector')\n\n    # Task space variables\n    current_x = np.array([0.5, 0.3, 0.8, 0.0, 0.0, 0.0])  # [pos, rot]\n    desired_x = np.array([0.6, 0.4, 0.8, 0.0, 0.0, 0.0])\n\n    current_q = np.zeros(6)\n    current_q_dot = np.zeros(6)\n\n    # Compute operational space control\n    tau_total, F_task = controller.compute_operational_space_control(\n        current_q, current_q_dot, desired_x, current_x\n    )\n\n    print(f\"Operational space control torques: {tau_total}\")\n    print(f\"Task space forces: {F_task}\")\n\n    return tau_total\n```\n\n### Adaptive Control for Unknown Dynamics\n\nAdaptive control techniques can handle uncertainties in robot dynamics:\n\n```python\nclass AdaptiveController:\n    \"\"\"\n    Adaptive controller for unknown robot dynamics\n    \"\"\"\n\n    def __init__(self, n_dof, dt=0.005):\n        self.n_dof = n_dof\n        self.dt = dt\n\n        # Adaptive parameters\n        self.theta_hat = np.zeros(n_dof)  # Estimated parameters\n        self.P = np.eye(n_dof) * 1000    # Parameter covariance matrix\n        self.gamma = 1.0                 # Learning rate\n\n        # Control gains\n        self.Kp = np.eye(n_dof) * 100\n        self.Kd = np.eye(n_dof) * 20\n\n    def regressor_matrix(self, q, q_dot, q_ddot_desired):\n        \"\"\"\n        Compute regressor matrix Y such that tau = Y * theta\n        This is a simplified example - in practice, this would be more complex\n        \"\"\"\n        # For a simple 1-DOF system: tau = m*q## + c*q# + g\n        # Y = [q##, q#, 1], theta = [m, c, g]\n        # For n-DOF, this becomes more complex\n\n        # Simplified regressor (this is just an example structure)\n        Y = np.zeros((self.n_dof, self.n_dof))\n        for i in range(self.n_dof):\n            Y[i, i] = q_ddot_desired[i]  # Acceleration terms\n            Y[i, (i+1) % self.n_dof] = q_dot[i]  # Velocity terms\n            Y[i, min(i+2, self.n_dof-1)] = 1.0   # Gravity terms (simplified)\n\n        return Y\n\n    def update(self, q, q_dot, q_desired, q_dot_desired, q_ddot_desired):\n        \"\"\"\n        Update adaptive controller\n        \"\"\"\n        # Compute tracking error\n        q_error = q_desired - q\n        q_dot_error = q_dot_desired - q_dot\n\n        # Compute control signal\n        s = q_dot_error + self.Kp @ q_error  # Sliding surface\n\n        # Regressor matrix\n        Y = self.regressor_matrix(q, q_dot, q_ddot_desired)\n\n        # Adaptive control law\n        tau_cmd = Y @ self.theta_hat - self.Kd @ s\n\n        # Parameter update law\n        # theta#_hat = gamma * P * Y^T * s\n        dtheta = self.gamma * self.P @ Y.T @ s * self.dt\n        self.theta_hat = self.theta_hat + dtheta\n\n        # Update parameter covariance\n        # P# = gamma * (Y^T * s * s^T * Y - P)\n        P_dot = self.gamma * (Y.T @ s @ s.T @ Y - self.P)\n        self.P = self.P + P_dot * self.dt\n\n        return tau_cmd, self.theta_hat\n\nclass RobustController:\n    \"\"\"\n    Robust controller to handle modeling uncertainties and disturbances\n    \"\"\"\n\n    def __init__(self, n_dof, dt=0.005):\n        self.n_dof = n_dof\n        self.dt = dt\n\n        # Control gains\n        self.Kp = np.eye(n_dof) * 100\n        self.Kd = np.eye(n_dof) * 20\n\n        # Robustness parameters\n        self.k_robust = 5.0  # Robust control gain\n        self.delta = 0.1     # Boundary layer thickness\n\n    def update(self, q, q_dot, q_desired, q_dot_desired, q_ddot_desired, disturbance_estimate=0):\n        \"\"\"\n        Update robust controller\n        \"\"\"\n        # Compute tracking error\n        q_error = q_desired - q\n        q_dot_error = q_dot_desired - q_dot\n\n        # Compute desired acceleration\n        q_ddot_cmd = q_ddot_desired + self.Kp @ q_error + self.Kd @ q_dot_error\n\n        # Robust control term to handle uncertainties\n        s = q_dot_error + self.Kp @ q_error  # Sliding surface\n\n        # Boundary layer saturation function\n        robust_term = np.zeros(self.n_dof)\n        for i in range(self.n_dof):\n            if abs(s[i]) <= self.delta:\n                robust_term[i] = s[i] / self.delta\n            else:\n                robust_term[i] = np.sign(s[i])\n\n        tau_robust = -self.k_robust * robust_term - disturbance_estimate\n\n        # Total control\n        tau_cmd = tau_robust\n\n        return tau_cmd\n```\n\n## Force/Torque Sensing and Control\n\n### Sensor Integration\n\nProper integration of force/torque sensors is crucial for force control:\n\n```python\nclass ForceTorqueSensor:\n    \"\"\"\n    Force/Torque sensor interface and processing\n    \"\"\"\n\n    def __init__(self, sensor_noise_std=0.1, bias_drift_rate=1e-5):\n        self.bias = np.zeros(6)  # 3 forces + 3 torques\n        self.bias_drift_rate = bias_drift_rate\n        self.noise_std = sensor_noise_std\n        self.raw_reading = np.zeros(6)\n\n    def read_sensor(self):\n        \"\"\"\n        Simulate sensor reading with noise and bias\n        \"\"\"\n        # Add noise\n        noise = np.random.normal(0, self.noise_std, 6)\n\n        # Simulate bias drift\n        self.bias += np.random.normal(0, self.bias_drift_rate, 6)\n\n        # Return biased and noisy reading\n        return self.raw_reading + self.bias + noise\n\n    def bias_calibration(self, num_samples=100):\n        \"\"\"\n        Calibrate sensor bias (assuming no external forces)\n        \"\"\"\n        samples = []\n        for _ in range(num_samples):\n            samples.append(self.read_sensor())\n\n        # Average to estimate bias\n        measured_bias = np.mean(samples, axis=0)\n        self.bias = measured_bias\n        return self.bias\n\n    def filter_reading(self, raw_reading, filter_coeff=0.1):\n        \"\"\"\n        Apply low-pass filter to sensor reading\n        \"\"\"\n        if not hasattr(self, 'filtered_reading'):\n            self.filtered_reading = raw_reading\n        else:\n            self.filtered_reading = (1 - filter_coeff) * self.filtered_reading + filter_coeff * raw_reading\n\n        return self.filtered_reading\n\nclass CartesianForceController:\n    \"\"\"\n    Cartesian force controller with sensor integration\n    \"\"\"\n\n    def __init__(self, robot_model, chain_name, dt=0.005):\n        self.robot_model = robot_model\n        self.chain_name = chain_name\n        self.dt = dt\n\n        # Force control gains\n        self.Kp_force = np.diag([100, 100, 100, 50, 50, 50])\n        self.Ki_force = np.diag([10, 10, 10, 5, 5, 5])\n        self.Kd_force = np.diag([20, 20, 20, 10, 10, 10])\n\n        # Force error integrator\n        self.force_error_integral = np.zeros(6)\n        self.max_integral = 100  # Anti-windup\n\n        # Sensor\n        self.ft_sensor = ForceTorqueSensor()\n\n    def update(self, q, q_dot, desired_wrench, current_wrench, contact_frame='world'):\n        \"\"\"\n        Update Cartesian force controller\n        desired_wrench: [Fx, Fy, Fz, Tx, Ty, Tz]\n        current_wrench: Measured wrench from sensor\n        \"\"\"\n        # Compute force error\n        force_error = desired_wrench - current_wrench\n\n        # Integrate error (with anti-windup)\n        self.force_error_integral = self.force_error_integral + force_error * self.dt\n        self.force_error_integral = np.clip(self.force_error_integral, -self.max_integral, self.max_integral)\n\n        # Compute Cartesian force command\n        F_cmd = (self.Kp_force @ force_error +\n                self.Ki_force @ self.force_error_integral +\n                self.Kd_force @ np.zeros(6))  # Simplified - would need force derivative\n\n        # Get Jacobian transpose to map forces to joint torques\n        chain = self.robot_model.chains[self.chain_name]\n        J = chain.get_jacobian(q)\n\n        # Map Cartesian forces to joint torques: τ = J^T * F\n        tau_cmd = J.T @ F_cmd\n\n        return tau_cmd, F_cmd\n\ndef example_force_control():\n    \"\"\"\n    Example of force control with sensor integration\n    \"\"\"\n    # Create robot model\n    robot_model = SimpleRobotModel(n_joints=6)\n\n    # Add a simple chain for demonstration\n    class SimpleChain:\n        def get_jacobian(self, q):\n            return np.eye(6)\n\n    robot_model.chains = {'end_effector': SimpleChain()}\n\n    controller = CartesianForceController(robot_model, 'end_effector')\n\n    # Desired and current wrenches\n    desired_wrench = np.array([10.0, 0.0, 0.0, 0.0, 0.0, 0.0])  # 10N in x direction\n    current_wrench = np.array([5.0, 1.0, 0.1, 0.05, 0.02, 0.01])  # Measured forces\n\n    current_q = np.zeros(6)\n    current_q_dot = np.zeros(6)\n\n    # Compute force control\n    tau_cmd, F_cmd = controller.update(\n        current_q, current_q_dot, desired_wrench, current_wrench\n    )\n\n    print(f\"Force control torques: {tau_cmd}\")\n    print(f\"Cartesian force command: {F_cmd}\")\n\n    return tau_cmd\n```\n\n## Practical Implementation Considerations\n\n### Control Architecture\n\nA practical dynamic control system requires careful architecture design:\n\n```python\nclass DynamicControlSystem:\n    \"\"\"\n    Complete dynamic control system architecture\n    \"\"\"\n\n    def __init__(self, robot_model, control_frequency=200):\n        self.robot_model = robot_model\n        self.control_frequency = control_frequency\n        self.dt = 1.0 / control_frequency\n\n        # Controllers\n        self.impedance_controller = None\n        self.force_controller = None\n        self.position_controller = None\n\n        # State variables\n        self.current_q = np.zeros(robot_model.get_num_joints())\n        self.current_q_dot = np.zeros(robot_model.get_num_joints())\n        self.current_q_ddot = np.zeros(robot_model.get_num_joints())\n\n        # External force estimator\n        self.external_force_estimator = ExternalForceEstimator(robot_model)\n\n        # Safety limits\n        self.max_torque = 100.0  # Nm\n        self.max_velocity = 2.0  # rad/s\n\n    def set_impedance_controller(self, M_d, D_d, K_d):\n        \"\"\"Set up impedance controller\"\"\"\n        self.impedance_controller = ImpedanceController(M_d, D_d, K_d, self.dt)\n\n    def set_force_controller(self, Kp, Ki, Kd):\n        \"\"\"Set up force controller\"\"\"\n        self.force_controller = {\n            'Kp': Kp,\n            'Ki': Ki,\n            'Kd': Kd,\n            'error_integral': np.zeros(6),\n            'error_derivative': np.zeros(6)\n        }\n\n    def update(self, desired_state, sensor_data):\n        \"\"\"\n        Main control update function\n        desired_state: dict with desired positions, velocities, forces, etc.\n        sensor_data: dict with joint positions, velocities, forces, etc.\n        \"\"\"\n        # Update internal state\n        self.current_q = sensor_data.get('joint_positions', self.current_q)\n        self.current_q_dot = sensor_data.get('joint_velocities', self.current_q_dot)\n\n        # Estimate external forces\n        external_forces = self.external_force_estimator.estimate(\n            self.current_q, self.current_q_dot, sensor_data.get('applied_torques', np.zeros(len(self.current_q)))\n        )\n\n        # Initialize total torque\n        tau_total = np.zeros(len(self.current_q))\n\n        # Apply different control modes based on task\n        control_mode = desired_state.get('control_mode', 'position')\n\n        if control_mode == 'position':\n            tau_total += self._compute_position_control(desired_state)\n        elif control_mode == 'impedance':\n            tau_total += self._compute_impedance_control(desired_state)\n        elif control_mode == 'force':\n            tau_total += self._compute_force_control(desired_state, external_forces)\n        elif control_mode == 'hybrid':\n            tau_total += self._compute_hybrid_control(desired_state, external_forces)\n\n        # Add gravity compensation\n        dynamics = RobotDynamics(self.robot_model)\n        g = dynamics.compute_gravity_vector(self.current_q)\n        tau_total += g\n\n        # Apply safety limits\n        tau_total = np.clip(tau_total, -self.max_torque, self.max_torque)\n\n        # Check velocity limits\n        if np.any(np.abs(self.current_q_dot) > self.max_velocity):\n            # Reduce control effort if velocity limits exceeded\n            tau_total *= 0.5\n\n        return tau_total\n\n    def _compute_position_control(self, desired_state):\n        \"\"\"Compute position control torques\"\"\"\n        # PD control: τ = Kp * (qd - q) + Kd * (q#d - q#)\n        q_desired = desired_state.get('q_desired', self.current_q)\n        q_dot_desired = desired_state.get('q_dot_desired', np.zeros_like(self.current_q))\n\n        Kp = desired_state.get('Kp_pos', np.eye(len(self.current_q)) * 100)\n        Kd = desired_state.get('Kd_pos', np.eye(len(self.current_q)) * 20)\n\n        pos_error = q_desired - self.current_q\n        vel_error = q_dot_desired - self.current_q_dot\n\n        tau_pos = Kp @ pos_error + Kd @ vel_error\n        return tau_pos\n\n    def _compute_impedance_control(self, desired_state):\n        \"\"\"Compute impedance control torques\"\"\"\n        if self.impedance_controller is None:\n            return np.zeros(len(self.current_q))\n\n        # This would involve more complex computation involving task space\n        # For now, return zeros\n        return np.zeros(len(self.current_q))\n\n    def _compute_force_control(self, desired_state, external_forces):\n        \"\"\"Compute force control torques\"\"\"\n        if 'force_controller' not in self.__dict__:\n            return np.zeros(len(self.current_q))\n\n        # This would involve Cartesian force control\n        # For now, return zeros\n        return np.zeros(len(self.current_q))\n\n    def _compute_hybrid_control(self, desired_state, external_forces):\n        \"\"\"Compute hybrid position/force control torques\"\"\"\n        # This would combine position and force control\n        # For now, return zeros\n        return np.zeros(len(self.current_q))\n\nclass ExternalForceEstimator:\n    \"\"\"\n    Estimate external forces using robot dynamics\n    \"\"\"\n\n    def __init__(self, robot_model, filter_coeff=0.9):\n        self.robot_model = robot_model\n        self.filter_coeff = filter_coeff\n        self.dynamics = RobotDynamics(robot_model)\n        self.estimated_force = np.zeros(robot_model.get_num_joints())\n\n    def estimate(self, q, q_dot, applied_tau, q_ddot=None):\n        \"\"\"\n        Estimate external forces using inverse dynamics residual\n        \"\"\"\n        if q_ddot is None:\n            # If acceleration not available, estimate it\n            # This is a simplified approach\n            q_ddot = np.zeros_like(q)\n\n        # Compute expected torques from dynamics\n        expected_tau = self.dynamics.inverse_dynamics(q, q_dot, q_ddot)\n\n        # External torque is the difference\n        external_tau = applied_tau - expected_tau\n\n        # Apply filtering to reduce noise\n        self.estimated_force = (self.filter_coeff * self.estimated_force +\n                               (1 - self.filter_coeff) * external_tau)\n\n        return self.estimated_force\n\ndef example_dynamic_control_system():\n    \"\"\"\n    Example of complete dynamic control system\n    \"\"\"\n    print(\"=== Dynamic Control System Example ===\")\n\n    # Create robot model\n    robot_model = SimpleRobotModel(n_joints=6)\n\n    # Initialize control system\n    control_system = DynamicControlSystem(robot_model, control_frequency=200)\n\n    # Set up controllers\n    M_d = np.eye(6) * 1.0\n    D_d = np.eye(6) * 10.0\n    K_d = np.eye(6) * 100.0\n    control_system.set_impedance_controller(M_d, D_d, K_d)\n\n    # Simulation\n    for step in range(100):\n        # Simulate sensor data\n        sensor_data = {\n            'joint_positions': np.random.normal(0, 0.1, 6),\n            'joint_velocities': np.random.normal(0, 0.1, 6),\n            'applied_torques': np.random.normal(0, 10, 6)\n        }\n\n        # Simulate desired state\n        desired_state = {\n            'control_mode': 'position',\n            'q_desired': np.zeros(6),\n            'q_dot_desired': np.zeros(6),\n            'Kp_pos': np.eye(6) * 100,\n            'Kd_pos': np.eye(6) * 20\n        }\n\n        # Update control system\n        torques = control_system.update(desired_state, sensor_data)\n\n        # Print progress every 20 steps\n        if step % 20 == 0:\n            print(f\"Step {step}: Max torque = {np.max(np.abs(torques)):.3f}\")\n\n    print(\"Dynamic control system example completed.\")\n    return torques\n\nif __name__ == \"__main__\":\n    # Run examples\n    print(\"Running Dynamics and Force Control Examples...\")\n\n    torques_rnea = example_rnea()\n    impedance_torques = example_impedance_control()\n    hybrid_torques = example_hybrid_control()\n    osc_torques = example_operational_space_control()\n    force_torques = example_force_control()\n    final_torques = example_dynamic_control_system()\n\n    print(\"\\n=== Summary ===\")\n    print(f\"RNEA example completed\")\n    print(f\"Impedance control torques: {impedance_torques}\")\n    print(f\"Hybrid control torques: {hybrid_torques}\")\n    print(f\"Operational space control torques: {osc_torques}\")\n    print(f\"Force control torques: {force_torques}\")\n    print(f\"Dynamic control system example completed\")",
    "module": "Module 3: The AI-Robot Brain (NVIDIA Isaac)",
    "chapter": "dynamics and force control.md",
    "path": "/docs/module-3/3.3-dynamics-and-force-control"
  },
  {
    "id": "module-3/3.4-training-and-evaluation",
    "title": "Chapter 3.4 – Training and Evaluation",
    "content": "\n# Chapter 3.4 – Training and Evaluation\n\n## Learning Objectives\n- Generate synthetic datasets for humanoid robot training\n- Evaluate performance metrics for humanoid navigation\n- Identify and handle failure cases in humanoid systems\n- Implement iterative improvement loops for humanoid AI systems\n\n## Dataset Generation\n\nTraining humanoid robots requires large, diverse datasets. Synthetic data generation using simulation environments like Isaac Sim provides a cost-effective approach:\n\n```python\nimport os\nimport json\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport open3d as o3d\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Tuple, Optional\nimport yaml\n\n@dataclass\nclass HumanoidTrainingSample:\n    \"\"\"Data class for humanoid training samples\"\"\"\n    image_path: str\n    depth_path: str\n    segmentation_path: str\n    pointcloud_path: str\n    joint_positions: List[float]\n    joint_velocities: List[float]\n    robot_pose: Dict[str, float]  # x, y, z, roll, pitch, yaw\n    action_labels: Dict[str, float]  # Desired joint positions, velocities\n    scenario: str  # indoor, outdoor, stairs, etc.\n    difficulty: str  # easy, medium, hard\n    timestamp: float\n\nclass DatasetGenerator:\n    def __init__(self, output_dir: str, robot_model: str = \"humanoid\"):\n        self.output_dir = output_dir\n        self.robot_model = robot_model\n        self.samples = []\n\n        # Create directory structure\n        os.makedirs(os.path.join(output_dir, \"images\"), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, \"depth\"), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, \"segmentation\"), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, \"pointclouds\"), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, \"labels\"), exist_ok=True)\n\n    def generate_scenario_dataset(self, scenario_config: Dict, num_samples: int = 1000):\n        \"\"\"Generate dataset for a specific scenario\"\"\"\n        print(f\"Generating {num_samples} samples for {scenario_config['name']} scenario\")\n\n        for i in range(num_samples):\n            # Generate environment\n            environment = self.generate_environment(scenario_config)\n\n            # Place robot in environment\n            robot_pose = self.place_robot_in_environment(environment, scenario_config)\n\n            # Capture sensor data\n            sample = self.capture_training_sample(robot_pose, environment, scenario_config)\n\n            # Apply domain randomization\n            sample = self.apply_domain_randomization(sample, scenario_config)\n\n            # Save sample\n            self.save_sample(sample, i)\n            self.samples.append(sample)\n\n            if i % 100 == 0:\n                print(f\"Generated {i}/{num_samples} samples\")\n\n    def generate_environment(self, config: Dict) -> Dict:\n        \"\"\"Generate environment based on configuration\"\"\"\n        environment = {\n            'type': config['type'],\n            'objects': [],\n            'lighting': {},\n            'textures': {}\n        }\n\n        # Add objects based on scenario\n        if config['type'] == 'indoor':\n            environment['objects'] = self.generate_indoor_objects(config)\n        elif config['type'] == 'outdoor':\n            environment['objects'] = self.generate_outdoor_objects(config)\n        elif config['type'] == 'stairs':\n            environment['objects'] = self.generate_stairs(config)\n        elif config['type'] == 'obstacle_course':\n            environment['objects'] = self.generate_obstacle_course(config)\n\n        # Configure lighting\n        environment['lighting'] = self.generate_lighting(config)\n\n        return environment\n\n    def generate_indoor_objects(self, config: Dict) -> List[Dict]:\n        \"\"\"Generate indoor environment objects\"\"\"\n        objects = []\n\n        # Add furniture\n        for i in range(np.random.randint(config.get('furniture_count', [3, 8])[0],\n                                        config.get('furniture_count', [3, 8])[1])):\n            obj_type = np.random.choice(['chair', 'table', 'shelf', 'box'])\n            obj = {\n                'type': obj_type,\n                'position': [\n                    np.random.uniform(-5, 5),\n                    np.random.uniform(-5, 5),\n                    0\n                ],\n                'rotation': [\n                    0, 0, np.random.uniform(0, 2*np.pi)\n                ],\n                'size': [\n                    np.random.uniform(0.5, 2.0),\n                    np.random.uniform(0.5, 2.0),\n                    np.random.uniform(0.5, 2.0)\n                ]\n            }\n            objects.append(obj)\n\n        return objects\n\n    def generate_outdoor_objects(self, config: Dict) -> List[Dict]:\n        \"\"\"Generate outdoor environment objects\"\"\"\n        objects = []\n\n        # Add terrain features\n        terrain_types = ['grass', 'concrete', 'gravel', 'sand']\n        for i in range(20):  # Generate patches of different terrain\n            patch = {\n                'type': np.random.choice(terrain_types),\n                'center': [\n                    np.random.uniform(-20, 20),\n                    np.random.uniform(-20, 20)\n                ],\n                'radius': np.random.uniform(2, 8),\n                'elevation': np.random.uniform(-0.1, 0.3)\n            }\n            objects.append(patch)\n\n        # Add obstacles\n        for i in range(np.random.randint(5, 15)):\n            obj_type = np.random.choice(['tree', 'rock', 'pole', 'barrier'])\n            obj = {\n                'type': obj_type,\n                'position': [\n                    np.random.uniform(-20, 20),\n                    np.random.uniform(-20, 20),\n                    0\n                ],\n                'height': np.random.uniform(1, 5),\n                'radius': np.random.uniform(0.2, 1.0)\n            }\n            objects.append(obj)\n\n        return objects\n\n    def capture_training_sample(self, robot_pose: Dict, environment: Dict, config: Dict) -> HumanoidTrainingSample:\n        \"\"\"Capture a training sample with all sensor data\"\"\"\n        # Simulate robot sensors\n        rgb_image = self.render_rgb_image(robot_pose, environment)\n        depth_image = self.render_depth_image(robot_pose, environment)\n        segmentation = self.render_segmentation(robot_pose, environment)\n        pointcloud = self.generate_pointcloud(depth_image, robot_pose)\n\n        # Generate robot state\n        joint_positions = self.generate_joint_positions(robot_pose, environment)\n        joint_velocities = self.generate_joint_velocities(joint_positions)\n\n        # Generate action labels based on scenario\n        action_labels = self.generate_action_labels(robot_pose, environment, config)\n\n        # Create sample\n        sample = HumanoidTrainingSample(\n            image_path=f\"images/sample_{len(self.samples):06d}.png\",\n            depth_path=f\"depth/sample_{len(self.samples):06d}.npy\",\n            segmentation_path=f\"segmentation/sample_{len(self.samples):06d}.png\",\n            pointcloud_path=f\"pointclouds/sample_{len(self.samples):06d}.pcd\",\n            joint_positions=joint_positions,\n            joint_velocities=joint_velocities,\n            robot_pose=robot_pose,\n            action_labels=action_labels,\n            scenario=config['name'],\n            difficulty=config.get('difficulty', 'medium'),\n            timestamp=float(len(self.samples))\n        )\n\n        return sample\n\n    def apply_domain_randomization(self, sample: HumanoidTrainingSample, config: Dict) -> HumanoidTrainingSample:\n        \"\"\"Apply domain randomization to sample\"\"\"\n        # Randomize lighting conditions\n        lighting_factor = np.random.uniform(0.5, 2.0)\n\n        # Randomize textures\n        texture_noise = np.random.uniform(0.0, 0.1)\n\n        # Randomize camera parameters\n        camera_noise = np.random.uniform(0.0, 0.05)\n\n        # Store domain randomization parameters\n        sample.domain_randomization = {\n            'lighting_factor': lighting_factor,\n            'texture_noise': texture_noise,\n            'camera_noise': camera_noise\n        }\n\n        return sample\n\n    def save_sample(self, sample: HumanoidTrainingSample, index: int):\n        \"\"\"Save training sample to disk\"\"\"\n        # Save images\n        image_path = os.path.join(self.output_dir, sample.image_path)\n        depth_path = os.path.join(self.output_dir, sample.depth_path)\n        seg_path = os.path.join(self.output_dir, sample.segmentation_path)\n        pc_path = os.path.join(self.output_dir, sample.pointcloud_path)\n\n        # Save RGB image\n        # (In simulation, this would be saved from rendered image)\n\n        # Save depth\n        np.save(depth_path, np.random.rand(480, 640))  # Placeholder\n\n        # Save segmentation\n        seg_img = (np.random.rand(480, 640) * 255).astype(np.uint8)  # Placeholder\n        Image.fromarray(seg_img).save(seg_path)\n\n        # Save point cloud\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(np.random.rand(1000, 3))  # Placeholder\n        o3d.io.write_point_cloud(pc_path, pcd)\n\n        # Save labels\n        labels_path = os.path.join(self.output_dir, \"labels\", f\"labels_{index:06d}.json\")\n        with open(labels_path, 'w') as f:\n            json.dump({\n                'joint_positions': sample.joint_positions,\n                'joint_velocities': sample.joint_velocities,\n                'robot_pose': sample.robot_pose,\n                'action_labels': sample.action_labels\n            }, f, indent=2)\n\n    def generate_action_labels(self, robot_pose: Dict, environment: Dict, config: Dict) -> Dict[str, float]:\n        \"\"\"Generate action labels for training\"\"\"\n        # This would implement scenario-specific action generation\n        # For navigation: generate next position, orientation\n        # For manipulation: generate joint targets\n        # For balance: generate balance recovery actions\n\n        action_labels = {}\n\n        if config['name'] == 'navigation':\n            # Generate navigation actions\n            action_labels['linear_velocity'] = np.random.uniform(-0.5, 0.5)\n            action_labels['angular_velocity'] = np.random.uniform(-0.5, 0.5)\n            action_labels['target_position'] = [\n                robot_pose['x'] + np.random.uniform(-1, 1),\n                robot_pose['y'] + np.random.uniform(-1, 1)\n            ]\n        elif config['name'] == 'balance':\n            # Generate balance recovery actions\n            action_labels['hip_roll'] = np.random.uniform(-0.1, 0.1)\n            action_labels['hip_pitch'] = np.random.uniform(-0.1, 0.1)\n            action_labels['ankle_roll'] = np.random.uniform(-0.1, 0.1)\n            action_labels['ankle_pitch'] = np.random.uniform(-0.1, 0.1)\n        elif config['name'] == 'manipulation':\n            # Generate manipulation actions\n            action_labels['arm_positions'] = [np.random.uniform(-1, 1) for _ in range(6)]\n            action_labels['gripper_position'] = np.random.uniform(0, 1)\n\n        return action_labels\n\n    def create_dataset_splits(self, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n        \"\"\"Create train/validation/test splits\"\"\"\n        indices = list(range(len(self.samples)))\n        np.random.shuffle(indices)\n\n        n_train = int(len(indices) * train_ratio)\n        n_val = int(len(indices) * val_ratio)\n\n        train_indices = indices[:n_train]\n        val_indices = indices[n_train:n_train+n_val]\n        test_indices = indices[n_train+n_val:]\n\n        splits = {\n            'train': [self.samples[i] for i in train_indices],\n            'validation': [self.samples[i] for i in val_indices],\n            'test': [self.samples[i] for i in test_indices]\n        }\n\n        # Save split information\n        split_info = {\n            'total_samples': len(self.samples),\n            'train_count': len(splits['train']),\n            'val_count': len(splits['validation']),\n            'test_count': len(splits['test']),\n            'ratios': {\n                'train': train_ratio,\n                'validation': val_ratio,\n                'test': test_ratio\n            }\n        }\n\n        with open(os.path.join(self.output_dir, 'dataset_splits.json'), 'w') as f:\n            json.dump(split_info, f, indent=2)\n\n        return splits\n\n# Example usage\ndef generate_humanoid_datasets():\n    generator = DatasetGenerator(\"./datasets/humanoid_training\")\n\n    # Define scenarios\n    scenarios = [\n        {\n            'name': 'indoor_navigation',\n            'type': 'indoor',\n            'difficulty': 'easy',\n            'furniture_count': [3, 8]\n        },\n        {\n            'name': 'outdoor_terrain',\n            'type': 'outdoor',\n            'difficulty': 'medium'\n        },\n        {\n            'name': 'stair_climbing',\n            'type': 'stairs',\n            'difficulty': 'hard'\n        },\n        {\n            'name': 'obstacle_avoidance',\n            'type': 'obstacle_course',\n            'difficulty': 'medium'\n        }\n    ]\n\n    # Generate datasets for each scenario\n    for scenario in scenarios:\n        generator.generate_scenario_dataset(scenario, num_samples=5000)\n\n    # Create dataset splits\n    splits = generator.create_dataset_splits()\n\n    print(f\"Dataset generation complete!\")\n    print(f\"Total samples: {len(generator.samples)}\")\n    print(f\"Train: {len(splits['train'])}, Val: {len(splits['validation'])}, Test: {len(splits['test'])}\")\n```\n\n## Performance Metrics\n\nEvaluating humanoid robot performance requires specialized metrics:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Tuple\nfrom dataclasses import dataclass\n\n@dataclass\nclass EvaluationMetrics:\n    \"\"\"Data class for evaluation metrics\"\"\"\n    # Navigation metrics\n    path_efficiency: float  # Ratio of optimal path length to actual path length\n    success_rate: float     # Percentage of successful navigations\n    time_to_completion: float  # Average time to reach goal\n\n    # Balance metrics\n    balance_stability: float  # Average deviation from stable pose\n    fall_rate: float         # Percentage of falls during tasks\n    recovery_time: float     # Average time to recover balance\n\n    # Perception metrics\n    detection_accuracy: float  # Accuracy of object detection\n    localization_precision: float  # Precision of robot localization\n    mapping_accuracy: float    # Accuracy of environment mapping\n\n    # General metrics\n    energy_efficiency: float   # Energy consumption per task\n    task_completion_rate: float  # Percentage of completed tasks\n    safety_score: float        # Overall safety assessment\n\nclass HumanoidEvaluator:\n    def __init__(self):\n        self.metrics_history = []\n        self.episode_data = []\n\n    def evaluate_navigation_performance(self, paths: List[List[Tuple]], goals: List[Tuple],\n                                      start_positions: List[Tuple]) -> Dict[str, float]:\n        \"\"\"Evaluate navigation performance metrics\"\"\"\n        path_efficiencies = []\n        success_count = 0\n\n        for path, goal, start in zip(paths, goals, start_positions):\n            if path and self.distance(path[-1], goal) < 0.5:  # Within 0.5m of goal\n                success_count += 1\n\n                # Calculate path efficiency\n                optimal_distance = self.distance(start, goal)\n                actual_distance = self.calculate_path_length(path)\n\n                if optimal_distance > 0:\n                    efficiency = optimal_distance / actual_distance if actual_distance > 0 else 0\n                    path_efficiencies.append(min(efficiency, 1.0))  # Cap at 1.0\n\n        success_rate = success_count / len(goals) if goals else 0\n        avg_efficiency = np.mean(path_efficiencies) if path_efficiencies else 0\n\n        return {\n            'path_efficiency': avg_efficiency,\n            'success_rate': success_rate,\n            'average_path_length': np.mean([self.calculate_path_length(p) for p in paths if p]) if paths else 0\n        }\n\n    def evaluate_balance_performance(self, robot_states: List[Dict],\n                                   balance_threshold: float = 0.3) -> Dict[str, float]:\n        \"\"\"Evaluate balance performance metrics\"\"\"\n        balance_deviation = []\n        falls = 0\n        recovery_times = []\n\n        for i, state in enumerate(robot_states):\n            # Calculate balance based on COM and ZMP\n            com_position = state.get('center_of_mass', np.array([0, 0, 0]))\n            zmp_position = state.get('zero_moment_point', np.array([0, 0, 0]))\n\n            # Calculate deviation from stable region\n            balance_error = np.linalg.norm(com_position[:2] - zmp_position[:2])\n            balance_deviation.append(balance_error)\n\n            # Check for falls (large balance errors)\n            if balance_error > balance_threshold:\n                falls += 1\n                # Calculate recovery time if next states show recovery\n                recovery_start = i\n                while recovery_start < len(robot_states):\n                    next_state = robot_states[recovery_start]\n                    next_com = next_state.get('center_of_mass', np.array([0, 0, 0]))\n                    next_zmp = next_state.get('zero_moment_point', np.array([0, 0, 0]))\n                    next_error = np.linalg.norm(next_com[:2] - next_zmp[:2])\n\n                    if next_error <= balance_threshold:\n                        recovery_times.append(recovery_start - i)\n                        break\n                    recovery_start += 1\n\n        avg_deviation = np.mean(balance_deviation) if balance_deviation else 0\n        fall_rate = falls / len(robot_states) if robot_states else 0\n        avg_recovery_time = np.mean(recovery_times) if recovery_times else 0\n\n        return {\n            'balance_stability': 1.0 - avg_deviation,  # Lower deviation = higher stability\n            'fall_rate': fall_rate,\n            'recovery_time': avg_recovery_time\n        }\n\n    def evaluate_perception_performance(self, detections: List[Dict],\n                                      ground_truth: List[Dict]) -> Dict[str, float]:\n        \"\"\"Evaluate perception performance metrics\"\"\"\n        ious = []\n        correct_classifications = 0\n        total_detections = 0\n\n        for det, gt in zip(detections, ground_truth):\n            # Calculate IoU for each detection\n            if 'bbox' in det and 'bbox' in gt:\n                iou = self.calculate_iou(det['bbox'], gt['bbox'])\n                ious.append(iou)\n\n            # Check classification accuracy\n            if det.get('class') == gt.get('class'):\n                correct_classifications += 1\n            total_detections += 1\n\n        detection_accuracy = np.mean(ious) if ious else 0\n        classification_accuracy = correct_classifications / total_detections if total_detections > 0 else 0\n\n        return {\n            'detection_accuracy': detection_accuracy,\n            'classification_accuracy': classification_accuracy,\n            'average_iou': np.mean(ious) if ious else 0\n        }\n\n    def calculate_iou(self, bbox1: List[float], bbox2: List[float]) -> float:\n        \"\"\"Calculate Intersection over Union\"\"\"\n        # bbox format: [x1, y1, x2, y2]\n        x1_inter = max(bbox1[0], bbox2[0])\n        y1_inter = max(bbox1[1], bbox2[1])\n        x2_inter = min(bbox1[2], bbox2[2])\n        y2_inter = min(bbox1[3], bbox2[3])\n\n        if x2_inter <= x1_inter or y2_inter <= y1_inter:\n            return 0.0\n\n        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n\n        union_area = area1 + area2 - inter_area\n        return inter_area / union_area if union_area > 0 else 0\n\n    def evaluate_energy_efficiency(self, joint_trajectories: List[np.ndarray],\n                                 time_intervals: List[float]) -> Dict[str, float]:\n        \"\"\"Evaluate energy efficiency based on joint movements\"\"\"\n        total_energy = 0.0\n        total_time = sum(time_intervals)\n\n        for trajectory, dt in zip(joint_trajectories, time_intervals):\n            # Calculate energy based on joint torques and velocities\n            # This is a simplified model - real implementation would use motor models\n            if len(trajectory) > 1:\n                velocities = np.diff(trajectory, axis=0) / dt\n                # Energy proportional to square of velocity\n                energy = np.sum(velocities ** 2) * dt\n                total_energy += energy\n\n        energy_efficiency = total_time / (total_energy + 1e-6)  # Higher is better\n\n        return {\n            'total_energy_consumption': total_energy,\n            'energy_efficiency': energy_efficiency,\n            'average_power': total_energy / total_time if total_time > 0 else 0\n        }\n\n    def generate_evaluation_report(self, metrics: EvaluationMetrics) -> str:\n        \"\"\"Generate comprehensive evaluation report\"\"\"\n        report = f\"\"\"\n        HUMANOID ROBOT EVALUATION REPORT\n        =================================\n\n        NAVIGATION PERFORMANCE:\n        - Path Efficiency: {metrics.path_efficiency:.3f} ({self.rate_performance(metrics.path_efficiency)})\n        - Success Rate: {metrics.success_rate:.1%} ({self.rate_performance(metrics.success_rate)})\n        - Time to Completion: {metrics.time_to_completion:.2f}s\n\n        BALANCE PERFORMANCE:\n        - Balance Stability: {metrics.balance_stability:.3f} ({self.rate_performance(metrics.balance_stability)})\n        - Fall Rate: {metrics.fall_rate:.1%} ({self.rate_performance(1-metrics.fall_rate)})\n        - Recovery Time: {metrics.recovery_time:.2f}s\n\n        PERCEPTION PERFORMANCE:\n        - Detection Accuracy: {metrics.detection_accuracy:.1%} ({self.rate_performance(metrics.detection_accuracy)})\n        - Localization Precision: {metrics.localization_precision:.3f} ({self.rate_performance(metrics.localization_precision)})\n        - Mapping Accuracy: {metrics.mapping_accuracy:.1%} ({self.rate_performance(metrics.mapping_accuracy)})\n\n        GENERAL PERFORMANCE:\n        - Energy Efficiency: {metrics.energy_efficiency:.3f} ({self.rate_performance(metrics.energy_efficiency)})\n        - Task Completion Rate: {metrics.task_completion_rate:.1%} ({self.rate_performance(metrics.task_completion_rate)})\n        - Safety Score: {metrics.safety_score:.3f} ({self.rate_performance(metrics.safety_score)})\n\n        OVERALL ASSESSMENT: {self.calculate_overall_score(metrics)}\n        \"\"\"\n\n        return report\n\n    def rate_performance(self, value: float) -> str:\n        \"\"\"Rate performance level\"\"\"\n        if value >= 0.9:\n            return \"EXCELLENT\"\n        elif value >= 0.8:\n            return \"GOOD\"\n        elif value >= 0.6:\n            return \"FAIR\"\n        elif value >= 0.4:\n            return \"POOR\"\n        else:\n            return \"CRITICAL\"\n\n    def calculate_overall_score(self, metrics: EvaluationMetrics) -> str:\n        \"\"\"Calculate overall performance score\"\"\"\n        # Weighted average of all metrics\n        weights = {\n            'navigation': 0.25,\n            'balance': 0.25,\n            'perception': 0.2,\n            'efficiency': 0.15,\n            'safety': 0.15\n        }\n\n        nav_score = (metrics.path_efficiency + metrics.success_rate) / 2\n        balance_score = (metrics.balance_stability + (1 - metrics.fall_rate)) / 2\n        perception_score = (metrics.detection_accuracy + metrics.localization_precision) / 2\n        efficiency_score = metrics.energy_efficiency  # Normalized\n        safety_score = metrics.safety_score\n\n        overall = (weights['navigation'] * nav_score +\n                  weights['balance'] * balance_score +\n                  weights['perception'] * perception_score +\n                  weights['efficiency'] * efficiency_score +\n                  weights['safety'] * safety_score)\n\n        return self.rate_performance(overall)\n\n# Visualization tools for evaluation\nclass EvaluationVisualizer:\n    def __init__(self):\n        self.figures = []\n\n    def plot_performance_trends(self, metrics_history: List[EvaluationMetrics],\n                               metric_names: List[str] = None):\n        \"\"\"Plot performance trends over time\"\"\"\n        if metric_names is None:\n            metric_names = ['path_efficiency', 'success_rate', 'balance_stability',\n                           'detection_accuracy', 'energy_efficiency']\n\n        fig, axes = plt.subplots(len(metric_names), 1, figsize=(12, 4*len(metric_names)))\n        if len(metric_names) == 1:\n            axes = [axes]\n\n        for i, metric_name in enumerate(metric_names):\n            values = [getattr(m, metric_name, 0) for m in metrics_history]\n            axes[i].plot(values, marker='o')\n            axes[i].set_title(f'{metric_name.replace(\"_\", \" \").title()} Over Time')\n            axes[i].set_xlabel('Evaluation Episode')\n            axes[i].set_ylabel('Score')\n            axes[i].grid(True)\n\n        plt.tight_layout()\n        self.figures.append(fig)\n        plt.show()\n\n    def plot_confusion_matrix(self, predictions: List[int], ground_truth: List[int],\n                             class_names: List[str]):\n        \"\"\"Plot confusion matrix for classification tasks\"\"\"\n        from sklearn.metrics import confusion_matrix\n        import seaborn as sns\n\n        cm = confusion_matrix(ground_truth, predictions)\n\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=class_names, yticklabels=class_names)\n        plt.title('Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.show()\n```\n\n## Failure Case Analysis\n\nUnderstanding and handling failure cases is crucial for humanoid robot systems:\n\n```python\nfrom enum import Enum\nfrom typing import List, Dict, Tuple, Optional\nimport traceback\n\nclass FailureType(Enum):\n    \"\"\"Types of failures in humanoid systems\"\"\"\n    BALANCE_LOSS = \"balance_loss\"\n    COLLISION = \"collision\"\n    SENSOR_MALFUNCTION = \"sensor_malfunction\"\n    CONTROL_ERROR = \"control_error\"\n    LOCALIZATION_FAILURE = \"localization_failure\"\n    PLANNING_FAILURE = \"planning_failure\"\n    COMMUNICATION_ERROR = \"communication_error\"\n    POWER_ISSUE = \"power_issue\"\n    ENVIRONMENT_CHANGE = \"environment_change\"\n\nclass FailureCase:\n    \"\"\"Data class for failure cases\"\"\"\n    def __init__(self, failure_type: FailureType, timestamp: float,\n                 robot_state: Dict, environment_state: Dict,\n                 triggered_action: str, error_message: str):\n        self.failure_type = failure_type\n        self.timestamp = timestamp\n        self.robot_state = robot_state\n        self.environment_state = environment_state\n        self.triggered_action = triggered_action\n        self.error_message = error_message\n        self.context = {}  # Additional context for analysis\n        self.recovery_actions = []  # Actions taken to recover\n\nclass FailureAnalyzer:\n    def __init__(self):\n        self.failure_cases = []\n        self.failure_patterns = {}\n        self.recovery_strategies = {}\n\n    def record_failure(self, failure_case: FailureCase):\n        \"\"\"Record a failure case for analysis\"\"\"\n        self.failure_cases.append(failure_case)\n        self.analyze_failure_pattern(failure_case)\n\n    def analyze_failure_pattern(self, failure_case: FailureCase):\n        \"\"\"Analyze patterns in failures\"\"\"\n        failure_type = failure_case.failure_type\n\n        if failure_type not in self.failure_patterns:\n            self.failure_patterns[failure_type] = {\n                'count': 0,\n                'timestamps': [],\n                'common_triggers': {},\n                'environment_factors': [],\n                'robot_state_patterns': []\n            }\n\n        pattern = self.failure_patterns[failure_type]\n        pattern['count'] += 1\n        pattern['timestamps'].append(failure_case.timestamp)\n\n        # Track common triggers\n        trigger = failure_case.triggered_action\n        if trigger in pattern['common_triggers']:\n            pattern['common_triggers'][trigger] += 1\n        else:\n            pattern['common_triggers'][trigger] = 1\n\n        # Store environment and robot state for pattern analysis\n        pattern['environment_factors'].append(failure_case.environment_state)\n        pattern['robot_state_patterns'].append(failure_case.robot_state)\n\n    def identify_common_failure_scenarios(self) -> Dict[str, List]:\n        \"\"\"Identify common failure scenarios\"\"\"\n        scenarios = {}\n\n        for failure_type, pattern in self.failure_patterns.items():\n            # Analyze what conditions lead to this failure type\n            common_scenario = self.analyze_scenario(pattern)\n            scenarios[failure_type.value] = common_scenario\n\n        return scenarios\n\n    def analyze_scenario(self, pattern: Dict) -> List[Dict]:\n        \"\"\"Analyze failure scenario\"\"\"\n        scenario_analysis = []\n\n        # Find most common triggers\n        if pattern['common_triggers']:\n            most_common_trigger = max(pattern['common_triggers'],\n                                    key=pattern['common_triggers'].get)\n            scenario_analysis.append({\n                'aspect': 'common_triggers',\n                'value': most_common_trigger,\n                'frequency': pattern['common_triggers'][most_common_trigger] / pattern['count']\n            })\n\n        # Analyze temporal patterns\n        if len(pattern['timestamps']) > 1:\n            time_intervals = np.diff(sorted(pattern['timestamps']))\n            avg_interval = np.mean(time_intervals)\n            scenario_analysis.append({\n                'aspect': 'temporal_pattern',\n                'value': f'Average interval: {avg_interval:.2f}s',\n                'frequency': 1.0\n            })\n\n        # Analyze environment factors\n        if pattern['environment_factors']:\n            # This would involve more complex analysis\n            scenario_analysis.append({\n                'aspect': 'environment_analysis',\n                'value': 'Requires detailed environmental analysis',\n                'frequency': 1.0\n            })\n\n        return scenario_analysis\n\n    def suggest_prevention_strategies(self, failure_type: FailureType) -> List[str]:\n        \"\"\"Suggest strategies to prevent specific failure types\"\"\"\n        strategies = {\n            FailureType.BALANCE_LOSS: [\n                \"Implement more aggressive balance control\",\n                \"Reduce walking speed on uneven terrain\",\n                \"Increase sensor fusion for better state estimation\",\n                \"Add pre-emptive balance adjustments\",\n                \"Improve step planning for stability\"\n            ],\n            FailureType.COLLISION: [\n                \"Increase safety margins in path planning\",\n                \"Improve obstacle detection sensitivity\",\n                \"Add more frequent collision checking\",\n                \"Implement emergency stop procedures\",\n                \"Enhance sensor coverage\"\n            ],\n            FailureType.SENSOR_MALFUNCTION: [\n                \"Implement sensor redundancy\",\n                \"Add sensor health monitoring\",\n                \"Create sensor validation routines\",\n                \"Develop fallback sensing strategies\",\n                \"Regular calibration schedules\"\n            ],\n            FailureType.CONTROL_ERROR: [\n                \"Improve control loop stability\",\n                \"Add control authority monitoring\",\n                \"Implement control saturation handling\",\n                \"Enhance state estimation accuracy\",\n                \"Add control performance monitoring\"\n            ],\n            FailureType.LOCALIZATION_FAILURE: [\n                \"Increase landmark density\",\n                \"Improve sensor fusion algorithms\",\n                \"Add multiple localization methods\",\n                \"Implement localization recovery\",\n                \"Enhance map quality\"\n            ],\n            FailureType.PLANNING_FAILURE: [\n                \"Improve path planning algorithms\",\n                \"Add replanning capabilities\",\n                \"Increase planning horizon\",\n                \"Add alternative planning strategies\",\n                \"Improve environment representation\"\n            ]\n        }\n\n        return strategies.get(failure_type, [\"General error prevention measures\"])\n\n    def generate_failure_report(self) -> str:\n        \"\"\"Generate comprehensive failure analysis report\"\"\"\n        report = \"HUMANOID ROBOT FAILURE ANALYSIS REPORT\\n\"\n        report += \"=\" * 50 + \"\\n\\n\"\n\n        report += f\"Total Failures Recorded: {len(self.failure_cases)}\\n\"\n        report += f\"Failure Types Identified: {len(self.failure_patterns)}\\n\\n\"\n\n        for failure_type, pattern in self.failure_patterns.items():\n            report += f\"FAILURE TYPE: {failure_type.value.upper()}\\n\"\n            report += f\"- Occurrences: {pattern['count']}\\n\"\n            report += f\"- Frequency: {pattern['count']/len(self.failure_cases)*100:.1f}%\\n\"\n\n            if pattern['common_triggers']:\n                most_common = max(pattern['common_triggers'],\n                                key=pattern['common_triggers'].get)\n                report += f\"- Most Common Trigger: {most_common} ({pattern['common_triggers'][most_common]} times)\\n\"\n\n            # Prevention strategies\n            strategies = self.suggest_prevention_strategies(failure_type)\n            report += \"- Prevention Strategies:\\n\"\n            for strategy in strategies[:3]:  # Show top 3 strategies\n                report += f\"  * {strategy}\\n\"\n\n            report += \"\\n\"\n\n        return report\n\n# Iterative Improvement Loop\nclass ImprovementLoop:\n    def __init__(self, evaluator: HumanoidEvaluator, analyzer: FailureAnalyzer):\n        self.evaluator = evaluator\n        self.analyzer = analyzer\n        self.iteration_count = 0\n        self.performance_history = []\n\n    def run_iteration(self, robot_system, test_scenarios: List[Dict]):\n        \"\"\"Run one iteration of the improvement loop\"\"\"\n        self.iteration_count += 1\n        print(f\"Starting iteration {self.iteration_count}\")\n\n        # 1. Test the current system\n        test_results = self.evaluate_current_system(robot_system, test_scenarios)\n\n        # 2. Analyze failures\n        failure_analysis = self.analyze_failures(robot_system)\n\n        # 3. Generate improvement recommendations\n        improvements = self.generate_improvements(test_results, failure_analysis)\n\n        # 4. Implement improvements\n        self.implement_improvements(robot_system, improvements)\n\n        # 5. Log results\n        self.performance_history.append({\n            'iteration': self.iteration_count,\n            'test_results': test_results,\n            'failures': failure_analysis,\n            'improvements': improvements\n        })\n\n        print(f\"Iteration {self.iteration_count} completed\")\n\n    def evaluate_current_system(self, robot_system, test_scenarios: List[Dict]) -> Dict:\n        \"\"\"Evaluate the current system performance\"\"\"\n        # Run test scenarios and collect metrics\n        results = {}\n\n        for scenario in test_scenarios:\n            scenario_results = robot_system.run_scenario(scenario)\n            results[scenario['name']] = scenario_results\n\n        return results\n\n    def analyze_failures(self, robot_system) -> Dict:\n        \"\"\"Analyze recent failures\"\"\"\n        # This would interface with the failure analyzer\n        return self.analyzer.identify_common_failure_scenarios()\n\n    def generate_improvements(self, test_results: Dict, failure_analysis: Dict) -> List[Dict]:\n        \"\"\"Generate improvement recommendations\"\"\"\n        improvements = []\n\n        # Based on test results and failure analysis\n        for scenario_name, results in test_results.items():\n            if results.get('success_rate', 1.0) < 0.8:  # Below 80% success rate\n                improvements.append({\n                    'type': 'algorithm_improvement',\n                    'target': scenario_name,\n                    'suggested_fix': f'Improve performance in {scenario_name}',\n                    'priority': 'high'\n                })\n\n        # Based on failure patterns\n        for failure_type, patterns in failure_analysis.items():\n            if patterns:  # If there are identified patterns\n                improvements.append({\n                    'type': 'failure_prevention',\n                    'target': failure_type,\n                    'suggested_fix': f'Address {failure_type} failure patterns',\n                    'priority': 'medium'\n                })\n\n        return improvements\n\n    def implement_improvements(self, robot_system, improvements: List[Dict]):\n        \"\"\"Implement suggested improvements\"\"\"\n        for improvement in improvements:\n            if improvement['type'] == 'algorithm_improvement':\n                # Implement algorithm changes\n                robot_system.update_algorithm(improvement['target'])\n            elif improvement['type'] == 'failure_prevention':\n                # Add failure prevention measures\n                robot_system.add_failure_prevention(improvement['target'])\n\n# Example usage\ndef run_humanoid_improvement_loop():\n    evaluator = HumanoidEvaluator()\n    analyzer = FailureAnalyzer()\n    improvement_loop = ImprovementLoop(evaluator, analyzer)\n\n    # Define test scenarios\n    test_scenarios = [\n        {'name': 'indoor_navigation', 'type': 'navigation', 'difficulty': 'easy'},\n        {'name': 'outdoor_terrain', 'type': 'navigation', 'difficulty': 'medium'},\n        {'name': 'balance_test', 'type': 'balance', 'difficulty': 'hard'},\n        {'name': 'object_detection', 'type': 'perception', 'difficulty': 'medium'}\n    ]\n\n    # Mock robot system\n    class MockRobotSystem:\n        def run_scenario(self, scenario):\n            # Simulate running a scenario\n            import random\n            return {\n                'success_rate': random.uniform(0.5, 1.0),\n                'time_taken': random.uniform(10, 60),\n                'energy_used': random.uniform(50, 200)\n            }\n\n        def update_algorithm(self, target):\n            print(f\"Updating algorithm for {target}\")\n\n        def add_failure_prevention(self, target):\n            print(f\"Adding failure prevention for {target}\")\n\n    robot_system = MockRobotSystem()\n\n    # Run several iterations\n    for i in range(5):\n        improvement_loop.run_iteration(robot_system, test_scenarios)\n\n    # Generate final reports\n    print(\"\\nFINAL EVALUATION REPORT:\")\n    print(evaluator.generate_evaluation_report(\n        EvaluationMetrics(\n            path_efficiency=0.85,\n            success_rate=0.92,\n            time_to_completion=25.0,\n            balance_stability=0.88,\n            fall_rate=0.05,\n            recovery_time=2.0,\n            detection_accuracy=0.91,\n            localization_precision=0.95,\n            mapping_accuracy=0.89,\n            energy_efficiency=0.78,\n            task_completion_rate=0.89,\n            safety_score=0.94\n        )\n    ))\n\n    print(\"\\nFAILURE ANALYSIS REPORT:\")\n    print(analyzer.generate_failure_report())\n\nif __name__ == \"__main__\":\n    run_humanoid_improvement_loop()\n```\n\n## Practical Exercise\n\n1. Generate a synthetic dataset for humanoid navigation tasks\n2. Implement evaluation metrics for balance and navigation\n3. Analyze failure cases from simulation runs\n4. Create an iterative improvement loop for performance enhancement\n\n## Summary\n\nTraining and evaluation of humanoid robots requires specialized approaches considering their unique challenges. Synthetic data generation provides diverse training scenarios, while specialized metrics evaluate balance, navigation, and perception performance. Failure analysis identifies system weaknesses, and iterative improvement loops continuously enhance performance. This systematic approach ensures humanoid robots develop robust capabilities for real-world deployment.",
    "module": "Module 3: The AI-Robot Brain (NVIDIA Isaac)",
    "chapter": "training and evaluation.md",
    "path": "/docs/module-3/3.4-training-and-evaluation"
  },
  {
    "id": "module-4/4.1-voice-to-action-pipelines",
    "title": "Chapter 4.1 – Voice-to-Action Pipelines",
    "content": "\n# Chapter 4.1 – Voice-to-Action Pipelines\n\n## Learning Objectives\n- Implement OpenAI Whisper for speech recognition in humanoid robots\n- Create speech-to-intent systems for robot command interpretation\n- Optimize for low-latency voice command processing\n- Ensure command reliability in noisy environments\n\n## OpenAI Whisper Integration\n\nOpenAI Whisper provides state-of-the-art speech recognition capabilities that are well-suited for humanoid robots. Here's how to integrate it effectively:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import JointState\nimport whisper\nimport torch\nimport numpy as np\nimport librosa\nimport threading\nimport queue\nfrom typing import Optional, Dict, Any\nimport time\n\nclass VoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__('voice_to_action_node')\n\n        # Initialize Whisper model\n        self.get_logger().info('Loading Whisper model...')\n        self.model = whisper.load_model(\"medium\")  # Choose based on compute resources\n\n        # For real-time applications, you might want to use a lighter model\n        # self.model = whisper.load_model(\"small\")  # Faster but less accurate\n        # self.model = whisper.load_model(\"tiny\")   # Fastest but least accurate\n\n        # Audio processing parameters\n        self.sample_rate = 16000  # Whisper expects 16kHz\n        self.audio_buffer = np.array([])\n        self.buffer_size = self.sample_rate * 2  # 2 seconds of audio\n        self.min_audio_length = self.sample_rate * 0.5  # Minimum 0.5 seconds\n\n        # Command processing\n        self.command_queue = queue.Queue()\n        self.intent_processor = IntentProcessor()\n\n        # Subscriptions\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            '/audio/input',\n            self.audio_callback,\n            10\n        )\n\n        # Publishers for different robot actions\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)\n        self.speech_pub = self.create_publisher(String, '/tts_input', 10)\n\n        # Timer for processing audio chunks\n        self.process_timer = self.create_timer(1.0, self.process_audio_buffer)\n\n        # Voice activity detection parameters\n        self.energy_threshold = 0.01\n        self.silence_duration = 0.5  # seconds of silence to trigger processing\n        self.last_voice_time = time.time()\n\n        self.get_logger().info('Voice-to-Action Node initialized')\n\n    def audio_callback(self, msg: AudioData):\n        \"\"\"Receive audio data and add to processing buffer\"\"\"\n        try:\n            # Convert audio data to numpy array\n            # Assuming audio data is in 16-bit PCM format\n            audio_int16 = np.frombuffer(msg.data, dtype=np.int16)\n            audio_float32 = audio_int16.astype(np.float32) / 32768.0\n\n            # Resample to 16kHz if needed\n            if msg.info.sample_rate != self.sample_rate:\n                audio_float32 = librosa.resample(\n                    audio_float32,\n                    orig_sr=msg.info.sample_rate,\n                    target_sr=self.sample_rate\n                )\n\n            # Append to buffer\n            self.audio_buffer = np.concatenate([self.audio_buffer, audio_float32])\n\n            # Check if we have enough audio and if voice activity is detected\n            if len(self.audio_buffer) >= self.min_audio_length:\n                energy = np.mean(np.abs(self.audio_buffer[-self.sample_rate:]))  # Last second\n                if energy > self.energy_threshold:\n                    self.last_voice_time = time.time()\n                elif (time.time() - self.last_voice_time) > self.silence_duration:\n                    # Silence detected after voice, process the utterance\n                    self.process_audio_buffer()\n\n            # Keep only the most recent buffer_size samples\n            if len(self.audio_buffer) > self.buffer_size:\n                self.audio_buffer = self.audio_buffer[-self.buffer_size:]\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing audio: {e}')\n\n    def process_audio_buffer(self):\n        \"\"\"Process accumulated audio buffer with Whisper\"\"\"\n        if len(self.audio_buffer) < self.min_audio_length:\n            return  # Not enough audio to process\n\n        try:\n            # Transcribe audio using Whisper\n            result = self.model.transcribe(\n                self.audio_buffer,\n                language='en',\n                temperature=0.0,  # More deterministic\n                compression_ratio_threshold=2.4,  # Filter out low-quality transcriptions\n                logprob_threshold=-1.0,  # Filter out low-confidence transcriptions\n                no_speech_threshold=0.6  # Filter out silence\n            )\n\n            transcription = result['text'].strip()\n\n            if transcription and len(transcription) > 3:  # Meaningful transcription\n                self.get_logger().info(f'Transcribed: \"{transcription}\"')\n\n                # Add to command queue for intent processing\n                self.command_queue.put({\n                    'transcription': transcription,\n                    'timestamp': time.time()\n                })\n\n                # Process commands in a separate thread to avoid blocking\n                threading.Thread(target=self.process_command, daemon=True).start()\n\n            # Clear buffer after processing\n            self.audio_buffer = np.array([])\n\n        except Exception as e:\n            self.get_logger().error(f'Error transcribing audio: {e}')\n\n    def process_command(self):\n        \"\"\"Process voice commands and convert to robot actions\"\"\"\n        try:\n            if not self.command_queue.empty():\n                command_data = self.command_queue.get_nowait()\n                transcription = command_data['transcription']\n\n                # Process intent\n                intent_result = self.intent_processor.parse_intent(transcription)\n\n                if intent_result:\n                    self.execute_robot_action(intent_result)\n                else:\n                    self.get_logger().warn(f'Could not parse intent from: \"{transcription}\"')\n                    # Respond with clarification request\n                    self.request_clarification(transcription)\n\n        except queue.Empty:\n            pass\n        except Exception as e:\n            self.get_logger().error(f'Error processing command: {e}')\n\n    def execute_robot_action(self, intent_result: Dict[str, Any]):\n        \"\"\"Execute the parsed intent as a robot action\"\"\"\n        action_type = intent_result['action']\n\n        if action_type == 'move':\n            self.execute_move_command(intent_result)\n        elif action_type == 'manipulate':\n            self.execute_manipulation_command(intent_result)\n        elif action_type == 'speak':\n            self.execute_speak_command(intent_result)\n        elif action_type == 'balance':\n            self.execute_balance_command(intent_result)\n        elif action_type == 'navigation':\n            self.execute_navigation_command(intent_result)\n        else:\n            self.get_logger().warn(f'Unknown action type: {action_type}')\n\n    def execute_move_command(self, intent_result: Dict[str, Any]):\n        \"\"\"Execute movement commands\"\"\"\n        direction = intent_result.get('direction', 'forward')\n        distance = intent_result.get('distance', 1.0)  # meters\n        speed = intent_result.get('speed', 0.5)  # m/s\n\n        cmd_vel = Twist()\n\n        if direction == 'forward':\n            cmd_vel.linear.x = speed\n        elif direction == 'backward':\n            cmd_vel.linear.x = -speed\n        elif direction == 'left':\n            cmd_vel.angular.z = speed\n        elif direction == 'right':\n            cmd_vel.angular.z = -speed\n        elif direction == 'turn_left':\n            cmd_vel.angular.z = speed\n        elif direction == 'turn_right':\n            cmd_vel.angular.z = -speed\n\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info(f'Moving {direction} at {speed}m/s for {distance/speed:.1f}s')\n\n    def execute_navigation_command(self, intent_result: Dict[str, Any]):\n        \"\"\"Execute navigation commands\"\"\"\n        destination = intent_result.get('destination', 'unknown')\n\n        # This would typically interface with navigation stack\n        # For now, just log the command\n        self.get_logger().info(f'Navigating to {destination}')\n\n        # Publish to navigation system\n        nav_cmd = String()\n        nav_cmd.data = f\"navigate_to_{destination.replace(' ', '_')}\"\n        self.speech_pub.publish(nav_cmd)  # Using speech pub temporarily\n\n    def request_clarification(self, transcription: str):\n        \"\"\"Request clarification for ambiguous commands\"\"\"\n        response = String()\n        response.data = f\"I heard you say '{transcription}' but I'm not sure what you mean. Could you please repeat or clarify?\"\n        self.speech_pub.publish(response)\n\nclass IntentProcessor:\n    \"\"\"Process voice transcriptions and extract intents\"\"\"\n\n    def __init__(self):\n        # Define command patterns\n        self.move_patterns = {\n            'forward': ['forward', 'ahead', 'go', 'move', 'straight', 'front'],\n            'backward': ['back', 'backward', 'reverse', 'behind'],\n            'left': ['left', 'port', 'starboard right'],\n            'right': ['right', 'starboard', 'port left'],\n            'turn_left': ['turn left', 'rotate left', 'pivot left'],\n            'turn_right': ['turn right', 'rotate right', 'pivot right']\n        }\n\n        self.navigation_patterns = {\n            'kitchen': ['kitchen', 'cooking area', 'food area'],\n            'bedroom': ['bedroom', 'sleeping area', 'bed area'],\n            'living_room': ['living room', 'living area', 'sitting area', 'couch'],\n            'bathroom': ['bathroom', 'restroom', 'toilet', 'shower'],\n            'office': ['office', 'study', 'work area', 'desk']\n        }\n\n        self.manipulation_patterns = {\n            'pick_up': ['pick up', 'grab', 'take', 'lift', 'get'],\n            'put_down': ['put down', 'place', 'drop', 'set'],\n            'wave': ['wave', 'hello', 'greet', 'waving'],\n            'point': ['point', 'show', 'indicate', 'direct']\n        }\n\n        self.balance_patterns = {\n            'balance': ['balance', 'steady', 'stable', 'don\\'t fall', 'keep balance'],\n            'crouch': ['crouch', 'squat', 'bend down', 'lower'],\n            'stand': ['stand', 'rise', 'up', 'straighten']\n        }\n\n    def parse_intent(self, transcription: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Parse the intent from a transcription\"\"\"\n        transcription_lower = transcription.lower()\n\n        # Check for movement commands\n        for action, patterns in self.move_patterns.items():\n            if any(pattern in transcription_lower for pattern in patterns):\n                # Extract distance if mentioned\n                distance = self.extract_distance(transcription_lower)\n                speed = self.extract_speed(transcription_lower)\n\n                return {\n                    'action': 'move',\n                    'direction': action,\n                    'distance': distance,\n                    'speed': speed\n                }\n\n        # Check for navigation commands\n        for destination, patterns in self.navigation_patterns.items():\n            if any(pattern in transcription_lower for pattern in patterns):\n                return {\n                    'action': 'navigation',\n                    'destination': destination\n                }\n\n        # Check for manipulation commands\n        for action, patterns in self.manipulation_patterns.items():\n            if any(pattern in transcription_lower for pattern in patterns):\n                return {\n                    'action': 'manipulate',\n                    'action_type': action\n                }\n\n        # Check for balance commands\n        for action, patterns in self.balance_patterns.items():\n            if any(pattern in transcription_lower for pattern in patterns):\n                return {\n                    'action': 'balance',\n                    'action_type': action\n                }\n\n        # Check for speaking commands\n        speak_keywords = ['say', 'speak', 'tell', 'repeat', 'announce']\n        if any(keyword in transcription_lower for keyword in speak_keywords):\n            return {\n                'action': 'speak',\n                'text': self.extract_speech_content(transcription_lower)\n            }\n\n        return None  # No recognizable intent found\n\n    def extract_distance(self, text: str) -> float:\n        \"\"\"Extract distance from text\"\"\"\n        # Look for patterns like \"2 meters\", \"3 feet\", etc.\n        import re\n\n        # Pattern for distance: number + unit\n        distance_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(meter|meters|metre|metres|foot|feet|inch|inches|cm|centimeter|centimeters)'\n\n        match = re.search(distance_pattern, text)\n        if match:\n            distance = float(match.group(1))\n            unit = match.group(2).lower()\n\n            # Convert to meters\n            if unit in ['foot', 'feet']:\n                return distance * 0.3048\n            elif unit in ['inch', 'inches']:\n                return distance * 0.0254\n            elif unit in ['cm', 'centimeter', 'centimeters']:\n                return distance * 0.01\n            else:  # meters\n                return distance\n\n        return 1.0  # Default distance\n\n    def extract_speed(self, text: str) -> float:\n        \"\"\"Extract speed from text\"\"\"\n        import re\n\n        # Look for speed indicators\n        slow_indicators = ['slow', 'slowly', 'carefully', 'gently']\n        fast_indicators = ['fast', 'quickly', 'hurry', 'hastily']\n\n        if any(indicator in text for indicator in slow_indicators):\n            return 0.3  # Slow speed\n        elif any(indicator in text for indicator in fast_indicators):\n            return 0.8  # Fast speed\n        else:\n            return 0.5  # Medium speed\n\n    def extract_speech_content(self, text: str) -> str:\n        \"\"\"Extract what the robot should say from speech command\"\"\"\n        import re\n\n        # Pattern: say/repeat/tell \"quoted text\" or after the command\n        quote_pattern = r'(?:say|repeat|tell|speak)\\s*[\"\\']([^\"\\']+)[\"\\']'\n        match = re.search(quote_pattern, text)\n\n        if match:\n            return match.group(1)\n        else:\n            # If no quotes, return everything after the command verb\n            for verb in ['say', 'repeat', 'tell', 'speak']:\n                if verb in text:\n                    idx = text.find(verb) + len(verb)\n                    content = text[idx:].strip()\n                    # Remove common filler words\n                    content = content.replace('to me', '').replace('to the robot', '').strip()\n                    return content or \"Hello, I received a speech command\"\n\n        return \"I have something to say\"\n\n# Real-time audio capture node\nclass AudioCaptureNode(Node):\n    def __init__(self):\n        super().__init__('audio_capture_node')\n\n        # Publisher for audio data\n        self.audio_pub = self.create_publisher(AudioData, '/audio/input', 10)\n\n        # Audio capture parameters\n        self.sample_rate = 16000\n        self.chunk_size = 1024\n        self.channels = 1\n\n        # Initialize PyAudio\n        import pyaudio\n        self.pyaudio_instance = pyaudio.PyAudio()\n\n        # Open audio stream\n        self.stream = self.pyaudio_instance.open(\n            format=pyaudio.paInt16,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        # Timer for capturing audio\n        self.capture_timer = self.create_timer(0.1, self.capture_audio)\n\n        self.get_logger().info('Audio Capture Node initialized')\n\n    def capture_audio(self):\n        \"\"\"Capture audio from microphone and publish\"\"\"\n        try:\n            # Read audio data\n            data = self.stream.read(self.chunk_size, exception_on_overflow=False)\n\n            # Create AudioData message\n            audio_msg = AudioData()\n            audio_msg.data = data\n\n            # Create AudioInfo (simplified)\n            from audio_common_msgs.msg import AudioInfo\n            audio_info = AudioInfo()\n            audio_info.sample_rate = self.sample_rate\n            audio_info.channels = self.channels\n            audio_info.format = 2  # 16-bit PCM\n            audio_info.type = 1  # Integer samples\n            audio_info.bits_per_sample = 16\n\n            audio_msg.info = audio_info\n\n            # Publish audio data\n            self.audio_pub.publish(audio_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error capturing audio: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    # Create both nodes\n    voice_node = VoiceToActionNode()\n    audio_node = AudioCaptureNode()\n\n    # Create executor to run both nodes\n    executor = rclpy.executors.MultiThreadedExecutor()\n    executor.add_node(voice_node)\n    executor.add_node(audio_node)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_node.destroy_node()\n        audio_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Latency Considerations\n\nFor humanoid robots, minimizing latency in voice command processing is crucial for natural interaction:\n\n```python\nimport asyncio\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\n\nclass LowLatencyVoiceProcessor:\n    def __init__(self, model_size=\"small\"):\n        self.model_size = model_size\n        self.model = None\n        self.load_model_async()\n\n        # Thread pool for parallel processing\n        self.executor = ThreadPoolExecutor(max_workers=2)\n\n        # Pre-allocated buffers\n        self.audio_buffer = np.zeros(32000, dtype=np.float32)  # 2 seconds at 16kHz\n        self.buffer_idx = 0\n\n        # Real-time constraints\n        self.max_processing_time = 0.2  # 200ms maximum processing time\n        self.warmup_model()\n\n    def load_model_async(self):\n        \"\"\"Load model in background thread\"\"\"\n        def load():\n            self.model = whisper.load_model(self.model_size)\n        threading.Thread(target=load, daemon=True).start()\n\n    def warmup_model(self):\n        \"\"\"Warm up the model with dummy data to reduce first-run latency\"\"\"\n        dummy_audio = np.random.randn(16000).astype(np.float32)  # 1 second\n        try:\n            # Run a quick transcription to warm up the model\n            self.model.transcribe(dummy_audio[:8000], language='en', temperature=0.0)  # Half second\n        except:\n            pass  # Model might not be loaded yet\n\n    def process_audio_chunk(self, audio_chunk: np.ndarray) -> Optional[str]:\n        \"\"\"Process audio chunk with latency constraints\"\"\"\n        start_time = time.time()\n\n        if self.model is None:\n            if time.time() - start_time > self.max_processing_time:\n                return None\n            time.sleep(0.01)  # Brief wait for model loading\n            return None\n\n        try:\n            # Set a timeout for processing\n            future = self.executor.submit(\n                self.model.transcribe,\n                audio_chunk,\n                language='en',\n                temperature=0.0\n            )\n\n            # Wait for result with timeout\n            result = future.result(timeout=self.max_processing_time - (time.time() - start_time))\n\n            if result and 'text' in result:\n                return result['text'].strip()\n\n        except Exception as e:\n            self.get_logger().warn(f'Latency constraint exceeded or error: {e}')\n\n        return None\n\n# Voice activity detection for reduced processing\nclass VoiceActivityDetector:\n    def __init__(self, threshold=0.01, silence_duration=0.5):\n        self.threshold = threshold\n        self.silence_duration = silence_duration\n        self.last_voice_time = time.time()\n\n    def is_voice_active(self, audio_chunk: np.ndarray) -> bool:\n        \"\"\"Detect if voice is active in the audio chunk\"\"\"\n        energy = np.mean(np.abs(audio_chunk))\n        is_active = energy > self.threshold\n\n        if is_active:\n            self.last_voice_time = time.time()\n\n        return is_active\n\n    def should_process(self) -> bool:\n        \"\"\"Determine if audio should be processed based on silence detection\"\"\"\n        return (time.time() - self.last_voice_time) < self.silence_duration\n```\n\n## Command Reliability\n\nEnsuring command reliability in noisy environments:\n\n```python\nimport sounddevice as sd\nimport webrtcvad\nimport collections\nimport wave\nimport contextlib\n\nclass ReliableVoiceCommandProcessor:\n    def __init__(self):\n        # Initialize VAD (Voice Activity Detection)\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(3)  # Aggressive VAD mode\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n\n        # Command validation\n        self.confidence_threshold = 0.8\n        self.repetition_threshold = 2  # Require command repetition for critical actions\n        self.command_history = collections.deque(maxlen=10)\n\n        # Noise suppression\n        self.noise_floor = 0.001\n        self.snr_threshold = 10  # Signal-to-noise ratio threshold\n\n    def preprocess_audio(self, audio_data: np.ndarray) -> np.ndarray:\n        \"\"\"Preprocess audio for better recognition in noisy environments\"\"\"\n        # Apply noise reduction\n        from scipy import signal\n\n        # Estimate noise spectrum (assuming first 0.5s is noise)\n        noise_samples = int(0.5 * self.sample_rate)\n        if len(audio_data) > noise_samples:\n            noise_spectrum = np.abs(np.fft.fft(audio_data[:noise_samples]))\n        else:\n            noise_spectrum = np.ones_like(audio_data) * self.noise_floor\n\n        # Apply spectral subtraction\n        audio_spectrum = np.fft.fft(audio_data)\n        enhanced_spectrum = np.maximum(\n            np.abs(audio_spectrum) - noise_spectrum * 0.5,  # Noise reduction factor\n            0.1 * np.abs(audio_spectrum)  # Preserve some original signal\n        )\n\n        # Reconstruct signal\n        enhanced_audio = np.real(np.fft.ifft(enhanced_spectrum * np.exp(1j * np.angle(audio_spectrum))))\n\n        return enhanced_audio.astype(np.float32)\n\n    def validate_command(self, transcription: str, confidence: float) -> bool:\n        \"\"\"Validate command reliability\"\"\"\n        # Check confidence threshold\n        if confidence < self.confidence_threshold:\n            return False\n\n        # Check for common misrecognitions\n        if self.is_common_misrecognition(transcription):\n            return False\n\n        # Check for command repetition (for critical actions)\n        if self.requires_repetition(transcription):\n            return self.check_command_repetition(transcription)\n\n        return True\n\n    def is_common_misrecognition(self, transcription: str) -> bool:\n        \"\"\"Check if transcription is likely a common misrecognition\"\"\"\n        common_misrecognitions = [\n            '',  # Empty transcription\n            'thank you',  # Often misrecognized\n            'i love you',  # Often misrecognized\n            'what is your name',  # Often misrecognized\n            'hello',  # Often misrecognized\n            'yes',  # Often misrecognized\n            'no',  # Often misrecognized\n        ]\n\n        return transcription.lower().strip() in common_misrecognitions\n\n    def requires_repetition(self, transcription: str) -> bool:\n        \"\"\"Check if command requires repetition (critical actions)\"\"\"\n        critical_actions = [\n            'shutdown', 'power off', 'stop', 'emergency', 'danger',\n            'fire', 'help', 'alert', 'warning', 'stop moving'\n        ]\n\n        return any(action in transcription.lower() for action in critical_actions)\n\n    def check_command_repetition(self, transcription: str) -> bool:\n        \"\"\"Check if command has been repeated recently\"\"\"\n        recent_commands = list(self.command_history)[-3:]  # Check last 3 commands\n        repetition_count = sum(1 for cmd in recent_commands if cmd == transcription)\n\n        return repetition_count >= self.repetition_threshold\n\n    def calculate_confidence(self, transcription: str, audio_features: Dict) -> float:\n        \"\"\"Calculate confidence score for transcription\"\"\"\n        # Multiple factors contribute to confidence:\n\n        # 1. Length factor (very short transcriptions less reliable)\n        length_factor = min(len(transcription.split()), 10) / 10.0\n        if length_factor < 0.2:\n            return 0.0  # Too short to be reliable\n\n        # 2. SNR factor\n        snr = audio_features.get('snr', 0)\n        snr_factor = min(snr / self.snr_threshold, 1.0) if self.snr_threshold > 0 else 1.0\n\n        # 3. Language model coherence (simplified)\n        coherence_factor = self.estimate_language_coherence(transcription)\n\n        # 4. Acoustic confidence (from ASR model)\n        acoustic_confidence = audio_features.get('acoustic_confidence', 0.8)\n\n        # Weighted combination\n        confidence = (\n            0.3 * length_factor +\n            0.2 * snr_factor +\n            0.3 * coherence_factor +\n            0.2 * acoustic_confidence\n        )\n\n        return min(confidence, 1.0)\n\n    def estimate_language_coherence(self, transcription: str) -> float:\n        \"\"\"Estimate how coherent the transcription is\"\"\"\n        import re\n\n        # Check for repeated words or fragments (indicating poor recognition)\n        words = transcription.lower().split()\n        if len(words) < 2:\n            return 0.3  # Very low confidence for single words\n\n        # Count repeated words\n        word_counts = {}\n        for word in words:\n            word_counts[word] = word_counts.get(word, 0) + 1\n\n        repeated_words = sum(1 for count in word_counts.values() if count > 1)\n        repetition_ratio = repeated_words / len(set(words)) if words else 0\n\n        # Lower confidence for high repetition\n        coherence = max(0.1, 1.0 - repetition_ratio * 0.5)\n\n        # Check for common English word patterns\n        common_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']\n        common_word_ratio = sum(1 for word in words if word in common_words) / len(words)\n\n        # Higher confidence if common words are present\n        coherence *= (0.5 + common_word_ratio * 0.5)\n\n        return coherence\n\n# Example launch file for the voice system\n\"\"\"\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Audio capture node\n        Node(\n            package='voice_to_action',\n            executable='audio_capture_node',\n            name='audio_capture',\n            parameters=[\n                {'sample_rate': 16000},\n                {'channels': 1}\n            ]\n        ),\n\n        # Voice-to-action processing node\n        Node(\n            package='voice_to_action',\n            executable='voice_to_action_node',\n            name='voice_processor',\n            parameters=[\n                {'model_size': 'small'},  # Use 'medium' for better accuracy if resources allow\n                {'energy_threshold': 0.01},\n                {'silence_duration': 0.5}\n            ]\n        )\n    ])\n\"\"\"\n```\n\n## Practical Exercise\n\n1. Set up Whisper model for real-time voice recognition\n2. Implement intent parsing for humanoid robot commands\n3. Optimize for low-latency processing\n4. Add noise reduction and reliability checks\n\n## Summary\n\nVoice-to-action pipelines enable natural human-robot interaction for humanoid robots. OpenAI Whisper provides robust speech recognition, while intent processing converts speech to meaningful robot actions. Optimizing for low latency and ensuring command reliability in noisy environments are crucial for effective voice control. The system should include voice activity detection, noise reduction, and validation mechanisms to provide reliable and responsive voice control for humanoid robots.",
    "module": "Module 4: Vision-Language-Action (VLA)",
    "chapter": "voice to action pipelines.md",
    "path": "/docs/module-4/4.1-voice-to-action-pipelines"
  },
  {
    "id": "module-4/4.2-language-grounding-robotics",
    "title": "Chapter 4.2 – Language Grounding in Robotics",
    "content": "\n# Chapter 4.2 – Language Grounding in Robotics\n\n## Learning Objectives\n- Implement symbol grounding for natural language understanding\n- Create world models for humanoid robots\n- Develop task abstraction mechanisms for complex commands\n- Define constraints and affordances for language-based robot control\n\n## Symbol Grounding\n\nSymbol grounding is the challenge of connecting linguistic symbols to real-world entities and actions. For humanoid robots, this involves mapping language to perceptions, actions, and environmental states:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Point\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom visualization_msgs.msg import MarkerArray\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional, Any\nimport spacy\nimport openai\nfrom transformers import pipeline\nfrom dataclasses import dataclass\n\n@dataclass\nclass GroundedObject:\n    \"\"\"Represents a grounded object with spatial and semantic information\"\"\"\n    name: str\n    semantic_label: str\n    position: Point\n    orientation: Tuple[float, float, float, float]  # quaternion\n    confidence: float\n    bounding_box: Optional[Tuple[float, float, float, float]] = None  # x, y, w, h\n    affordances: List[str] = None\n    attributes: Dict[str, Any] = None\n\n@dataclass\nclass GroundedAction:\n    \"\"\"Represents a grounded action with parameters\"\"\"\n    action_type: str  # move, grasp, manipulate, etc.\n    target_object: Optional[str] = None\n    target_location: Optional[Point] = None\n    parameters: Dict[str, Any] = None\n    constraints: List[str] = None\n\nclass SymbolGroundingNode(Node):\n    def __init__(self):\n        super().__init__('symbol_grounding_node')\n\n        # Initialize NLP components\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            self.get_logger().warn(\"spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n            self.nlp = None\n\n        # Object grounding\n        self.object_detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n        self.scene_graph = SceneGraph()\n        self.semantic_map = SemanticMap()\n\n        # Spatial grounding\n        self.spatial_reasoner = SpatialReasoner()\n\n        # Subscriptions\n        self.language_sub = self.create_subscription(\n            String, '/natural_language_command', self.language_callback, 10\n        )\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.image_callback, 10\n        )\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2, '/camera/depth/points', self.pointcloud_callback, 10\n        )\n\n        # Publishers\n        self.grounded_command_pub = self.create_publisher(\n            String, '/grounded_robot_command', 10\n        )\n        self.visualization_pub = self.create_publisher(\n            MarkerArray, '/grounding_visualization', 10\n        )\n\n        # Object and scene databases\n        self.objects_in_scene: Dict[str, GroundedObject] = {}\n        self.language_command_buffer = []\n\n        self.get_logger().info('Symbol Grounding Node initialized')\n\n    def language_callback(self, msg: String):\n        \"\"\"Process natural language command\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received language command: \"{command}\"')\n\n        try:\n            # Parse the command using NLP\n            parsed_command = self.parse_command(command)\n\n            # Ground symbols in the command to objects in the scene\n            grounded_command = self.ground_command(parsed_command)\n\n            # Publish grounded command\n            grounded_msg = String()\n            grounded_msg.data = str(grounded_command)\n            self.grounded_command_pub.publish(grounded_msg)\n\n            self.get_logger().info(f'Grounded command: {grounded_command}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing language command: {e}')\n\n    def image_callback(self, msg: Image):\n        \"\"\"Process image to detect and ground objects\"\"\"\n        try:\n            # Convert ROS image to format suitable for object detection\n            image_np = self.ros_image_to_numpy(msg)\n\n            # Detect objects in the image\n            detections = self.object_detector(image_np)\n\n            # Create grounded objects from detections\n            for detection in detections:\n                label = detection['label']\n                score = detection['score']\n                bbox = detection['box']\n\n                if score > 0.5:  # Confidence threshold\n                    # Convert 2D bounding box to 3D position using depth information\n                    position_3d = self.get_3d_position_from_bbox(bbox)\n\n                    grounded_obj = GroundedObject(\n                        name=self.generate_object_name(label),\n                        semantic_label=label,\n                        position=position_3d,\n                        orientation=(0.0, 0.0, 0.0, 1.0),  # Identity quaternion\n                        confidence=score,\n                        bounding_box=(bbox['xmin'], bbox['ymin'],\n                                   bbox['xmax'] - bbox['xmin'],\n                                   bbox['ymax'] - bbox['ymin']),\n                        affordances=self.get_affordances_for_object(label),\n                        attributes={'color': self.get_object_color(image_np, bbox)}\n                    )\n\n                    # Add to scene graph\n                    self.scene_graph.add_object(grounded_obj)\n\n                    # Store in local database\n                    self.objects_in_scene[grounded_obj.name] = grounded_obj\n\n                    self.get_logger().info(f'Detected and grounded object: {grounded_obj.name}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def pointcloud_callback(self, msg: PointCloud2):\n        \"\"\"Process point cloud for 3D object grounding\"\"\"\n        try:\n            # Convert PointCloud2 to numpy array\n            pointcloud_np = self.pointcloud_to_numpy(msg)\n\n            # Perform 3D object detection and clustering\n            clusters = self.cluster_pointcloud(pointcloud_np)\n\n            # Update object positions and add new objects\n            for cluster in clusters:\n                obj_position = self.calculate_cluster_centroid(cluster)\n                # Update existing objects or create new ones based on proximity\n                self.update_object_positions(obj_position)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing point cloud: {e}')\n\n    def parse_command(self, command: str) -> Dict[str, Any]:\n        \"\"\"Parse natural language command using NLP\"\"\"\n        if not self.nlp:\n            return {'raw_command': command, 'entities': [], 'actions': []}\n\n        doc = self.nlp(command)\n\n        entities = []\n        actions = []\n\n        # Extract named entities\n        for ent in doc.ents:\n            entities.append({\n                'text': ent.text,\n                'label': ent.label_,\n                'start': ent.start_char,\n                'end': ent.end_char\n            })\n\n        # Extract verbs (potential actions)\n        for token in doc:\n            if token.pos_ == 'VERB':\n                actions.append({\n                    'text': token.text,\n                    'lemma': token.lemma_,\n                    'pos': token.pos_\n                })\n\n        # Extract dependencies and relationships\n        relationships = []\n        for token in doc:\n            if token.dep_ in ['nsubj', 'dobj', 'prep', 'pobj']:\n                relationships.append({\n                    'token': token.text,\n                    'dependency': token.dep_,\n                    'head': token.head.text\n                })\n\n        return {\n            'raw_command': command,\n            'entities': entities,\n            'actions': actions,\n            'relationships': relationships,\n            'parsed_doc': doc\n        }\n\n    def ground_command(self, parsed_command: Dict[str, Any]) -> GroundedAction:\n        \"\"\"Ground symbols in command to objects and locations in the scene\"\"\"\n        command = parsed_command['raw_command']\n\n        # Extract action\n        action = self.extract_action(parsed_command)\n\n        # Ground target object\n        target_obj_name = self.extract_target_object(parsed_command)\n        target_object = self.find_closest_object(target_obj_name) if target_obj_name else None\n\n        # Ground target location\n        target_location = self.extract_target_location(parsed_command)\n\n        # Extract action parameters\n        parameters = self.extract_action_parameters(parsed_command)\n\n        # Determine constraints\n        constraints = self.extract_constraints(parsed_command)\n\n        return GroundedAction(\n            action_type=action,\n            target_object=target_object.name if target_object else None,\n            target_location=target_location,\n            parameters=parameters,\n            constraints=constraints\n        )\n\n    def extract_action(self, parsed_command: Dict[str, Any]) -> str:\n        \"\"\"Extract the main action from the command\"\"\"\n        # Map common action words to robot actions\n        action_mappings = {\n            'move': ['go', 'move', 'walk', 'travel', 'approach'],\n            'grasp': ['grasp', 'grab', 'take', 'pick up', 'hold'],\n            'release': ['release', 'drop', 'put down', 'place'],\n            'manipulate': ['manipulate', 'move', 'push', 'pull', 'turn'],\n            'look_at': ['look at', 'see', 'observe', 'watch'],\n            'point_at': ['point at', 'point to', 'indicate'],\n            'navigate': ['navigate', 'go to', 'reach', 'move to']\n        }\n\n        command_lower = parsed_command['raw_command'].lower()\n\n        for action, keywords in action_mappings.items():\n            for keyword in keywords:\n                if keyword in command_lower:\n                    return action\n\n        # Default action if none found\n        return 'navigate'\n\n    def extract_target_object(self, parsed_command: Dict[str, Any]) -> Optional[str]:\n        \"\"\"Extract target object from command\"\"\"\n        # Look for entities that are objects\n        for entity in parsed_command['entities']:\n            if entity['label'] in ['OBJECT', 'PRODUCT', 'ENTITY']:  # Custom labels\n                # Try to match with detected objects\n                for obj_name, obj in self.objects_in_scene.items():\n                    if entity['text'].lower() in obj_name.lower() or \\\n                       entity['text'].lower() in obj.semantic_label.lower():\n                        return obj_name\n\n        # If no direct match, use closest object based on semantic similarity\n        for entity in parsed_command['entities']:\n            for obj_name, obj in self.objects_in_scene.items():\n                if self.semantic_similarity(entity['text'], obj.semantic_label) > 0.7:\n                    return obj_name\n\n        return None\n\n    def extract_target_location(self, parsed_command: Dict[str, Any]) -> Optional[Point]:\n        \"\"\"Extract target location from command\"\"\"\n        command_lower = parsed_command['raw_command'].lower()\n\n        # Look for location-related keywords\n        location_keywords = {\n            'near': ['near', 'close to', 'beside', 'next to'],\n            'on': ['on', 'on top of', 'above'],\n            'under': ['under', 'below', 'beneath'],\n            'behind': ['behind', 'back of'],\n            'in_front_of': ['in front of', 'before', 'ahead of'],\n            'left': ['left', 'to the left', 'left side'],\n            'right': ['right', 'to the right', 'right side'],\n            'relative': ['relative to', 'compared to']\n        }\n\n        for relation, keywords in location_keywords.items():\n            for keyword in keywords:\n                if keyword in command_lower:\n                    # Find reference object\n                    reference_obj = self.extract_reference_object(parsed_command)\n                    if reference_obj:\n                        # Calculate relative position\n                        return self.spatial_reasoner.calculate_relative_position(\n                            reference_obj.position, relation\n                        )\n\n        return None\n\n    def semantic_similarity(self, text1: str, text2: str) -> float:\n        \"\"\"Calculate semantic similarity between two texts\"\"\"\n        # In practice, this would use embeddings or more sophisticated methods\n        # For now, simple word overlap\n        words1 = set(text1.lower().split())\n        words2 = set(text2.lower().split())\n\n        intersection = words1.intersection(words2)\n        union = words1.union(words2)\n\n        if not union:\n            return 0.0\n\n        return len(intersection) / len(union)\n\n    def find_closest_object(self, target_name: str) -> Optional[GroundedObject]:\n        \"\"\"Find the closest object matching the target name\"\"\"\n        best_match = None\n        best_similarity = 0.0\n\n        for obj_name, obj in self.objects_in_scene.items():\n            similarity = self.semantic_similarity(target_name, obj_name)\n            if similarity > best_similarity:\n                best_similarity = similarity\n                best_match = obj\n\n        return best_match\n\n    def get_affordances_for_object(self, object_type: str) -> List[str]:\n        \"\"\"Get possible affordances for an object type\"\"\"\n        affordance_map = {\n            'chair': ['sit_on', 'move', 'push'],\n            'table': ['place_on', 'touch', 'move_around'],\n            'cup': ['grasp', 'lift', 'drink_from'],\n            'bottle': ['grasp', 'lift', 'pour_from'],\n            'door': ['open', 'close', 'push', 'pull'],\n            'box': ['move', 'grasp', 'stack'],\n            'ball': ['grasp', 'throw', 'kick'],\n            'person': ['greet', 'follow', 'avoid'],\n            'robot': ['interact', 'follow', 'avoid'],\n            'wall': ['avoid', 'follow_along'],\n            'floor': ['walk_on', 'stand_on'],\n            'ceiling': ['look_up_at']\n        }\n\n        return affordance_map.get(object_type.lower(), ['approach', 'observe'])\n\n    def generate_object_name(self, semantic_label: str) -> str:\n        \"\"\"Generate a unique name for an object\"\"\"\n        counter = 1\n        base_name = semantic_label.lower().replace(' ', '_')\n\n        while f\"{base_name}_{counter}\" in self.objects_in_scene:\n            counter += 1\n\n        return f\"{base_name}_{counter}\"\n\n    def get_object_color(self, image: np.ndarray, bbox: Dict) -> str:\n        \"\"\"Extract dominant color from object bounding box\"\"\"\n        xmin, ymin = int(bbox['xmin']), int(bbox['ymin'])\n        xmax, ymax = int(bbox['xmax']), int(bbox['ymax'])\n\n        # Extract region of interest\n        roi = image[ymin:ymax, xmin:xmax]\n\n        # Calculate dominant color (simplified)\n        if roi.size > 0:\n            avg_color = np.mean(roi, axis=(0, 1))\n            return self.rgb_to_color_name(avg_color)\n        else:\n            return \"unknown\"\n\n    def rgb_to_color_name(self, rgb: np.ndarray) -> str:\n        \"\"\"Convert RGB values to color name\"\"\"\n        r, g, b = rgb.astype(int)\n\n        # Simplified color classification\n        if r > g and r > b:\n            return \"red\" if r > 100 else \"dark\"\n        elif g > r and g > b:\n            return \"green\" if g > 100 else \"dark\"\n        elif b > r and b > g:\n            return \"blue\" if b > 100 else \"dark\"\n        elif r == g == b:\n            return \"gray\" if r > 100 else \"black\"\n        else:\n            return \"multicolor\"\n\n    def get_3d_position_from_bbox(self, bbox: Dict) -> Point:\n        \"\"\"Estimate 3D position from 2D bounding box using depth information\"\"\"\n        # This is a simplified approach\n        # In practice, you'd use depth from aligned depth image\n        center_x = (bbox['xmin'] + bbox['xmax']) / 2\n        center_y = (bbox['ymin'] + bbox['ymax']) / 2\n\n        # Placeholder - in real implementation, you'd get depth at this pixel\n        depth_estimate = 1.0  # meters\n\n        # Convert pixel coordinates to 3D position\n        # This assumes known camera parameters\n        fx, fy = 554.0, 554.0  # Camera intrinsic parameters (placeholder)\n        cx, cy = 320.0, 240.0  # Camera center (placeholder)\n\n        x = (center_x - cx) * depth_estimate / fx\n        y = (center_y - cy) * depth_estimate / fy\n        z = depth_estimate\n\n        position = Point()\n        position.x = x\n        position.y = y\n        position.z = z\n\n        return position\n\nclass SpatialReasoner:\n    \"\"\"Handles spatial relationships and reasoning\"\"\"\n    def __init__(self):\n        self.reference_frame = 'map'  # or 'base_link', 'camera', etc.\n\n    def calculate_relative_position(self, reference_pos: Point,\n                                  relation: str) -> Point:\n        \"\"\"Calculate position relative to a reference object\"\"\"\n        offset = Point()\n\n        if relation == 'near':\n            offset.x = 0.5  # 0.5m in front\n            offset.y = 0.0\n            offset.z = 0.0\n        elif relation == 'on':\n            offset.x = 0.0\n            offset.y = 0.0\n            offset.z = 0.5  # 0.5m above\n        elif relation == 'under':\n            offset.x = 0.0\n            offset.y = 0.0\n            offset.z = -0.5  # 0.5m below\n        elif relation == 'behind':\n            offset.x = -0.5  # 0.5m behind\n            offset.y = 0.0\n            offset.z = 0.0\n        elif relation == 'in_front_of':\n            offset.x = 0.5  # 0.5m in front\n            offset.y = 0.0\n            offset.z = 0.0\n        elif relation == 'left':\n            offset.x = 0.0\n            offset.y = 0.5  # 0.5m to the left\n            offset.z = 0.0\n        elif relation == 'right':\n            offset.x = 0.0\n            offset.y = -0.5  # 0.5m to the right\n            offset.z = 0.0\n\n        # Calculate absolute position\n        result = Point()\n        result.x = reference_pos.x + offset.x\n        result.y = reference_pos.y + offset.y\n        result.z = reference_pos.z + offset.z\n\n        return result\n\n    def is_within_workspace(self, position: Point, robot_base: Point) -> bool:\n        \"\"\"Check if position is within robot's workspace\"\"\"\n        # Calculate distance from robot base\n        distance = np.sqrt(\n            (position.x - robot_base.x)**2 +\n            (position.y - robot_base.y)**2 +\n            (position.z - robot_base.z)**2\n        )\n\n        # Assume humanoid workspace is roughly 1m radius\n        return distance <= 1.0\n\nclass SceneGraph:\n    \"\"\"Maintains relationships between objects in the scene\"\"\"\n    def __init__(self):\n        self.nodes = {}  # Object name -> GroundedObject\n        self.edges = {}  # Relationships between objects\n\n    def add_object(self, obj: GroundedObject):\n        \"\"\"Add an object to the scene graph\"\"\"\n        self.nodes[obj.name] = obj\n\n    def add_relationship(self, obj1_name: str, relationship: str, obj2_name: str):\n        \"\"\"Add a relationship between two objects\"\"\"\n        if obj1_name not in self.edges:\n            self.edges[obj1_name] = {}\n        self.edges[obj1_name][relationship] = obj2_name\n\n    def find_objects_by_property(self, property_name: str, property_value: Any) -> List[str]:\n        \"\"\"Find objects with a specific property\"\"\"\n        matches = []\n        for name, obj in self.nodes.items():\n            if hasattr(obj, property_name) and getattr(obj, property_name) == property_value:\n                matches.append(name)\n        return matches\n\n    def get_spatial_relationship(self, obj1_name: str, obj2_name: str) -> str:\n        \"\"\"Get the spatial relationship between two objects\"\"\"\n        # Calculate geometric relationship\n        if obj1_name in self.nodes and obj2_name in self.nodes:\n            pos1 = self.nodes[obj1_name].position\n            pos2 = self.nodes[obj2_name].position\n\n            # Calculate relative position\n            dx = pos2.x - pos1.x\n            dy = pos2.y - pos1.y\n            dz = pos2.z - pos1.z\n\n            # Determine spatial relationship\n            if abs(dx) > abs(dy) and abs(dx) > abs(dz):\n                if dx > 0:\n                    return 'right_of'\n                else:\n                    return 'left_of'\n            elif abs(dy) > abs(dx) and abs(dy) > abs(dz):\n                if dy > 0:\n                    return 'above'\n                else:\n                    return 'below'\n            else:\n                if dz > 0:\n                    return 'in_front_of'\n                else:\n                    return 'behind'\n\n        return 'unknown'\n\nclass SemanticMap:\n    \"\"\"Maintains semantic information about the environment\"\"\"\n    def __init__(self):\n        self.regions = {}  # Area name -> objects and properties\n        self.semantic_annotations = {}  # Object -> semantic properties\n\n    def annotate_object(self, object_name: str, annotations: Dict[str, Any]):\n        \"\"\"Add semantic annotations to an object\"\"\"\n        self.semantic_annotations[object_name] = annotations\n\n    def get_region_for_position(self, position: Point) -> Optional[str]:\n        \"\"\"Get the semantic region for a given position\"\"\"\n        # In practice, this would use a semantic map\n        # For now, return a generic region\n        return \"unknown_region\"\n\n# Example usage and integration\nclass LanguageGroundingDemo:\n    \"\"\"Demonstration of language grounding capabilities\"\"\"\n    def __init__(self):\n        self.symbol_grounding = SymbolGroundingNode()\n\n    def demonstrate_grounding(self):\n        \"\"\"Demonstrate various grounding scenarios\"\"\"\n        test_commands = [\n            \"Go to the red chair\",\n            \"Grasp the blue cup on the table\",\n            \"Move the box to the left of the door\",\n            \"Look at the person near the window\",\n            \"Navigate to the kitchen\"\n        ]\n\n        for command in test_commands:\n            print(f\"\\nProcessing command: '{command}'\")\n\n            # Parse command\n            parsed = self.symbol_grounding.parse_command(command)\n            print(f\"Parsed: {parsed['actions']}, {parsed['entities']}\")\n\n            # Ground command (simulated)\n            # In practice, this would require actual objects in the scene\n            print(f\"Grounded command would target specific objects and locations\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SymbolGroundingNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## World Models for Humanoid Robots\n\nHumanoid robots require sophisticated world models that account for their unique capabilities and constraints:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Optional\nimport numpy as np\nfrom geometry_msgs.msg import Point, Pose\nfrom std_msgs.msg import Header\n\n@dataclass\nclass HumanoidState:\n    \"\"\"Represents the state of a humanoid robot\"\"\"\n    base_pose: Pose\n    joint_positions: Dict[str, float]\n    joint_velocities: Dict[str, float]\n    center_of_mass: Point\n    zero_moment_point: Point\n    support_polygon: List[Point]  # Points of contact with ground\n    balance_state: str  # stable, unstable, recovering\n    energy_level: float  # Battery or power level\n\n@dataclass\nclass HumanoidCapabilities:\n    \"\"\"Defines what a humanoid robot can do\"\"\"\n    max_linear_velocity: float = 0.5  # m/s\n    max_angular_velocity: float = 0.5  # rad/s\n    step_height: float = 0.1  # Maximum step height\n    step_width: float = 0.3  # Maximum step width\n    reach_distance: float = 0.8  # Max reach from base\n    lifting_capacity: float = 5.0  # Max weight (kg)\n    balance_recovery_time: float = 2.0  # Time to recover balance (s)\n    walking_types: List[str] = None  # Walk types: 'flat', 'stairs', 'uneven'\n\n@dataclass\nclass WorldState:\n    \"\"\"Represents the current state of the world\"\"\"\n    timestamp: float\n    robot_state: HumanoidState\n    objects: Dict[str, GroundedObject]\n    static_environment: Dict[str, Any]  # Maps, layouts, etc.\n    dynamic_entities: List[Any]  # Moving objects, people, etc.\n    affordances: Dict[str, List[str]]  # What can be done with objects\n\nclass WorldModel:\n    \"\"\"Maintains and updates the robot's world model\"\"\"\n    def __init__(self, robot_capabilities: HumanoidCapabilities):\n        self.capabilities = robot_capabilities\n        self.current_state = None\n        self.belief_state = {}  # Probabilistic beliefs about world\n        self.action_history = []\n        self.goal_stack = []\n\n    def update_from_perception(self, sensor_data: Dict[str, Any]):\n        \"\"\"Update world model based on sensor data\"\"\"\n        # Update objects based on new detections\n        if 'objects' in sensor_data:\n            for obj_data in sensor_data['objects']:\n                self.update_object_beliefs(obj_data)\n\n        # Update robot state based on odometry and IMU\n        if 'robot_state' in sensor_data:\n            self.update_robot_state(sensor_data['robot_state'])\n\n        # Update spatial relationships\n        self.update_spatial_relations()\n\n    def update_object_beliefs(self, obj_data: Dict[str, Any]):\n        \"\"\"Update beliefs about a specific object\"\"\"\n        obj_name = obj_data.get('name', 'unknown')\n\n        # Update confidence in object existence\n        current_confidence = self.belief_state.get(f'exists_{obj_name}', 0.0)\n        detection_confidence = obj_data.get('confidence', 0.8)\n\n        # Apply Bayes rule to update belief\n        new_confidence = self.update_probability(\n            current_confidence,\n            detection_confidence,\n            positive=True\n        )\n\n        self.belief_state[f'exists_{obj_name}'] = new_confidence\n\n        # Update object properties\n        if new_confidence > 0.5:  # Only update if object likely exists\n            if 'position' in obj_data:\n                self.belief_state[f'position_{obj_name}'] = obj_data['position']\n            if 'affordances' in obj_data:\n                self.belief_state[f'affordances_{obj_name}'] = obj_data['affordances']\n\n    def update_robot_state(self, robot_data: Dict[str, Any]):\n        \"\"\"Update the robot's internal state\"\"\"\n        if self.current_state is None:\n            self.current_state = HumanoidState(\n                base_pose=robot_data.get('pose', Pose()),\n                joint_positions=robot_data.get('joint_positions', {}),\n                joint_velocities=robot_data.get('joint_velocities', {}),\n                center_of_mass=robot_data.get('com', Point()),\n                zero_moment_point=robot_data.get('zmp', Point()),\n                support_polygon=robot_data.get('support_polygon', []),\n                balance_state=robot_data.get('balance_state', 'stable'),\n                energy_level=robot_data.get('energy', 1.0)\n            )\n        else:\n            # Update existing state\n            self.current_state.base_pose = robot_data.get('pose', self.current_state.base_pose)\n            self.current_state.joint_positions.update(robot_data.get('joint_positions', {}))\n            self.current_state.joint_velocities.update(robot_data.get('joint_velocities', {}))\n            self.current_state.center_of_mass = robot_data.get('com', self.current_state.center_of_mass)\n            self.current_state.zero_moment_point = robot_data.get('zmp', self.current_state.zero_moment_point)\n            self.current_state.balance_state = robot_data.get('balance_state', self.current_state.balance_state)\n            self.current_state.energy_level = robot_data.get('energy', self.current_state.energy_level)\n\n    def can_execute_action(self, action: GroundedAction) -> Tuple[bool, List[str]]:\n        \"\"\"Check if robot can execute a given action\"\"\"\n        reasons = []\n\n        # Check if target object exists with sufficient confidence\n        if action.target_object:\n            obj_confidence = self.belief_state.get(f'exists_{action.target_object}', 0.0)\n            if obj_confidence < 0.7:\n                reasons.append(f\"Target object '{action.target_object}' not confidently detected\")\n                return False, reasons\n\n            # Check if object is reachable\n            obj_position = self.belief_state.get(f'position_{action.target_object}')\n            if obj_position and self.current_state:\n                distance = self.calculate_distance(\n                    self.current_state.base_pose.position,\n                    obj_position\n                )\n\n                if distance > self.capabilities.reach_distance:\n                    reasons.append(f\"Target object out of reach ({distance:.2f}m > {self.capabilities.reach_distance}m)\")\n                    return False, reasons\n\n        # Check action-specific constraints\n        if action.action_type == 'grasp':\n            # Check lifting capacity\n            obj_weight = self.belief_state.get(f'weight_{action.target_object}', 1.0)  # Assume 1kg if unknown\n            if obj_weight > self.capabilities.lifting_capacity:\n                reasons.append(f\"Object too heavy ({obj_weight}kg > {self.capabilities.lifting_capacity}kg)\")\n                return False, reasons\n\n        elif action.action_type == 'navigate':\n            # Check if target location is feasible\n            if action.target_location:\n                # Check if location is traversable for humanoid\n                if not self.is_traversable(action.target_location):\n                    reasons.append(\"Target location not traversable for humanoid\")\n                    return False, reasons\n\n        elif action.action_type == 'balance':\n            # Check if robot is currently stable enough to perform balance action\n            if self.current_state and self.current_state.balance_state != 'stable':\n                reasons.append(\"Robot not in stable state for balance action\")\n                return False, reasons\n\n        return True, reasons\n\n    def calculate_distance(self, pos1: Point, pos2: Point) -> float:\n        \"\"\"Calculate Euclidean distance between two points\"\"\"\n        return np.sqrt(\n            (pos2.x - pos1.x)**2 +\n            (pos2.y - pos1.y)**2 +\n            (pos2.z - pos1.z)**2\n        )\n\n    def is_traversable(self, location: Point) -> bool:\n        \"\"\"Check if location is traversable for humanoid\"\"\"\n        # In practice, this would check a traversability map\n        # For now, assume flat ground is traversable\n        # Check if location is not too steep\n        # Check for obstacles\n        return True  # Simplified for example\n\n    def predict_action_outcome(self, action: GroundedAction) -> WorldState:\n        \"\"\"Predict the outcome of executing an action\"\"\"\n        # Create a copy of current world state\n        predicted_state = WorldState(\n            timestamp=self.current_state.timestamp if self.current_state else 0.0,\n            robot_state=self.current_state,\n            objects=self.belief_state.copy(),\n            static_environment={},\n            dynamic_entities=[],\n            affordances={}\n        )\n\n        # Predict changes based on action\n        if action.action_type == 'navigate':\n            if action.target_location:\n                # Update robot position prediction\n                if predicted_state.robot_state:\n                    predicted_state.robot_state.base_pose.position = action.target_location\n\n        elif action.action_type == 'grasp':\n            if action.target_object:\n                # Update object state to indicate it's grasped\n                predicted_state.belief_state[f'grasped_by_robot_{action.target_object}'] = True\n\n        return predicted_state\n\n    def update_probability(self, prior: float, likelihood: float, positive: bool = True) -> float:\n        \"\"\"Update probability using Bayesian inference\"\"\"\n        if positive:\n            # P(H|D) = P(D|H) * P(H) / P(D)\n            # Where P(D) = P(D|H) * P(H) + P(D|¬H) * P(¬H)\n            p_d_given_h = likelihood\n            p_d_given_not_h = 1 - likelihood  # False positive rate\n\n            numerator = p_d_given_h * prior\n            denominator = (p_d_given_h * prior) + (p_d_given_not_h * (1 - prior))\n\n            if denominator == 0:\n                return 0.0\n\n            return min(numerator / denominator, 1.0)\n        else:\n            # Negative evidence\n            return prior * (1 - likelihood)\n```\n\n## Task Abstraction\n\nComplex commands need to be decomposed into simpler actions:\n\n```python\nfrom enum import Enum\nfrom typing import Callable, Any\n\nclass TaskStatus(Enum):\n    PENDING = \"pending\"\n    EXECUTING = \"executing\"\n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\n@dataclass\nclass Task:\n    \"\"\"Represents an abstract task\"\"\"\n    name: str\n    description: str\n    prerequisites: List[str]  # Other tasks that must be completed first\n    subtasks: List['Task']  # Decomposed subtasks\n    action: Optional[Callable] = None  # Direct action if primitive\n    parameters: Dict[str, Any] = None\n    status: TaskStatus = TaskStatus.PENDING\n    dependencies: List[str] = None\n\nclass TaskDecomposer:\n    \"\"\"Decomposes high-level tasks into executable subtasks\"\"\"\n    def __init__(self, world_model: WorldModel):\n        self.world_model = world_model\n        self.task_library = self.build_task_library()\n\n    def build_task_library(self) -> Dict[str, Callable]:\n        \"\"\"Build a library of known task decompositions\"\"\"\n        return {\n            'fetch_object': self.decompose_fetch_object,\n            'navigate_to_location': self.decompose_navigate_to_location,\n            'manipulate_object': self.decompose_manipulate_object,\n            'perform_household_task': self.decompose_household_task,\n            'assist_human': self.decompose_assist_human,\n        }\n\n    def decompose_command(self, command: GroundedAction) -> Task:\n        \"\"\"Decompose a grounded command into a task hierarchy\"\"\"\n        # Determine the appropriate decomposition based on action type\n        if command.action_type in self.task_library:\n            return self.task_library[command.action_type](command)\n        else:\n            # Default decomposition for unknown actions\n            return self.decompose_generic_action(command)\n\n    def decompose_fetch_object(self, command: GroundedAction) -> Task:\n        \"\"\"Decompose 'fetch object' command\"\"\"\n        if not command.target_object:\n            raise ValueError(\"Fetch command requires target object\")\n\n        # Check if object is known\n        obj_confidence = self.world_model.belief_state.get(f'exists_{command.target_object}', 0.0)\n        if obj_confidence < 0.5:\n            # Need to search for object first\n            return Task(\n                name=f\"search_and_fetch_{command.target_object}\",\n                description=f\"Search for {command.target_object} and then fetch it\",\n                prerequisites=[],\n                subtasks=[\n                    self.create_search_task(command.target_object),\n                    self.create_navigate_task(command.target_object),\n                    self.create_grasp_task(command.target_object),\n                    self.create_return_task()\n                ],\n                parameters={'target_object': command.target_object}\n            )\n        else:\n            # Object is known, proceed directly\n            return Task(\n                name=f\"fetch_{command.target_object}\",\n                description=f\"Fetch {command.target_object}\",\n                prerequisites=[],\n                subtasks=[\n                    self.create_navigate_task(command.target_object),\n                    self.create_grasp_task(command.target_object),\n                    self.create_return_task()\n                ],\n                parameters={'target_object': command.target_object}\n            )\n\n    def decompose_navigate_to_location(self, command: GroundedAction) -> Task:\n        \"\"\"Decompose navigation command\"\"\"\n        if not command.target_location:\n            raise ValueError(\"Navigation command requires target location\")\n\n        return Task(\n            name=f\"navigate_to_{hash(str(command.target_location))}\",\n            description=f\"Navigate to location {command.target_location}\",\n            prerequisites=[],\n            subtasks=[\n                self.create_path_planning_task(command.target_location),\n                self.create_motion_execution_task(command.target_location)\n            ],\n            parameters={'target_location': command.target_location}\n        )\n\n    def create_search_task(self, target_object: str) -> Task:\n        \"\"\"Create a task to search for an object\"\"\"\n        return Task(\n            name=f\"search_for_{target_object}\",\n            description=f\"Search for {target_object}\",\n            prerequisites=[],\n            subtasks=[\n                Task(\n                    name=f\"scan_area_for_{target_object}\",\n                    description=\"Scan surrounding area for the object\",\n                    prerequisites=[],\n                    subtasks=[],\n                    action=self.execute_scan_action,\n                    parameters={'target_object': target_object}\n                )\n            ],\n            action=self.execute_search_action,\n            parameters={'target_object': target_object}\n        )\n\n    def create_navigate_task(self, target_object: str) -> Task:\n        \"\"\"Create a navigation task to reach an object\"\"\"\n        return Task(\n            name=f\"navigate_to_{target_object}\",\n            description=f\"Navigate to {target_object}\",\n            prerequisites=[f\"locate_{target_object}\"],  # Need to know where it is\n            subtasks=[\n                Task(\n                    name=f\"plan_path_to_{target_object}\",\n                    description=\"Plan path to object\",\n                    prerequisites=[],\n                    subtasks=[],\n                    action=self.execute_path_planning,\n                    parameters={'target_object': target_object}\n                ),\n                Task(\n                    name=f\"execute_navigation_to_{target_object}\",\n                    description=\"Execute planned navigation\",\n                    prerequisites=[f\"plan_path_to_{target_object}\"],\n                    subtasks=[],\n                    action=self.execute_navigation,\n                    parameters={'target_object': target_object}\n                )\n            ],\n            parameters={'target_object': target_object}\n        )\n\n    def create_grasp_task(self, target_object: str) -> Task:\n        \"\"\"Create a grasping task\"\"\"\n        return Task(\n            name=f\"grasp_{target_object}\",\n            description=f\"Grasp {target_object}\",\n            prerequisites=[f\"reach_{target_object}\"],  # Need to be close first\n            subtasks=[\n                Task(\n                    name=f\"approach_{target_object}\",\n                    description=\"Approach the object carefully\",\n                    prerequisites=[],\n                    subtasks=[],\n                    action=self.execute_approach,\n                    parameters={'target_object': target_object}\n                ),\n                Task(\n                    name=f\"align_gripper_{target_object}\",\n                    description=\"Align gripper with object\",\n                    prerequisites=[f\"approach_{target_object}\"],\n                    subtasks=[],\n                    action=self.execute_alignment,\n                    parameters={'target_object': target_object}\n                ),\n                Task(\n                    name=f\"execute_grasp_{target_object}\",\n                    description=\"Execute grasp motion\",\n                    prerequisites=[f\"align_gripper_{target_object}\"],\n                    subtasks=[],\n                    action=self.execute_grasp,\n                    parameters={'target_object': target_object}\n                )\n            ],\n            parameters={'target_object': target_object}\n        )\n\n    def create_path_planning_task(self, target_location: Point) -> Task:\n        \"\"\"Create a path planning task\"\"\"\n        return Task(\n            name=f\"plan_path_to_{hash(str(target_location))}\",\n            description=\"Plan path to target location\",\n            prerequisites=[],\n            subtasks=[],\n            action=self.execute_path_planning,\n            parameters={'target_location': target_location}\n        )\n\n    def create_motion_execution_task(self, target_location: Point) -> Task:\n        \"\"\"Create a motion execution task\"\"\"\n        return Task(\n            name=f\"move_to_{hash(str(target_location))}\",\n            description=\"Execute motion to target location\",\n            prerequisites=[f\"plan_path_to_{hash(str(target_location))}\"],\n            subtasks=[],\n            action=self.execute_navigation,\n            parameters={'target_location': target_location}\n        )\n\n    def execute_search_action(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute search action\"\"\"\n        target_object = params['target_object']\n        self.world_model.symbol_grounding.get_logger().info(f\"Searching for {target_object}\")\n\n        # In practice, this would involve:\n        # - Turning head/body to scan area\n        # - Using vision to detect the object\n        # - Updating world model with detection results\n\n        # For simulation, assume search succeeds\n        return True\n\n    def execute_path_planning(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute path planning\"\"\"\n        target_location = params['target_location']\n        self.world_model.symbol_grounding.get_logger().info(f\"Planning path to {target_location}\")\n\n        # Use navigation stack for path planning\n        # This would interface with Nav2 or similar\n        return True\n\n    def execute_navigation(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute navigation\"\"\"\n        target_location = params.get('target_location')\n        target_object = params.get('target_object')\n\n        if target_location:\n            self.world_model.symbol_grounding.get_logger().info(f\"Navigating to {target_location}\")\n        elif target_object:\n            self.world_model.symbol_grounding.get_logger().info(f\"Navigating to {target_object}\")\n\n        # This would send navigation goals to the robot\n        return True\n\n    def execute_grasp(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute grasping action\"\"\"\n        target_object = params['target_object']\n        self.world_model.symbol_grounding.get_logger().info(f\"Attempting to grasp {target_object}\")\n\n        # This would send grasp commands to the robot\n        return True\n\n# Integration example\ndef integrate_language_grounding_with_task_system():\n    \"\"\"Example of integrating language grounding with task abstraction\"\"\"\n\n    # Initialize capabilities\n    capabilities = HumanoidCapabilities(\n        max_linear_velocity=0.3,\n        max_angular_velocity=0.4,\n        step_height=0.15,\n        reach_distance=0.9,\n        lifting_capacity=3.0\n    )\n\n    # Initialize world model\n    world_model = WorldModel(capabilities)\n\n    # Initialize task decomposer\n    task_decomposer = TaskDecomposer(world_model)\n\n    # Example: Ground a command and decompose it\n    grounded_command = GroundedAction(\n        action_type='fetch_object',\n        target_object='red_cup',\n        target_location=None,\n        parameters={'precision': 'high'},\n        constraints=['avoid_people', 'be_safe']\n    )\n\n    # Decompose the command into tasks\n    root_task = task_decomposer.decompose_command(grounded_command)\n\n    print(f\"Decomposed command into task: {root_task.name}\")\n    print(f\"Subtasks: {[subtask.name for subtask in root_task.subtasks]}\")\n\n    return root_task\n```\n\n## Constraints and Affordances\n\nDefining what the robot can and cannot do based on language commands:\n\n```python\nclass ConstraintHandler:\n    \"\"\"Handles constraints derived from language commands\"\"\"\n    def __init__(self, world_model: WorldModel):\n        self.world_model = world_model\n        self.constraint_templates = self.define_constraint_templates()\n\n    def define_constraint_templates(self) -> Dict[str, Callable]:\n        \"\"\"Define constraint templates that can be instantiated from language\"\"\"\n        return {\n            'spatial': self.handle_spacial_constraint,\n            'temporal': self.handle_temporal_constraint,\n            'physical': self.handle_physical_constraint,\n            'social': self.handle_social_constraint,\n            'safety': self.handle_safety_constraint\n        }\n\n    def parse_constraints_from_command(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse constraints from natural language command\"\"\"\n        constraints = []\n\n        # Look for spatial constraints\n        if 'avoid' in command.lower() or 'stay away' in command.lower():\n            constraints.append({\n                'type': 'spatial',\n                'constraint_subtype': 'avoid_region',\n                'parameters': self.extract_region(command)\n            })\n\n        if 'near' in command.lower() or 'close to' in command.lower():\n            constraints.append({\n                'type': 'spatial',\n                'constraint_subtype': 'proximity',\n                'parameters': self.extract_reference_object(command)\n            })\n\n        # Look for safety constraints\n        if 'carefully' in command.lower() or 'safely' in command.lower():\n            constraints.append({\n                'type': 'safety',\n                'constraint_subtype': 'reduce_speed',\n                'parameters': {'factor': 0.5}  # Reduce speed by half\n            })\n\n        if 'slowly' in command.lower():\n            constraints.append({\n                'type': 'safety',\n                'constraint_subtype': 'reduce_speed',\n                'parameters': {'factor': 0.3}  # Reduce speed significantly\n            })\n\n        # Look for temporal constraints\n        if 'quickly' in command.lower() or 'fast' in command.lower():\n            constraints.append({\n                'type': 'temporal',\n                'constraint_subtype': 'minimize_time',\n                'parameters': {}\n            })\n\n        # Look for social constraints\n        if 'people' in command.lower() or 'person' in command.lower():\n            constraints.append({\n                'type': 'social',\n                'constraint_subtype': 'maintain_personal_space',\n                'parameters': {'distance': 1.0}  # 1 meter personal space\n            })\n\n        return constraints\n\n    def apply_constraints_to_plan(self, task_plan: Task, constraints: List[Dict[str, Any]]) -> Task:\n        \"\"\"Apply constraints to a task plan\"\"\"\n        constrained_plan = task_plan\n\n        for constraint in constraints:\n            constraint_handler = self.constraint_templates.get(constraint['type'])\n            if constraint_handler:\n                constrained_plan = constraint_handler(constrained_plan, constraint)\n\n        return constrained_plan\n\n    def handle_spacial_constraint(self, task_plan: Task, constraint: Dict[str, Any]) -> Task:\n        \"\"\"Handle spatial constraints\"\"\"\n        if constraint['constraint_subtype'] == 'avoid_region':\n            # Modify navigation tasks to avoid specific regions\n            for subtask in task_plan.subtasks:\n                if 'navigate' in subtask.name:\n                    # Add avoidance constraints to path planning\n                    if 'avoid_regions' not in subtask.parameters:\n                        subtask.parameters['avoid_regions'] = []\n                    subtask.parameters['avoid_regions'].append(\n                        constraint['parameters']['region']\n                    )\n\n        elif constraint['constraint_subtype'] == 'proximity':\n            # Ensure robot stays near a specific object\n            for subtask in task_plan.subtasks:\n                if 'navigate' in subtask.name:\n                    subtask.parameters['proximity_constraint'] = constraint['parameters']['object']\n\n        return task_plan\n\n    def handle_safety_constraint(self, task_plan: Task, constraint: Dict[str, Any]) -> Task:\n        \"\"\"Handle safety constraints\"\"\"\n        if constraint['constraint_subtype'] == 'reduce_speed':\n            # Reduce speed for all motion-related tasks\n            factor = constraint['parameters']['factor']\n            for subtask in task_plan.subtasks:\n                if any(motion_word in subtask.name for motion_word in ['navigate', 'move', 'motion']):\n                    if 'speed_factor' not in subtask.parameters:\n                        subtask.parameters['speed_factor'] = factor\n                    else:\n                        # Use the most conservative (lowest) speed factor\n                        subtask.parameters['speed_factor'] = min(\n                            subtask.parameters['speed_factor'],\n                            factor\n                        )\n\n        return task_plan\n\n    def extract_region(self, command: str) -> Dict[str, Any]:\n        \"\"\"Extract region information from command\"\"\"\n        # In practice, this would use more sophisticated NLP\n        # For now, return a placeholder\n        return {'region': 'unknown', 'object': self.extract_reference_object(command)}\n\n    def extract_reference_object(self, command: str) -> str:\n        \"\"\"Extract reference object from command\"\"\"\n        # Simple keyword extraction\n        # In practice, use proper NLP\n        if 'table' in command.lower():\n            return 'table'\n        elif 'chair' in command.lower():\n            return 'chair'\n        elif 'person' in command.lower():\n            return 'person'\n        else:\n            return 'unknown'\n\n# Example of complete integration\ndef complete_language_grounding_example():\n    \"\"\"Complete example integrating all components\"\"\"\n\n    # Initialize system components\n    capabilities = HumanoidCapabilities()\n    world_model = WorldModel(capabilities)\n    task_decomposer = TaskDecomposer(world_model)\n    constraint_handler = ConstraintHandler(world_model)\n\n    # Simulate a natural language command\n    raw_command = \"Carefully navigate near the table and gently grasp the red cup, avoiding people\"\n\n    # Parse command (simplified - in practice use NLP)\n    parsed = {\n        'raw_command': raw_command,\n        'entities': [{'text': 'table', 'label': 'OBJECT'}, {'text': 'red cup', 'label': 'OBJECT'}],\n        'actions': [{'text': 'navigate', 'lemma': 'navigate'}, {'text': 'grasp', 'lemma': 'grasp'}]\n    }\n\n    # Create grounded action (simplified)\n    grounded_action = GroundedAction(\n        action_type='fetch_object',\n        target_object='red_cup',\n        target_location=None,  # Will be determined from context\n        parameters={'precision': 'high'},\n        constraints=[]\n    )\n\n    # Parse constraints from command\n    constraints = constraint_handler.parse_constraints_from_command(raw_command)\n\n    # Decompose command into tasks\n    task_plan = task_decomposer.decompose_command(grounded_action)\n\n    # Apply constraints to task plan\n    constrained_task_plan = constraint_handler.apply_constraints_to_plan(task_plan, constraints)\n\n    print(f\"Original command: {raw_command}\")\n    print(f\"Constraints identified: {[(c['constraint_subtype'], c.get('parameters', {})) for c in constraints]}\")\n    print(f\"Task plan: {constrained_task_plan.name}\")\n    print(f\"Subtasks: {[sub.name for sub in constrained_task_plan.subtasks]}\")\n\n    # Check if robot can execute the constrained plan\n    can_execute, reasons = world_model.can_execute_action(grounded_action)\n    print(f\"Can execute: {can_execute}, Reasons: {reasons}\")\n\n    return constrained_task_plan\n\nif __name__ == \"__main__\":\n    # Run the complete example\n    task_plan = complete_language_grounding_example()\n```\n\n## Practical Exercise\n\n1. Implement symbol grounding for common household objects\n2. Create a world model that tracks object states and relationships\n3. Develop task decomposition for multi-step commands\n4. Define constraints based on natural language modifiers\n\n## Summary\n\nLanguage grounding in robotics connects natural language to real-world entities, actions, and states. For humanoid robots, this requires sophisticated understanding of spatial relationships, affordances, and constraints. The system must maintain a dynamic world model, decompose complex commands into executable tasks, and respect constraints expressed in natural language. Successful language grounding enables intuitive human-robot interaction for complex manipulation and navigation tasks.",
    "module": "Module 4: Vision-Language-Action (VLA)",
    "chapter": "language grounding robotics.md",
    "path": "/docs/module-4/4.2-language-grounding-robotics"
  },
  {
    "id": "module-4/4.3-llm-based-cognitive-planning",
    "title": "Chapter 4.3 – LLM-Based Cognitive Planning",
    "content": "\n# Chapter 4.3 – LLM-Based Cognitive Planning\n\n## Learning Objectives\n- Implement cognitive planning with Large Language Models\n- Integrate ROS 2 action orchestration with LLMs\n- Design error recovery mechanisms for autonomous systems\n- Create tool-using agents for humanoid robotics\n\n## Cognitive Planning with LLMs\n\nLarge Language Models can serve as high-level cognitive planners for humanoid robots, decomposing complex tasks into executable sequences:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import Pose, Point\nfrom move_base_msgs.action import MoveBase\nfrom manipulation_msgs.action import GraspObject\nfrom sensor_msgs.msg import Image\nimport openai\nimport json\nimport re\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass\nimport asyncio\nimport threading\nimport queue\n\n@dataclass\nclass TaskPlan:\n    \"\"\"Represents a high-level task plan\"\"\"\n    id: str\n    description: str\n    subtasks: List['SubTask']\n    dependencies: List[str]  # IDs of tasks that must complete first\n    priority: int = 1  # Higher number = higher priority\n\n@dataclass\nclass SubTask:\n    \"\"\"Represents a subtask in the plan\"\"\"\n    id: str\n    action_type: str  # 'navigation', 'manipulation', 'perception', 'communication'\n    parameters: Dict[str, Any]\n    estimated_duration: float  # in seconds\n    success_criteria: List[str]  # Conditions that define success\n    recovery_procedures: List[str]  # What to do if it fails\n\nclass CognitivePlannerNode(Node):\n    def __init__(self):\n        super().__init__('cognitive_planner_node')\n\n        # Initialize LLM client\n        self.llm_client = openai.OpenAI(api_key=self.get_parameter_or_set_default('openai_api_key', ''))\n\n        # ROS 2 action clients\n        self.nav_client = ActionClient(self, MoveBase, 'move_base')\n        self.grasp_client = ActionClient(self, GraspObject, 'grasp_object')\n\n        # Subscriptions\n        self.high_level_command_sub = self.create_subscription(\n            String, '/high_level_command', self.command_callback, 10\n        )\n        self.task_status_sub = self.create_subscription(\n            String, '/task_status', self.task_status_callback, 10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, '/generated_plan', 10)\n        self.status_pub = self.create_publisher(String, '/cognitive_status', 10)\n\n        # Task management\n        self.current_plan: Optional[TaskPlan] = None\n        self.active_tasks: Dict[str, Any] = {}\n        self.completed_tasks: List[str] = []\n        self.failed_tasks: List[str] = []\n\n        # System state\n        self.robot_state = {\n            'location': Point(x=0.0, y=0.0, z=0.0),\n            'battery_level': 1.0,\n            'grippers': {'left': 'free', 'right': 'free'},\n            'current_task': None\n        }\n\n        # Environment knowledge\n        self.known_objects = {}\n        self.known_locations = {}\n\n        # Initialize\n        self.load_environment_knowledge()\n        self.plan_queue = queue.Queue()\n\n        self.get_logger().info('Cognitive Planner Node initialized')\n\n    def get_parameter_or_set_default(self, param_name: str, default_value: str) -> str:\n        \"\"\"Get parameter or set default\"\"\"\n        param_desc = rclpy.parameter.ParameterDescriptor()\n        self.declare_parameter(param_name, default_value, param_desc)\n        return self.get_parameter(param_name).value or default_value\n\n    def command_callback(self, msg: String):\n        \"\"\"Handle high-level natural language commands\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received high-level command: \"{command}\"')\n\n        # Generate plan using LLM\n        plan = self.generate_plan_with_llm(command)\n\n        if plan:\n            self.current_plan = plan\n            self.execute_plan(plan)\n        else:\n            self.get_logger().error(f'Failed to generate plan for command: \"{command}\"')\n\n    def generate_plan_with_llm(self, command: str) -> Optional[TaskPlan]:\n        \"\"\"Generate a task plan using LLM\"\"\"\n        try:\n            # Construct prompt for the LLM\n            prompt = self.construct_planning_prompt(command)\n\n            response = self.llm_client.chat.completions.create(\n                model=\"gpt-4-turbo\",  # Use appropriate model\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": self.get_planning_system_prompt()\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": prompt\n                    }\n                ],\n                temperature=0.3,\n                max_tokens=1000,\n                response_format={\"type\": \"json_object\"}\n            )\n\n            # Parse the response\n            plan_json = json.loads(response.choices[0].message.content)\n\n            # Convert to TaskPlan object\n            plan = self.json_to_task_plan(plan_json)\n\n            # Publish the plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan_json)\n            self.plan_pub.publish(plan_msg)\n\n            self.get_logger().info(f'Generated plan with {len(plan.subtasks)} subtasks')\n\n            return plan\n\n        except Exception as e:\n            self.get_logger().error(f'Error generating plan with LLM: {e}')\n            return None\n\n    def construct_planning_prompt(self, command: str) -> str:\n        \"\"\"Construct prompt for LLM planning\"\"\"\n        prompt = f\"\"\"\n        You are a cognitive planner for a humanoid robot. Your task is to decompose high-level commands into executable subtasks.\n\n        Command: \"{command}\"\n\n        Current Robot State:\n        - Location: {self.robot_state['location']}\n        - Battery Level: {self.robot_state['battery_level']:.2f}\n        - Grippers: {self.robot_state['grippers']}\n        - Available Actions: navigation, manipulation, perception, communication\n\n        Known Objects: {list(self.known_objects.keys())}\n        Known Locations: {list(self.known_locations.keys())}\n\n        Please generate a detailed task plan in JSON format with the following structure:\n        {{\n            \"id\": \"unique_plan_id\",\n            \"description\": \"Brief description of the plan\",\n            \"subtasks\": [\n                {{\n                    \"id\": \"unique_subtask_id\",\n                    \"action_type\": \"navigation|manipulation|perception|communication\",\n                    \"parameters\": {{\"param1\": \"value1\", \"param2\": \"value2\"}},\n                    \"estimated_duration\": 10.0,\n                    \"success_criteria\": [\"condition1\", \"condition2\"],\n                    \"recovery_procedures\": [\"procedure1\", \"procedure2\"]\n                }}\n            ],\n            \"dependencies\": [\"subtask_id_1\", \"subtask_id_2\"]\n        }}\n\n        The plan should be executable by a humanoid robot with basic navigation and manipulation capabilities.\n        Consider safety, efficiency, and the robot's current state when generating the plan.\n        \"\"\"\n\n        return prompt\n\n    def get_planning_system_prompt(self) -> str:\n        \"\"\"Get system prompt for planning\"\"\"\n        return \"\"\"\n        You are an expert cognitive planner for humanoid robots. Your role is to decompose complex natural language commands into executable task plans.\n\n        Guidelines:\n        1. Break down complex tasks into atomic subtasks\n        2. Ensure each subtask is achievable with basic robot capabilities\n        3. Consider spatial relationships and object affordances\n        4. Include error recovery procedures\n        5. Respect robot limitations (reach, carrying capacity, etc.)\n        6. Prioritize safety in all actions\n        7. Consider dependencies between subtasks\n\n        Action Types:\n        - navigation: Moving the robot to a location\n        - manipulation: Grasping, placing, opening, closing objects\n        - perception: Looking for objects, scanning environment\n        - communication: Speaking, signaling, acknowledging\n\n        Each subtask should have:\n        - Specific parameters for execution\n        - Success criteria that can be verified\n        - Recovery procedures for common failure modes\n        - Estimated duration for planning purposes\n        \"\"\"\n\n    def json_to_task_plan(self, plan_json: Dict[str, Any]) -> TaskPlan:\n        \"\"\"Convert JSON response to TaskPlan object\"\"\"\n        subtasks = []\n        for subtask_json in plan_json.get('subtasks', []):\n            subtask = SubTask(\n                id=subtask_json.get('id', f'subtask_{len(subtasks)}'),\n                action_type=subtask_json.get('action_type', 'unknown'),\n                parameters=subtask_json.get('parameters', {}),\n                estimated_duration=subtask_json.get('estimated_duration', 10.0),\n                success_criteria=subtask_json.get('success_criteria', []),\n                recovery_procedures=subtask_json.get('recovery_procedures', [])\n            )\n            subtasks.append(subtask)\n\n        plan = TaskPlan(\n            id=plan_json.get('id', 'plan_0'),\n            description=plan_json.get('description', 'No description'),\n            subtasks=subtasks,\n            dependencies=plan_json.get('dependencies', []),\n            priority=plan_json.get('priority', 1)\n        )\n\n        return plan\n\n    def execute_plan(self, plan: TaskPlan):\n        \"\"\"Execute the generated plan\"\"\"\n        self.get_logger().info(f'Executing plan: {plan.description}')\n\n        for subtask in plan.subtasks:\n            if subtask.id in self.completed_tasks:\n                continue  # Already completed\n\n            if self.check_dependencies_met(subtask, plan.dependencies):\n                self.execute_subtask(subtask)\n            else:\n                self.get_logger().warn(f'Skipping subtask {subtask.id} - dependencies not met')\n\n    def check_dependencies_met(self, subtask: SubTask, dependencies: List[str]) -> bool:\n        \"\"\"Check if dependencies for a subtask are met\"\"\"\n        # For now, just check if dependent tasks are completed\n        # In practice, this would be more sophisticated\n        for dep in dependencies:\n            if dep not in self.completed_tasks:\n                return False\n        return True\n\n    def execute_subtask(self, subtask: SubTask):\n        \"\"\"Execute a single subtask\"\"\"\n        self.get_logger().info(f'Executing subtask: {subtask.id} ({subtask.action_type})')\n\n        if subtask.action_type == 'navigation':\n            self.execute_navigation_subtask(subtask)\n        elif subtask.action_type == 'manipulation':\n            self.execute_manipulation_subtask(subtask)\n        elif subtask.action_type == 'perception':\n            self.execute_perception_subtask(subtask)\n        elif subtask.action_type == 'communication':\n            self.execute_communication_subtask(subtask)\n        else:\n            self.get_logger().error(f'Unknown action type: {subtask.action_type}')\n\n    def execute_navigation_subtask(self, subtask: SubTask):\n        \"\"\"Execute navigation subtask\"\"\"\n        try:\n            target_pose = Pose()\n            target_pose.position.x = subtask.parameters.get('x', 0.0)\n            target_pose.position.y = subtask.parameters.get('y', 0.0)\n            target_pose.position.z = subtask.parameters.get('z', 0.0)\n\n            # Create navigation goal\n            goal_msg = MoveBase.Goal()\n            goal_msg.target_pose.header.frame_id = 'map'\n            goal_msg.target_pose.header.stamp = self.get_clock().now().to_msg()\n            goal_msg.target_pose.pose = target_pose\n\n            # Send goal asynchronously\n            self.nav_client.wait_for_server()\n            future = self.nav_client.send_goal_async(goal_msg)\n\n            # Store active task\n            self.active_tasks[subtask.id] = {\n                'type': 'navigation',\n                'future': future,\n                'subtask': subtask\n            }\n\n            # Wait for result in a separate thread\n            threading.Thread(\n                target=self.wait_for_navigation_result,\n                args=(subtask.id, future),\n                daemon=True\n            ).start()\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing navigation subtask: {e}')\n            self.handle_subtask_failure(subtask)\n\n    def execute_manipulation_subtask(self, subtask: SubTask):\n        \"\"\"Execute manipulation subtask\"\"\"\n        try:\n            # Create manipulation goal\n            goal_msg = GraspObject.Goal()\n            goal_msg.object_name = subtask.parameters.get('object_name', '')\n            goal_msg.pre_grasp_distance = subtask.parameters.get('approach_distance', 0.1)\n            goal_msg.grasp_type = subtask.parameters.get('grasp_type', 'pinch')\n\n            # Send goal\n            self.grasp_client.wait_for_server()\n            future = self.grasp_client.send_goal_async(goal_msg)\n\n            # Store active task\n            self.active_tasks[subtask.id] = {\n                'type': 'manipulation',\n                'future': future,\n                'subtask': subtask\n            }\n\n            # Wait for result\n            threading.Thread(\n                target=self.wait_for_manipulation_result,\n                args=(subtask.id, future),\n                daemon=True\n            ).start()\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing manipulation subtask: {e}')\n            self.handle_subtask_failure(subtask)\n\n    def execute_perception_subtask(self, subtask: SubTask):\n        \"\"\"Execute perception subtask\"\"\"\n        try:\n            # For perception, we might need to call a service or trigger object detection\n            object_name = subtask.parameters.get('object_name', 'any')\n            search_area = subtask.parameters.get('search_area', 'current_view')\n\n            # This would typically call object detection service\n            # For now, simulate the process\n            result = self.simulate_perception_task(object_name, search_area)\n\n            if result:\n                self.handle_subtask_success(subtask)\n            else:\n                self.handle_subtask_failure(subtask)\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing perception subtask: {e}')\n            self.handle_subtask_failure(subtask)\n\n    def execute_communication_subtask(self, subtask: SubTask):\n        \"\"\"Execute communication subtask\"\"\"\n        try:\n            # Publish speech command\n            speech_pub = self.create_publisher(String, '/tts_input', 10)\n            speech_msg = String()\n            speech_msg.data = subtask.parameters.get('text', 'Hello')\n            speech_pub.publish(speech_msg)\n\n            # Mark as completed immediately\n            self.handle_subtask_success(subtask)\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing communication subtask: {e}')\n            self.handle_subtask_failure(subtask)\n\n    def wait_for_navigation_result(self, task_id: str, future):\n        \"\"\"Wait for navigation result\"\"\"\n        try:\n            goal_handle = future.result()\n            result_future = goal_handle.get_result_async()\n\n            # Wait for result with timeout\n            rclpy.spin_until_future_complete(self, result_future, timeout_sec=30.0)\n\n            if result_future.result() is not None:\n                result = result_future.result().result\n                status = result_future.result().status\n\n                if status == GoalStatus.STATUS_SUCCEEDED:\n                    self.handle_subtask_success(self.active_tasks[task_id]['subtask'])\n                else:\n                    self.handle_subtask_failure(self.active_tasks[task_id]['subtask'])\n            else:\n                self.get_logger().error(f'Navigation task {task_id} timed out')\n                self.handle_subtask_failure(self.active_tasks[task_id]['subtask'])\n\n        except Exception as e:\n            self.get_logger().error(f'Error waiting for navigation result: {e}')\n            self.handle_subtask_failure(self.active_tasks[task_id]['subtask'])\n\n        finally:\n            if task_id in self.active_tasks:\n                del self.active_tasks[task_id]\n\n    def wait_for_manipulation_result(self, task_id: str, future):\n        \"\"\"Wait for manipulation result\"\"\"\n        try:\n            goal_handle = future.result()\n            result_future = goal_handle.get_result_async()\n\n            # Wait for result with timeout\n            rclpy.spin_until_future_complete(self, result_future, timeout_sec=30.0)\n\n            if result_future.result() is not None:\n                result = result_future.result().result\n                status = result_future.result().status\n\n                if status == GoalStatus.STATUS_SUCCEEDED:\n                    self.handle_subtask_success(self.active_tasks[task_id]['subtask'])\n                else:\n                    self.handle_subtask_failure(self.active_tasks[task_id]['subtask'])\n            else:\n                self.get_logger().error(f'Manipulation task {task_id} timed out')\n                self.handle_subtask_failure(self.active_tasks[task_id]['subtask'])\n\n        except Exception as e:\n            self.get_logger().error(f'Error waiting for manipulation result: {e}')\n            self.handle_subtask_failure(self.active_tasks[task_id]['subtask'])\n\n        finally:\n            if task_id in self.active_tasks:\n                del self.active_tasks[task_id]\n\n    def handle_subtask_success(self, subtask: SubTask):\n        \"\"\"Handle successful subtask completion\"\"\"\n        self.get_logger().info(f'Subtask {subtask.id} completed successfully')\n        self.completed_tasks.append(subtask.id)\n\n        # Update robot state based on task completion\n        self.update_robot_state_after_task(subtask)\n\n        # Check if overall plan is complete\n        if self.current_plan and all(st.id in self.completed_tasks for st in self.current_plan.subtasks):\n            self.get_logger().info('Plan completed successfully!')\n            self.current_plan = None\n\n    def handle_subtask_failure(self, subtask: SubTask):\n        \"\"\"Handle subtask failure and recovery\"\"\"\n        self.get_logger().error(f'Subtask {subtask.id} failed')\n\n        # Add to failed tasks\n        self.failed_tasks.append(subtask.id)\n\n        # Try recovery procedures\n        for recovery_proc in subtask.recovery_procedures:\n            if self.execute_recovery_procedure(recovery_proc, subtask):\n                # Recovery succeeded, retry the task\n                self.get_logger().info(f'Recovery procedure succeeded, retrying task {subtask.id}')\n                self.execute_subtask(subtask)\n                return\n\n        # If all recovery attempts fail, escalate\n        self.escalate_failure(subtask)\n\n    def execute_recovery_procedure(self, procedure: str, failed_task: SubTask) -> bool:\n        \"\"\"Execute a recovery procedure\"\"\"\n        self.get_logger().info(f'Attempting recovery procedure: {procedure}')\n\n        if procedure == 'retry_with_different_approach':\n            # Modify parameters and retry\n            new_params = failed_task.parameters.copy()\n            # Add variation to approach\n            new_params['approach_angle'] = new_params.get('approach_angle', 0) + 0.2\n            retry_task = SubTask(\n                id=f\"{failed_task.id}_retry\",\n                action_type=failed_task.action_type,\n                parameters=new_params,\n                estimated_duration=failed_task.estimated_duration,\n                success_criteria=failed_task.success_criteria,\n                recovery_procedures=failed_task.recovery_procedures\n            )\n            self.execute_subtask(retry_task)\n            return True\n\n        elif procedure == 'request_human_assistance':\n            # Request help from human operator\n            help_request = String()\n            help_request.data = f\"Need assistance with task: {failed_task.id}\"\n            help_pub = self.create_publisher(String, '/help_requests', 10)\n            help_pub.publish(help_request)\n            return True\n\n        elif procedure == 'use_alternative_method':\n            # Try alternative approach\n            if failed_task.action_type == 'navigation':\n                # Try a different path planning algorithm\n                new_params = failed_task.parameters.copy()\n                new_params['planner_type'] = 'alternative'\n                retry_task = SubTask(\n                    id=f\"{failed_task.id}_alt\",\n                    action_type=failed_task.action_type,\n                    parameters=new_params,\n                    estimated_duration=failed_task.estimated_duration,\n                    success_criteria=failed_task.success_criteria,\n                    recovery_procedures=failed_task.recovery_procedures\n                )\n                self.execute_subtask(retry_task)\n                return True\n\n        return False\n\n    def escalate_failure(self, failed_task: SubTask):\n        \"\"\"Escalate task failure to higher level\"\"\"\n        self.get_logger().error(f'All recovery attempts failed for task {failed_task.id}')\n\n        # Publish failure notification\n        status_msg = String()\n        status_msg.data = f\"TASK_FAILED: {failed_task.id} - {failed_task.action_type}\"\n        self.status_pub.publish(status_msg)\n\n        # If this was part of a larger plan, consider replanning\n        if self.current_plan:\n            self.consider_replanning(failed_task)\n\n    def consider_replanning(self, failed_task: SubTask):\n        \"\"\"Consider replanning when a task fails\"\"\"\n        self.get_logger().info('Considering replanning due to task failure')\n\n        # For now, just log and continue with remaining tasks\n        # In practice, this would call the LLM to generate an alternative plan\n        remaining_tasks = [\n            st for st in self.current_plan.subtasks\n            if st.id not in self.completed_tasks and st.id != failed_task.id\n        ]\n\n        if remaining_tasks:\n            self.get_logger().info(f'Continuing with {len(remaining_tasks)} remaining tasks')\n        else:\n            self.get_logger().info('No remaining tasks in plan')\n\n    def update_robot_state_after_task(self, subtask: SubTask):\n        \"\"\"Update robot state after task completion\"\"\"\n        if subtask.action_type == 'navigation':\n            # Update location\n            self.robot_state['location'].x = subtask.parameters.get('x', self.robot_state['location'].x)\n            self.robot_state['location'].y = subtask.parameters.get('y', self.robot_state['location'].y)\n            self.robot_state['location'].z = subtask.parameters.get('z', self.robot_state['location'].z)\n\n        elif subtask.action_type == 'manipulation':\n            if 'grasp' in subtask.action_type.lower():\n                # Update gripper state\n                hand = subtask.parameters.get('hand', 'right')\n                self.robot_state['grippers'][hand] = 'occupied'\n            elif 'place' in subtask.action_type.lower():\n                # Update gripper state\n                hand = subtask.parameters.get('hand', 'right')\n                self.robot_state['grippers'][hand] = 'free'\n\n    def task_status_callback(self, msg: String):\n        \"\"\"Handle task status updates from other nodes\"\"\"\n        try:\n            status_data = json.loads(msg.data)\n            task_id = status_data.get('task_id')\n            status = status_data.get('status')\n\n            if task_id in self.active_tasks:\n                if status == 'completed':\n                    self.handle_subtask_success(self.active_tasks[task_id]['subtask'])\n                elif status == 'failed':\n                    self.handle_subtask_failure(self.active_tasks[task_id]['subtask'])\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f'Invalid JSON in task status message: {msg.data}')\n\n    def load_environment_knowledge(self):\n        \"\"\"Load known objects and locations from environment\"\"\"\n        # In practice, this would come from semantic mapping or prior learning\n        # For demonstration:\n        self.known_objects = {\n            'red_cup': {'type': 'container', 'location': 'kitchen_table', 'properties': {'color': 'red', 'contains': 'water'}},\n            'blue_bottle': {'type': 'container', 'location': 'counter', 'properties': {'color': 'blue', 'contains': 'juice'}},\n            'chair_1': {'type': 'furniture', 'location': 'dining_room', 'properties': {'material': 'wood', 'occupiable': True}},\n            'book': {'type': 'document', 'location': 'shelf', 'properties': {'title': 'Robotics Handbook', 'author': 'Expert'}}\n        }\n\n        self.known_locations = {\n            'kitchen': {'coordinates': {'x': 5.0, 'y': 2.0}, 'contents': ['red_cup']},\n            'living_room': {'coordinates': {'x': 0.0, 'y': 0.0}, 'contents': ['chair_1']},\n            'dining_room': {'coordinates': {'x': 3.0, 'y': 1.0}, 'contents': ['chair_1']},\n            'office': {'coordinates': {'x': -2.0, 'y': 1.0}, 'contents': ['book']},\n            'entrance': {'coordinates': {'x': 0.0, 'y': -5.0}, 'contents': []}\n        }\n\n    def simulate_perception_task(self, object_name: str, search_area: str) -> bool:\n        \"\"\"Simulate perception task (in practice, this would call actual perception nodes)\"\"\"\n        # Check if object is known\n        if object_name in self.known_objects:\n            # Object exists, return success\n            return True\n\n        # Object not known, might need to search\n        # In practice, this would trigger object detection in the specified area\n        return False\n\nclass PlanValidator:\n    \"\"\"Validates plans generated by LLMs\"\"\"\n    def __init__(self, robot_capabilities: Dict[str, Any]):\n        self.capabilities = robot_capabilities\n\n    def validate_plan(self, plan: TaskPlan) -> Tuple[bool, List[str]]:\n        \"\"\"Validate a plan for feasibility\"\"\"\n        issues = []\n\n        # Check if robot can physically perform all actions\n        for subtask in plan.subtasks:\n            if not self.validate_subtask(subtask):\n                issues.append(f\"Subtask {subtask.id} not feasible with robot capabilities\")\n\n        # Check for logical consistency\n        if not self.check_logical_consistency(plan):\n            issues.append(\"Plan has logical inconsistencies\")\n\n        # Check resource constraints\n        if not self.check_resource_constraints(plan):\n            issues.append(\"Plan exceeds resource constraints\")\n\n        return len(issues) == 0, issues\n\n    def validate_subtask(self, subtask: SubTask) -> bool:\n        \"\"\"Validate if a subtask is feasible\"\"\"\n        if subtask.action_type == 'manipulation':\n            # Check if object is graspable\n            object_weight = subtask.parameters.get('weight', 1.0)\n            if object_weight > self.capabilities.get('max_lift_weight', 5.0):\n                return False\n\n        elif subtask.action_type == 'navigation':\n            # Check if destination is reachable\n            distance = self.calculate_distance(subtask.parameters)\n            max_range = self.capabilities.get('max_navigation_range', 10.0)\n            if distance > max_range:\n                return False\n\n        return True\n\n    def check_logical_consistency(self, plan: TaskPlan) -> bool:\n        \"\"\"Check if plan is logically consistent\"\"\"\n        # Check for contradictory subtasks\n        # Check for circular dependencies\n        # Check if preconditions are met\n        return True  # Simplified for example\n\n    def check_resource_constraints(self, plan: TaskPlan) -> bool:\n        \"\"\"Check if plan respects resource constraints\"\"\"\n        total_time = sum(st.estimated_duration for st in plan.subtasks)\n        max_time = self.capabilities.get('max_operation_time', 3600.0)  # 1 hour\n\n        if total_time > max_time:\n            return False\n\n        return True\n\n    def calculate_distance(self, params: Dict[str, Any]) -> float:\n        \"\"\"Calculate distance for navigation tasks\"\"\"\n        x = params.get('x', 0.0)\n        y = params.get('y', 0.0)\n        return (x**2 + y**2)**0.5\n\n# Example usage\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CognitivePlannerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## ROS 2 Action Orchestration\n\nIntegrating LLM-based planning with ROS 2 action architecture:\n\n```python\nimport rclpy\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nfrom geometry_msgs.msg import PoseStamped\nimport json\nfrom typing import Optional\n\nclass HierarchicalActionServer:\n    \"\"\"Action server that orchestrates complex tasks using LLM planning\"\"\"\n    def __init__(self, node: CognitivePlannerNode):\n        self.node = node\n        self.goal_handle = None\n\n        # Create action server\n        self.action_server = ActionServer(\n            node,\n            ComplexTask,\n            'complex_task_executor',\n            execute_callback=self.execute_callback,\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback,\n            callback_group=ReentrantCallbackGroup()\n        )\n\n    def goal_callback(self, goal_request):\n        \"\"\"Accept or reject goal requests\"\"\"\n        self.node.get_logger().info(f'Received complex task request: {goal_request.description}')\n        return GoalResponse.ACCEPT\n\n    def cancel_callback(self, goal_handle):\n        \"\"\"Accept or reject cancel requests\"\"\"\n        self.node.get_logger().info('Received cancel request')\n        return CancelResponse.ACCEPT\n\n    def execute_callback(self, goal_handle):\n        \"\"\"Execute the complex task\"\"\"\n        self.node.get_logger().info('Executing complex task...')\n        feedback_msg = ComplexTask.Feedback()\n        result = ComplexTask.Result()\n\n        try:\n            # Generate plan using LLM\n            plan = self.node.generate_plan_with_llm(goal_handle.request.description)\n            if not plan:\n                result.success = False\n                result.message = \"Failed to generate plan\"\n                goal_handle.abort()\n                return result\n\n            # Execute plan\n            for i, subtask in enumerate(plan.subtasks):\n                # Check if goal was cancelled\n                if goal_handle.is_cancel_requested:\n                    result.success = False\n                    result.message = \"Goal cancelled\"\n                    goal_handle.canceled()\n                    return result\n\n                # Update feedback\n                feedback_msg.current_subtask = subtask.id\n                feedback_msg.progress = (i + 1) / len(plan.subtasks)\n                goal_handle.publish_feedback(feedback_msg)\n\n                # Execute subtask\n                self.node.execute_subtask(subtask)\n\n                # Wait for subtask completion\n                # In practice, this would be more sophisticated\n                self.node.get_clock().sleep_for(Duration(seconds=1.0))\n\n            # All subtasks completed\n            result.success = True\n            result.message = f\"Completed {len(plan.subtasks)} subtasks\"\n            goal_handle.succeed()\n            self.node.get_logger().info('Complex task completed successfully')\n\n        except Exception as e:\n            self.node.get_logger().error(f'Error executing complex task: {e}')\n            result.success = False\n            result.message = f\"Execution error: {str(e)}\"\n            goal_handle.abort()\n\n        return result\n\n# Custom action message (would be defined in .action file)\n\"\"\"\n# ComplexTask.action\nstring description\n---\nbool success\nstring message\n---\nstring current_subtask\nfloat32 progress\n\"\"\"\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.duration import Duration\n\nclass ComplexTask:\n    class Goal:\n        def __init__(self):\n            self.description = \"\"\n\n    class Result:\n        def __init__(self):\n            self.success = False\n            self.message = \"\"\n\n    class Feedback:\n        def __init__(self):\n            self.current_subtask = \"\"\n            self.progress = 0.0\n\n# Error Recovery Mechanisms\nclass ErrorRecoveryManager:\n    \"\"\"Manages error recovery for LLM-based cognitive planning\"\"\"\n    def __init__(self, cognitive_planner: CognitivePlannerNode):\n        self.planner = cognitive_planner\n        self.recovery_strategies = self.define_recovery_strategies()\n\n    def define_recovery_strategies(self) -> Dict[str, callable]:\n        \"\"\"Define various recovery strategies\"\"\"\n        return {\n            'retry': self.retry_strategy,\n            'alternative_approach': self.alternative_approach_strategy,\n            'human_intervention': self.human_intervention_strategy,\n            'task_decomposition': self.task_decomposition_strategy,\n            'resource_reallocation': self.resource_reallocation_strategy,\n            'plan_revision': self.plan_revision_strategy\n        }\n\n    def handle_error(self, error_type: str, context: Dict[str, Any]) -> bool:\n        \"\"\"Handle an error using appropriate strategy\"\"\"\n        if error_type in self.recovery_strategies:\n            strategy = self.recovery_strategies[error_type]\n            return strategy(context)\n        else:\n            self.planner.get_logger().error(f'No recovery strategy for error type: {error_type}')\n            return False\n\n    def retry_strategy(self, context: Dict[str, Any]) -> bool:\n        \"\"\"Simple retry strategy\"\"\"\n        task = context.get('failed_task')\n        max_retries = context.get('max_retries', 3)\n        current_retry = context.get('current_retry', 0)\n\n        if current_retry < max_retries:\n            self.planner.get_logger().info(f'Retrying task {task.id}, attempt {current_retry + 1}')\n            context['current_retry'] = current_retry + 1\n            self.planner.execute_subtask(task)\n            return True\n\n        return False\n\n    def alternative_approach_strategy(self, context: Dict[str, Any]) -> bool:\n        \"\"\"Try an alternative approach to the same goal\"\"\"\n        task = context.get('failed_task')\n        self.planner.get_logger().info(f'Trying alternative approach for task {task.id}')\n\n        # Generate alternative plan using LLM\n        alternative_plan = self.generate_alternative_plan(task, context)\n        if alternative_plan:\n            self.planner.execute_subtask(alternative_plan)\n            return True\n\n        return False\n\n    def human_intervention_strategy(self, context: Dict[str, Any]) -> bool:\n        \"\"\"Request human intervention\"\"\"\n        task = context.get('failed_task')\n        error_details = context.get('error_details', '')\n\n        self.planner.get_logger().info(f'Requesting human intervention for task {task.id}')\n\n        # Publish help request\n        help_msg = String()\n        help_msg.data = f\"Human assistance needed: Task {task.id} failed. Error: {error_details}\"\n        help_pub = self.planner.create_publisher(String, '/help_requests', 10)\n        help_pub.publish(help_msg)\n\n        return True\n\n    def task_decomposition_strategy(self, context: Dict[str, Any]) -> bool:\n        \"\"\"Decompose failed task into smaller subtasks\"\"\"\n        task = context.get('failed_task')\n        self.planner.get_logger().info(f'Decomposing task {task.id} into smaller tasks')\n\n        # Ask LLM to break down the task\n        decomposition_prompt = f\"\"\"\n        The following task failed: {task.description}\n        Task parameters: {task.parameters}\n        Error context: {context.get('error_details', 'Unknown error')}\n\n        Please decompose this task into smaller, more manageable subtasks that are less likely to fail.\n        Provide the decomposition in the same format as the original task.\n        \"\"\"\n\n        try:\n            response = self.planner.llm_client.chat.completions.create(\n                model=\"gpt-4-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a task decomposition expert for robotics.\"},\n                    {\"role\": \"user\", \"content\": decomposition_prompt}\n                ],\n                temperature=0.3,\n                response_format={\"type\": \"json_object\"}\n            )\n\n            decomposition = json.loads(response.choices[0].message.content)\n\n            # Execute decomposed tasks\n            for subtask_data in decomposition.get('subtasks', []):\n                subtask = self.planner.json_to_subtask(subtask_data)\n                self.planner.execute_subtask(subtask)\n\n            return True\n\n        except Exception as e:\n            self.planner.get_logger().error(f'Error in task decomposition: {e}')\n            return False\n\n    def plan_revision_strategy(self, context: Dict[str, Any]) -> bool:\n        \"\"\"Revise the overall plan based on failure\"\"\"\n        failed_task = context.get('failed_task')\n        current_plan = context.get('current_plan')\n\n        self.planner.get_logger().info('Revising overall plan due to task failure')\n\n        # Generate new plan that accounts for the failure\n        revision_prompt = f\"\"\"\n        The following task failed: {failed_task.description}\n        Original plan: {current_plan.description if current_plan else 'Unknown'}\n        Error context: {context.get('error_details', 'Unknown error')}\n\n        Please generate a revised plan that achieves the same overall goal but avoids the failure mode.\n        Consider alternative approaches, different sequences, or modified parameters.\n        \"\"\"\n\n        try:\n            response = self.planner.llm_client.chat.completions.create(\n                model=\"gpt-4-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a plan revision expert for robotics.\"},\n                    {\"role\": \"user\", \"content\": revision_prompt}\n                ],\n                temperature=0.4,\n                response_format={\"type\": \"json_object\"}\n            )\n\n            revised_plan_data = json.loads(response.choices[0].message.content)\n            revised_plan = self.planner.json_to_task_plan(revised_plan_data)\n\n            # Execute the revised plan\n            self.planner.current_plan = revised_plan\n            self.planner.execute_plan(revised_plan)\n\n            return True\n\n        except Exception as e:\n            self.planner.get_logger().error(f'Error in plan revision: {e}')\n            return False\n\n    def generate_alternative_plan(self, task: SubTask, context: Dict[str, Any]) -> Optional[SubTask]:\n        \"\"\"Generate an alternative approach to the same goal\"\"\"\n        # In practice, this would call the LLM to generate alternative approaches\n        # For now, return a modified version of the original task\n        return SubTask(\n            id=f\"{task.id}_alt\",\n            action_type=task.action_type,\n            parameters=self.modify_parameters_for_alternative(task.parameters),\n            estimated_duration=task.estimated_duration,\n            success_criteria=task.success_criteria,\n            recovery_procedures=task.recovery_procedures\n        )\n\n    def modify_parameters_for_alternative(self, parameters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Modify parameters to create an alternative approach\"\"\"\n        new_params = parameters.copy()\n\n        # Example modifications:\n        # - Change approach angle\n        # - Use different hand/arm\n        # - Increase safety margins\n        # - Use different tool/object\n\n        if 'approach_angle' in new_params:\n            new_params['approach_angle'] += 0.3  # Different angle\n        elif 'hand' in new_params:\n            # Switch hands if available\n            if new_params['hand'] == 'right':\n                new_params['hand'] = 'left'\n            else:\n                new_params['hand'] = 'right'\n\n        # Add safety margin\n        if 'safety_margin' in new_params:\n            new_params['safety_margin'] *= 1.5\n        else:\n            new_params['safety_margin'] = 0.1\n\n        return new_params\n\n# Tool-Using Agents\nclass ToolUsingAgent:\n    \"\"\"An agent that can use tools based on LLM reasoning\"\"\"\n    def __init__(self, cognitive_planner: CognitivePlannerNode):\n        self.planner = cognitive_planner\n        self.available_tools = self.register_tools()\n        self.tool_usage_history = []\n\n    def register_tools(self) -> Dict[str, callable]:\n        \"\"\"Register available tools that the agent can use\"\"\"\n        return {\n            'navigation_tool': self.navigation_tool,\n            'object_detection_tool': self.object_detection_tool,\n            'grasping_tool': self.grasping_tool,\n            'manipulation_tool': self.manipulation_tool,\n            'communication_tool': self.communication_tool,\n            'state_query_tool': self.state_query_tool,\n            'environment_mapping_tool': self.environment_mapping_tool,\n            'battery_check_tool': self.battery_check_tool\n        }\n\n    def navigation_tool(self, **kwargs) -> Dict[str, Any]:\n        \"\"\"Tool for navigation\"\"\"\n        target_x = kwargs.get('x', 0.0)\n        target_y = kwargs.get('y', 0.0)\n        target_z = kwargs.get('z', 0.0)\n\n        # Create and execute navigation subtask\n        nav_subtask = SubTask(\n            id=f\"nav_to_{target_x}_{target_y}\",\n            action_type='navigation',\n            parameters={'x': target_x, 'y': target_y, 'z': target_z},\n            estimated_duration=30.0,\n            success_criteria=['reached_destination'],\n            recovery_procedures=['retry_with_different_path']\n        )\n\n        self.planner.execute_subtask(nav_subtask)\n\n        return {\n            'success': True,\n            'result': f'Navigated to ({target_x}, {target_y}, {target_z})',\n            'execution_time': 30.0\n        }\n\n    def object_detection_tool(self, **kwargs) -> Dict[str, Any]:\n        \"\"\"Tool for detecting objects\"\"\"\n        object_name = kwargs.get('object_name', 'any')\n        search_area = kwargs.get('search_area', 'current_view')\n\n        # Simulate object detection\n        detected = self.planner.simulate_perception_task(object_name, search_area)\n\n        if detected and object_name in self.planner.known_objects:\n            obj_info = self.planner.known_objects[object_name]\n            return {\n                'success': True,\n                'result': f'Found {object_name} at {obj_info[\"location\"]}',\n                'object_info': obj_info\n            }\n        else:\n            return {\n                'success': False,\n                'result': f'Could not find {object_name}',\n                'object_info': None\n            }\n\n    def grasping_tool(self, **kwargs) -> Dict[str, Any]:\n        \"\"\"Tool for grasping objects\"\"\"\n        object_name = kwargs.get('object_name')\n        approach_distance = kwargs.get('approach_distance', 0.1)\n\n        # Create and execute grasping subtask\n        grasp_subtask = SubTask(\n            id=f\"grasp_{object_name}\",\n            action_type='manipulation',\n            parameters={\n                'object_name': object_name,\n                'approach_distance': approach_distance,\n                'grasp_type': kwargs.get('grasp_type', 'pinch')\n            },\n            estimated_duration=15.0,\n            success_criteria=['object_grasped'],\n            recovery_procedures=['retry_with_different_grasp', 'use_alternative_object']\n        )\n\n        self.planner.execute_subtask(grasp_subtask)\n\n        return {\n            'success': True,\n            'result': f'Attempted to grasp {object_name}',\n            'execution_time': 15.0\n        }\n\n    def communication_tool(self, **kwargs) -> Dict[str, Any]:\n        \"\"\"Tool for communication\"\"\"\n        text = kwargs.get('text', 'Hello')\n        recipient = kwargs.get('recipient', 'everyone')\n\n        # Publish speech\n        speech_pub = self.planner.create_publisher(String, '/tts_input', 10)\n        speech_msg = String()\n        speech_msg.data = text\n        speech_pub.publish(speech_msg)\n\n        return {\n            'success': True,\n            'result': f'Said: \"{text}\" to {recipient}',\n            'execution_time': 2.0\n        }\n\n    def state_query_tool(self, **kwargs) -> Dict[str, Any]:\n        \"\"\"Tool for querying robot state\"\"\"\n        requested_state = kwargs.get('state_property', 'all')\n\n        if requested_state == 'all':\n            return {\n                'success': True,\n                'result': self.planner.robot_state,\n                'execution_time': 0.1\n            }\n        else:\n            value = self.planner.robot_state.get(requested_state)\n            return {\n                'success': True,\n                'result': {requested_state: value},\n                'execution_time': 0.1\n            }\n\n    def use_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"Use a registered tool\"\"\"\n        if tool_name in self.available_tools:\n            self.planner.get_logger().info(f'Using tool: {tool_name} with params: {kwargs}')\n\n            try:\n                result = self.available_tools[tool_name](**kwargs)\n\n                # Log tool usage\n                self.tool_usage_history.append({\n                    'tool': tool_name,\n                    'params': kwargs,\n                    'result': result,\n                    'timestamp': self.planner.get_clock().now().nanoseconds\n                })\n\n                return result\n            except Exception as e:\n                self.planner.get_logger().error(f'Error using tool {tool_name}: {e}')\n                return {\n                    'success': False,\n                    'result': f'Error: {str(e)}',\n                    'execution_time': 0.0\n                }\n        else:\n            return {\n                'success': False,\n                'result': f'Unknown tool: {tool_name}',\n                'execution_time': 0.0\n            }\n\n    def plan_with_tools(self, goal: str) -> Optional[TaskPlan]:\n        \"\"\"Generate a plan that utilizes available tools\"\"\"\n        # Use LLM to determine which tools to use for the goal\n        tool_selection_prompt = f\"\"\"\n        Goal: {goal}\n\n        Available tools:\n        {list(self.available_tools.keys())}\n\n        Robot capabilities and current state:\n        {self.planner.robot_state}\n\n        Known objects and locations:\n        Objects: {list(self.planner.known_objects.keys())}\n        Locations: {list(self.planner.known_locations.keys())}\n\n        Please generate a plan that uses the appropriate tools to achieve the goal.\n        For each step, specify which tool to use and with what parameters.\n        Return the plan in JSON format with tool usage steps.\n        \"\"\"\n\n        try:\n            response = self.planner.llm_client.chat.completions.create(\n                model=\"gpt-4-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a tool-using planning expert for robotics.\"},\n                    {\"role\": \"user\", \"content\": tool_selection_prompt}\n                ],\n                temperature=0.3,\n                response_format={\"type\": \"json_object\"}\n            )\n\n            plan_data = json.loads(response.choices[0].message.content)\n\n            # Convert to TaskPlan format\n            subtasks = []\n            for step in plan_data.get('steps', []):\n                tool_name = step.get('tool')\n                params = step.get('parameters', {})\n\n                subtask = SubTask(\n                    id=f\"tool_{tool_name}_{len(subtasks)}\",\n                    action_type='tool_usage',\n                    parameters={'tool_name': tool_name, 'tool_params': params},\n                    estimated_duration=step.get('estimated_duration', 10.0),\n                    success_criteria=step.get('success_criteria', ['tool_executed']),\n                    recovery_procedures=step.get('recovery_procedures', ['retry_tool'])\n                )\n\n                subtasks.append(subtask)\n\n            plan = TaskPlan(\n                id=f\"tool_based_plan_{len(self.tool_usage_history)}\",\n                description=f\"Tool-based plan for: {goal}\",\n                subtasks=subtasks,\n                dependencies=[],\n                priority=1\n            )\n\n            return plan\n\n        except Exception as e:\n            self.planner.get_logger().error(f'Error in tool-based planning: {e}')\n            return None\n\ndef main_with_tools(args=None):\n    \"\"\"Main function with tool integration\"\"\"\n    rclpy.init(args=args)\n    node = CognitivePlannerNode()\n\n    # Initialize components\n    error_manager = ErrorRecoveryManager(node)\n    tool_agent = ToolUsingAgent(node)\n\n    # Example: Use tools to achieve a complex goal\n    goal = \"Go to the kitchen, find the red cup, grasp it, and bring it to the living room\"\n\n    # Plan using tools\n    tool_based_plan = tool_agent.plan_with_tools(goal)\n    if tool_based_plan:\n        node.get_logger().info(f'Executing tool-based plan with {len(tool_based_plan.subtasks)} steps')\n\n        for subtask in tool_based_plan.subtasks:\n            if 'tool_name' in subtask.parameters:\n                tool_name = subtask.parameters['tool_name']\n                tool_params = subtask.parameters.get('tool_params', {})\n\n                result = tool_agent.use_tool(tool_name, **tool_params)\n                node.get_logger().info(f'Tool result: {result}')\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main_with_tools()\n```\n\n## Practical Exercise\n\n1. Implement LLM-based cognitive planning for a complex household task\n2. Integrate ROS 2 action orchestration with the planning system\n3. Design error recovery mechanisms for common failure modes\n4. Create a tool-using agent that can perform multi-step tasks\n\n## Summary\n\nLLM-based cognitive planning provides high-level reasoning capabilities for humanoid robots, enabling complex task decomposition and execution. The system integrates with ROS 2 action architecture for reliable execution, implements robust error recovery mechanisms, and supports tool-using agents that can perform sophisticated multi-step tasks. This approach allows humanoid robots to interpret natural language commands, generate executable plans, and adapt to changing conditions and failures during execution.",
    "module": "Module 4: Vision-Language-Action (VLA)",
    "chapter": "llm based cognitive planning.md",
    "path": "/docs/module-4/4.3-llm-based-cognitive-planning"
  },
  {
    "id": "module-4/4.4-capstone-project-autonomous-humanoid",
    "title": "4.4 Capstone Project: The Autonomous Humanoid",
    "content": "# 4.4 Capstone Project: The Autonomous Humanoid\n\n## Overview\n\nThis capstone project integrates all the concepts learned throughout the book to create a complete autonomous humanoid robot system. The project involves building a simulated humanoid robot that can receive voice commands, plan paths, navigate through environments with obstacles, identify objects using computer vision, and manipulate those objects. This comprehensive project demonstrates the integration of ROS 2, simulation environments, perception systems, and AI-driven decision making.\n\n## Learning Objectives\n\nBy completing this capstone project, you will be able to:\n- Integrate multiple robotics subsystems into a cohesive autonomous system\n- Implement voice command processing with natural language understanding\n- Design and implement path planning and navigation systems\n- Apply computer vision techniques for object identification and localization\n- Execute manipulation tasks with precise control\n- Validate and test the complete autonomous system\n- Deploy and demonstrate the system in simulation\n\n## Project Architecture\n\nThe autonomous humanoid system consists of several interconnected modules:\n\n```python\nimport rospy\nimport numpy as np\nimport cv2\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan, Imu\nfrom geometry_msgs.msg import PoseStamped, Twist, Point\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom actionlib_msgs.msg import GoalStatus\nfrom cv_bridge import CvBridge\nimport speech_recognition as sr\nimport openai\nimport threading\nimport time\nimport queue\nfrom typing import Dict, List, Tuple, Optional\nimport json\nimport os\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass RobotState(Enum):\n    IDLE = \"idle\"\n    LISTENING = \"listening\"\n    PROCESSING_COMMAND = \"processing_command\"\n    NAVIGATING = \"navigating\"\n    PERCEIVING = \"perceiving\"\n    MANIPULATING = \"manipulating\"\n    EXECUTING_TASK = \"executing_task\"\n    RETURNING_HOME = \"returning_home\"\n    EMERGENCY_STOP = \"emergency_stop\"\n\n@dataclass\nclass TaskPlan:\n    \"\"\"Structured representation of a robot task\"\"\"\n    id: str\n    command: str\n    intent: str\n    target_object: Optional[str]\n    target_location: Optional[str]\n    priority: int\n    status: str = \"pending\"\n    start_time: Optional[float] = None\n    end_time: Optional[float] = None\n\nclass AutonomousHumanoidSystem:\n    \"\"\"\n    Complete autonomous humanoid robot system integrating all subsystems\n    \"\"\"\n    def __init__(self):\n        # Initialize ROS node\n        rospy.init_node('autonomous_humanoid', anonymous=True)\n\n        # Initialize subsystem components\n        self.speech_recognizer = SpeechRecognitionSystem()\n        self.nlu_processor = NaturalLanguageUnderstandingSystem()\n        self.path_planner = PathPlanningSystem()\n        self.navigation_system = NavigationSystem()\n        self.perception_system = PerceptionSystem()\n        self.manipulation_system = ManipulationSystem()\n        self.behavior_manager = BehaviorManager()\n        self.safety_system = SafetySystem()\n\n        # System state\n        self.current_state = RobotState.IDLE\n        self.current_task = None\n        self.task_queue = queue.Queue()\n        self.emergency_stop = False\n\n        # Publishers and subscribers\n        self.status_pub = rospy.Publisher('/humanoid/status', String, queue_size=10)\n        self.command_sub = rospy.Subscriber('/humanoid/command', String, self.command_callback)\n        self.emergency_stop_sub = rospy.Subscriber('/humanoid/emergency_stop', Bool, self.emergency_stop_callback)\n\n        # CV bridge for image processing\n        self.cv_bridge = CvBridge()\n\n        # Initialize systems\n        self.initialize_subsystems()\n\n        rospy.loginfo(\"Autonomous Humanoid System initialized\")\n\n    def initialize_subsystems(self):\n        \"\"\"Initialize all subsystems\"\"\"\n        self.speech_recognizer.initialize()\n        self.nlu_processor.initialize()\n        self.path_planner.initialize()\n        self.navigation_system.initialize()\n        self.perception_system.initialize()\n        self.manipulation_system.initialize()\n        self.behavior_manager.initialize()\n        self.safety_system.initialize()\n\n        rospy.loginfo(\"All subsystems initialized\")\n\n    def command_callback(self, msg: String):\n        \"\"\"Handle incoming commands\"\"\"\n        try:\n            command_data = json.loads(msg.data)\n            command_type = command_data.get('type', 'voice')\n            command_value = command_data.get('value', '')\n\n            if command_type == 'voice':\n                self.process_voice_command(command_value)\n            elif command_type == 'text':\n                self.process_text_command(command_value)\n            elif command_type == 'emergency_stop':\n                self.trigger_emergency_stop()\n        except json.JSONDecodeError:\n            rospy.logerr(f\"Invalid JSON command: {msg.data}\")\n        except Exception as e:\n            rospy.logerr(f\"Error processing command: {e}\")\n\n    def emergency_stop_callback(self, msg: Bool):\n        \"\"\"Handle emergency stop signal\"\"\"\n        if msg.data:\n            self.trigger_emergency_stop()\n        else:\n            self.release_emergency_stop()\n\n    def process_voice_command(self, voice_command: str):\n        \"\"\"Process voice command through the system\"\"\"\n        rospy.loginfo(f\"Processing voice command: {voice_command}\")\n\n        # Update state\n        self.current_state = RobotState.PROCESSING_COMMAND\n        self.publish_status(\"Processing command\")\n\n        # Natural language understanding\n        intent, entities = self.nlu_processor.process_command(voice_command)\n\n        # Create task based on intent\n        task = self.create_task_from_intent(intent, entities)\n        self.task_queue.put(task)\n\n        # Execute task\n        self.execute_current_task()\n\n    def process_text_command(self, text_command: str):\n        \"\"\"Process text command\"\"\"\n        rospy.loginfo(f\"Processing text command: {text_command}\")\n\n        # Similar processing as voice command but without speech recognition\n        intent, entities = self.nlu_processor.process_command(text_command)\n        task = self.create_task_from_intent(intent, entities)\n        self.task_queue.put(task)\n        self.execute_current_task()\n\n    def create_task_from_intent(self, intent: str, entities: Dict) -> TaskPlan:\n        \"\"\"Create a task plan from NLU results\"\"\"\n        task_id = f\"task_{int(time.time())}\"\n\n        task = TaskPlan(\n            id=task_id,\n            command=intent,\n            intent=self.map_intent_to_action(intent),\n            target_object=entities.get('object'),\n            target_location=entities.get('location'),\n            priority=entities.get('priority', 1)\n        )\n\n        return task\n\n    def map_intent_to_action(self, intent: str) -> str:\n        \"\"\"Map natural language intent to robot action\"\"\"\n        intent_mapping = {\n            'navigation': 'navigate_to_location',\n            'object_fetch': 'fetch_object',\n            'object_place': 'place_object',\n            'object_identify': 'identify_object',\n            'greeting': 'greet_user',\n            'help': 'provide_assistance',\n            'clean': 'clean_area',\n            'follow': 'follow_user',\n            'stop': 'stop_robot'\n        }\n\n        return intent_mapping.get(intent.lower(), 'unknown')\n\n    def execute_current_task(self):\n        \"\"\"Execute the current task in the queue\"\"\"\n        if not self.task_queue.empty():\n            task = self.task_queue.get()\n            self.current_task = task\n            task.start_time = time.time()\n\n            rospy.loginfo(f\"Executing task: {task.intent} for object: {task.target_object} at location: {task.target_location}\")\n\n            try:\n                if task.intent == 'navigate_to_location':\n                    self.execute_navigation_task(task)\n                elif task.intent == 'fetch_object':\n                    self.execute_fetch_object_task(task)\n                elif task.intent == 'place_object':\n                    self.execute_place_object_task(task)\n                elif task.intent == 'identify_object':\n                    self.execute_identify_object_task(task)\n                elif task.intent == 'greet_user':\n                    self.execute_greet_user_task(task)\n                elif task.intent == 'provide_assistance':\n                    self.execute_assistance_task(task)\n                elif task.intent == 'clean_area':\n                    self.execute_clean_task(task)\n                elif task.intent == 'follow_user':\n                    self.execute_follow_task(task)\n                elif task.intent == 'stop_robot':\n                    self.execute_stop_task(task)\n                else:\n                    rospy.logwarn(f\"Unknown task intent: {task.intent}\")\n                    self.current_state = RobotState.IDLE\n\n            except Exception as e:\n                rospy.logerr(f\"Error executing task {task.id}: {e}\")\n                self.current_state = RobotState.IDLE\n                task.status = \"failed\"\n            finally:\n                task.end_time = time.time()\n                task.status = \"completed\" if self.current_state != RobotState.EMERGENCY_STOP else \"interrupted\"\n\n    def execute_navigation_task(self, task: TaskPlan):\n        \"\"\"Execute navigation task\"\"\"\n        if not task.target_location:\n            rospy.logerr(\"Navigation task requires target location\")\n            return\n\n        self.current_state = RobotState.NAVIGATING\n        self.publish_status(f\"Navigating to {task.target_location}\")\n\n        # Plan path to target location\n        target_pose = self.path_planner.get_location_pose(task.target_location)\n        if target_pose:\n            success = self.navigation_system.navigate_to_pose(target_pose)\n            if success:\n                rospy.loginfo(f\"Successfully navigated to {task.target_location}\")\n            else:\n                rospy.logerr(f\"Failed to navigate to {task.target_location}\")\n        else:\n            rospy.logerr(f\"Unknown location: {task.target_location}\")\n\n        self.current_state = RobotState.IDLE\n\n    def execute_fetch_object_task(self, task: TaskPlan):\n        \"\"\"Execute object fetch task\"\"\"\n        if not task.target_object:\n            rospy.logerr(\"Fetch task requires target object\")\n            return\n\n        self.current_state = RobotState.PERCEIVING\n        self.publish_status(f\"Looking for {task.target_object}\")\n\n        # Detect object in environment\n        object_pose = self.perception_system.detect_object(task.target_object)\n        if object_pose:\n            # Navigate to object\n            self.current_state = RobotState.NAVIGATING\n            self.publish_status(f\"Navigating to {task.target_object}\")\n\n            object_approach_pose = self.calculate_approach_pose(object_pose)\n            success = self.navigation_system.navigate_to_pose(object_approach_pose)\n            if success:\n                # Manipulate object\n                self.current_state = RobotState.MANIPULATING\n                self.publish_status(f\"Manipulating {task.target_object}\")\n\n                manipulation_success = self.manipulation_system.grasp_object(object_pose)\n                if manipulation_success:\n                    rospy.loginfo(f\"Successfully grasped {task.target_object}\")\n\n                    # If target location specified, navigate there to place\n                    if task.target_location:\n                        target_pose = self.path_planner.get_location_pose(task.target_location)\n                        if target_pose:\n                            self.navigation_system.navigate_to_pose(target_pose)\n                            self.manipulation_system.place_object(target_pose)\n                else:\n                    rospy.logerr(f\"Failed to grasp {task.target_object}\")\n            else:\n                rospy.logerr(f\"Failed to navigate to {task.target_object}\")\n        else:\n            rospy.logerr(f\"Could not find {task.target_object}\")\n\n        self.current_state = RobotState.IDLE\n\n    def execute_place_object_task(self, task: TaskPlan):\n        \"\"\"Execute object placement task\"\"\"\n        if not task.target_location:\n            rospy.logerr(\"Place task requires target location\")\n            return\n\n        target_pose = self.path_planner.get_location_pose(task.target_location)\n        if target_pose:\n            # Navigate to placement location\n            self.current_state = RobotState.NAVIGATING\n            self.publish_status(f\"Navigating to place object at {task.target_location}\")\n\n            success = self.navigation_system.navigate_to_pose(target_pose)\n            if success:\n                # Place object\n                self.current_state = RobotState.MANIPULATING\n                self.publish_status(\"Placing object\")\n\n                placement_success = self.manipulation_system.place_object(target_pose)\n                if placement_success:\n                    rospy.loginfo(f\"Successfully placed object at {task.target_location}\")\n                else:\n                    rospy.logerr(f\"Failed to place object at {task.target_location}\")\n            else:\n                rospy.logerr(f\"Failed to navigate to placement location\")\n        else:\n            rospy.logerr(f\"Unknown placement location: {task.target_location}\")\n\n        self.current_state = RobotState.IDLE\n\n    def execute_identify_object_task(self, task: TaskPlan):\n        \"\"\"Execute object identification task\"\"\"\n        self.current_state = RobotState.PERCEIVING\n        self.publish_status(\"Identifying objects in environment\")\n\n        # Detect and identify objects\n        objects = self.perception_system.identify_objects_in_view()\n        identified_objects = []\n\n        for obj in objects:\n            if not task.target_object or task.target_object.lower() in obj.name.lower():\n                identified_objects.append({\n                    'name': obj.name,\n                    'pose': obj.pose,\n                    'confidence': obj.confidence\n                })\n\n        # Report findings\n        if identified_objects:\n            rospy.loginfo(f\"Found objects: {[obj['name'] for obj in identified_objects]}\")\n            # In real implementation, this would be reported back to the user\n        else:\n            rospy.loginfo(\"No objects found matching criteria\")\n\n        self.current_state = RobotState.IDLE\n\n    def execute_greet_user_task(self, task: TaskPlan):\n        \"\"\"Execute user greeting task\"\"\"\n        self.current_state = RobotState.EXECUTING_TASK\n        self.publish_status(\"Greeting user\")\n\n        # Perform greeting behavior\n        self.behavior_manager.perform_greeting()\n        rospy.loginfo(\"Greeting completed\")\n\n        self.current_state = RobotState.IDLE\n\n    def execute_assistance_task(self, task: TaskPlan):\n        \"\"\"Execute assistance task\"\"\"\n        self.current_state = RobotState.EXECUTING_TASK\n        self.publish_status(\"Providing assistance\")\n\n        # Perform assistance behavior\n        self.behavior_manager.perform_assistance()\n        rospy.loginfo(\"Assistance provided\")\n\n        self.current_state = RobotState.IDLE\n\n    def execute_clean_task(self, task: TaskPlan):\n        \"\"\"Execute cleaning task\"\"\"\n        self.current_state = RobotState.EXECUTING_TASK\n        self.publish_status(\"Performing cleaning task\")\n\n        # Navigate to cleaning area\n        cleaning_area = task.target_location or \"current_area\"\n        target_pose = self.path_planner.get_location_pose(cleaning_area)\n\n        if target_pose:\n            success = self.navigation_system.navigate_to_pose(target_pose)\n            if success:\n                # Perform cleaning behavior\n                self.behavior_manager.perform_cleaning()\n                rospy.loginfo(f\"Cleaning completed in {cleaning_area}\")\n            else:\n                rospy.logerr(f\"Failed to navigate to cleaning area\")\n        else:\n            rospy.logerr(f\"Unknown cleaning area: {cleaning_area}\")\n\n        self.current_state = RobotState.IDLE\n\n    def execute_follow_task(self, task: TaskPlan):\n        \"\"\"Execute follow user task\"\"\"\n        self.current_state = RobotState.NAVIGATING\n        self.publish_status(\"Following user\")\n\n        # Start following behavior\n        self.navigation_system.start_following()\n        rospy.loginfo(\"Started following user\")\n\n        # This would typically run until stopped by another command\n        # For simulation, we'll just follow for a set time\n        rospy.sleep(10.0)\n\n        self.navigation_system.stop_following()\n        self.current_state = RobotState.IDLE\n\n    def execute_stop_task(self, task: TaskPlan):\n        \"\"\"Execute stop task\"\"\"\n        self.current_state = RobotState.IDLE\n        self.publish_status(\"Stopped\")\n        rospy.loginfo(\"Robot stopped\")\n\n    def calculate_approach_pose(self, object_pose: PoseStamped) -> PoseStamped:\n        \"\"\"Calculate approach pose for object manipulation\"\"\"\n        approach_pose = PoseStamped()\n        approach_pose.header = object_pose.header\n\n        # Approach from front of object\n        approach_pose.pose.position.x = object_pose.pose.position.x - 0.5  # 50cm in front\n        approach_pose.pose.position.y = object_pose.pose.position.y\n        approach_pose.pose.position.z = object_pose.pose.position.z  # Same height\n\n        # Orient toward object\n        approach_pose.pose.orientation.w = 1.0  # Facing object\n\n        return approach_pose\n\n    def trigger_emergency_stop(self):\n        \"\"\"Trigger emergency stop\"\"\"\n        self.emergency_stop = True\n        self.current_state = RobotState.EMERGENCY_STOP\n        self.publish_status(\"EMERGENCY STOP\")\n        rospy.logerr(\"Emergency stop triggered!\")\n\n    def release_emergency_stop(self):\n        \"\"\"Release emergency stop\"\"\"\n        self.emergency_stop = False\n        self.current_state = RobotState.IDLE\n        self.publish_status(\"Emergency stop released\")\n\n    def publish_status(self, status: str):\n        \"\"\"Publish robot status\"\"\"\n        status_msg = String()\n        status_msg.data = f\"STATE: {self.current_state.value} | STATUS: {status}\"\n        self.status_pub.publish(status_msg)\n\n    def run_system(self):\n        \"\"\"Main system execution loop\"\"\"\n        rate = rospy.Rate(10)  # 10 Hz\n\n        while not rospy.is_shutdown():\n            if self.emergency_stop:\n                rospy.sleep(0.1)  # Emergency stop active, just sleep\n                continue\n\n            # Check for safety\n            if not self.safety_system.check_safety():\n                self.trigger_emergency_stop()\n                continue\n\n            # Process tasks if system is idle\n            if self.current_state == RobotState.IDLE and not self.task_queue.empty():\n                self.execute_current_task()\n\n            # Continue current task if applicable\n            if self.current_state == RobotState.NAVIGATING:\n                # Monitor navigation progress\n                nav_status = self.navigation_system.get_status()\n                if nav_status == 'completed':\n                    self.current_state = RobotState.IDLE\n                elif nav_status == 'failed':\n                    rospy.logerr(\"Navigation failed\")\n                    self.current_state = RobotState.IDLE\n\n            # Publish current status\n            self.publish_status(f\"Operating in {self.current_state.value} state\")\n\n            rate.sleep()\n\nclass SpeechRecognitionSystem:\n    \"\"\"Handles voice command recognition\"\"\"\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.is_listening = False\n        self.command_queue = queue.Queue()\n\n    def initialize(self):\n        \"\"\"Initialize speech recognition system\"\"\"\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n        rospy.loginfo(\"Speech recognition system initialized\")\n\n    def start_listening(self):\n        \"\"\"Start continuous listening for commands\"\"\"\n        self.is_listening = True\n        listening_thread = threading.Thread(target=self._continuous_listening, daemon=True)\n        listening_thread.start()\n\n    def _continuous_listening(self):\n        \"\"\"Continuous listening loop\"\"\"\n        while self.is_listening:\n            try:\n                with self.microphone as source:\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\n\n                # Recognize speech\n                text = self.recognizer.recognize_google(audio)\n                rospy.loginfo(f\"Recognized: {text}\")\n\n                # Add to command queue\n                command_msg = String()\n                command_msg.data = json.dumps({\n                    'type': 'voice',\n                    'value': text\n                })\n                self.command_queue.put(command_msg)\n\n            except sr.WaitTimeoutError:\n                continue\n            except sr.UnknownValueError:\n                rospy.loginfo(\"Could not understand audio\")\n            except sr.RequestError as e:\n                rospy.logerr(f\"Speech recognition error: {e}\")\n            except Exception as e:\n                rospy.logerr(f\"Error in speech recognition: {e}\")\n\n    def stop_listening(self):\n        \"\"\"Stop listening\"\"\"\n        self.is_listening = False\n\nclass NaturalLanguageUnderstandingSystem:\n    \"\"\"Processes natural language commands\"\"\"\n    def __init__(self):\n        self.intent_classifier = None\n        self.entity_extractor = None\n        self.command_templates = {\n            'go_to': [\n                'go to {location}',\n                'move to {location}',\n                'navigate to {location}',\n                'walk to {location}'\n            ],\n            'fetch': [\n                'fetch {object}',\n                'get {object}',\n                'bring me {object}',\n                'pick up {object}',\n                'grab {object}'\n            ],\n            'place': [\n                'place {object} at {location}',\n                'put {object} at {location}',\n                'drop {object} at {location}'\n            ],\n            'identify': [\n                'find {object}',\n                'locate {object}',\n                'where is {object}',\n                'show me {object}'\n            ],\n            'clean': [\n                'clean {location}',\n                'tidy up {location}',\n                'organize {location}'\n            ],\n            'follow': [\n                'follow me',\n                'come with me',\n                'stay with me'\n            ],\n            'greet': [\n                'hello',\n                'hi',\n                'greetings',\n                'good morning',\n                'good afternoon'\n            ],\n            'help': [\n                'help',\n                'assist me',\n                'what can you do',\n                'how can you help'\n            ],\n            'stop': [\n                'stop',\n                'halt',\n                'pause',\n                'freeze'\n            ]\n        }\n\n    def initialize(self):\n        \"\"\"Initialize NLU system\"\"\"\n        # In practice, this would load ML models\n        rospy.loginfo(\"NLU system initialized\")\n\n    def process_command(self, command: str) -> Tuple[str, Dict]:\n        \"\"\"Process natural language command and extract intent and entities\"\"\"\n        command_lower = command.lower()\n\n        # Simple rule-based intent classification (in practice, use ML models)\n        intent = self._classify_intent(command_lower)\n        entities = self._extract_entities(command_lower, intent)\n\n        return intent, entities\n\n    def _classify_intent(self, command: str) -> str:\n        \"\"\"Classify intent based on command text\"\"\"\n        # Check for each intent type\n        for intent, templates in self.command_templates.items():\n            for template in templates:\n                # Simple keyword matching\n                if any(keyword in command for keyword in template.split()):\n                    return intent\n\n        # More sophisticated pattern matching\n        if any(word in command for word in ['go', 'move', 'navigate', 'walk', 'to']):\n            if any(loc in command for loc in ['kitchen', 'bedroom', 'office', 'living room', 'bathroom']):\n                return 'navigation'\n\n        if any(word in command for word in ['get', 'fetch', 'bring', 'pick', 'grab']):\n            return 'object_fetch'\n\n        if any(word in command for word in ['place', 'put', 'drop']):\n            return 'object_place'\n\n        if any(word in command for word in ['find', 'locate', 'where', 'show']):\n            return 'object_identify'\n\n        if any(word in command for word in ['clean', 'tidy', 'organize']):\n            return 'clean'\n\n        if any(word in command for word in ['follow', 'come', 'stay']):\n            return 'follow'\n\n        if any(word in command for word in ['hello', 'hi', 'greet', 'morning', 'afternoon']):\n            return 'greeting'\n\n        if any(word in command for word in ['help', 'assist']):\n            return 'help'\n\n        if any(word in command for word in ['stop', 'halt', 'pause']):\n            return 'stop'\n\n        return 'unknown'\n\n    def _extract_entities(self, command: str, intent: str) -> Dict:\n        \"\"\"Extract entities (objects, locations, etc.) from command\"\"\"\n        entities = {}\n\n        # Extract common objects\n        objects = ['cup', 'bottle', 'book', 'phone', 'box', 'ball', 'toy', 'plate', 'glass']\n        for obj in objects:\n            if obj in command:\n                entities['object'] = obj\n                break\n\n        # Extract common locations\n        locations = ['kitchen', 'bedroom', 'office', 'living room', 'bathroom', 'dining room', 'hallway', 'garden']\n        for loc in locations:\n            if loc in command:\n                entities['location'] = loc\n                break\n\n        # Extract other entities based on intent\n        if intent == 'greeting':\n            # Extract greeting type\n            if 'morning' in command:\n                entities['greeting_type'] = 'morning'\n            elif 'afternoon' in command:\n                entities['greeting_type'] = 'afternoon'\n            elif 'evening' in command:\n                entities['greeting_type'] = 'evening'\n            else:\n                entities['greeting_type'] = 'general'\n\n        # Extract priority if specified\n        if any(word in command for word in ['urgent', 'quickly', 'fast', 'immediately']):\n            entities['priority'] = 2\n        elif any(word in command for word in ['slowly', 'carefully', 'gently']):\n            entities['priority'] = 0\n        else:\n            entities['priority'] = 1\n\n        return entities\n\nclass PathPlanningSystem:\n    \"\"\"Handles path planning for navigation\"\"\"\n    def __init__(self):\n        self.map_data = None\n        self.locations = {}\n        self.planner = None\n\n    def initialize(self):\n        \"\"\"Initialize path planning system\"\"\"\n        # Load map and predefined locations\n        self._load_map_data()\n        self._load_predefined_locations()\n        rospy.loginfo(\"Path planning system initialized\")\n\n    def _load_map_data(self):\n        \"\"\"Load map data (in practice, from ROS map server)\"\"\"\n        # This would normally connect to ROS map server\n        # For simulation, we'll create a simple map representation\n        self.map_data = {\n            'resolution': 0.05,  # meters per pixel\n            'origin': [-10, -10, 0],  # [x, y, theta] in map frame\n            'size': [400, 400]  # width, height in pixels\n        }\n\n    def _load_predefined_locations(self):\n        \"\"\"Load predefined location poses\"\"\"\n        self.locations = {\n            'kitchen': PoseStamped(\n                header=rospy.Header(frame_id='map'),\n                pose=Pose(\n                    position=Point(x=2.0, y=1.0, z=0.0),\n                    orientation=Quaternion(w=1.0, x=0.0, y=0.0, z=0.0)\n                )\n            ),\n            'bedroom': PoseStamped(\n                header=rospy.Header(frame_id='map'),\n                pose=Pose(\n                    position=Point(x=-2.0, y=1.5, z=0.0),\n                    orientation=Quaternion(w=1.0, x=0.0, y=0.0, z=0.0)\n                )\n            ),\n            'office': PoseStamped(\n                header=rospy.Header(frame_id='map'),\n                pose=Pose(\n                    position=Point(x=0.0, y=-2.0, z=0.0),\n                    orientation=Quaternion(w=1.0, x=0.0, y=0.0, z=0.0)\n                )\n            ),\n            'living_room': PoseStamped(\n                header=rospy.Header(frame_id='map'),\n                pose=Pose(\n                    position=Point(x=0.0, y=0.0, z=0.0),\n                    orientation=Quaternion(w=1.0, x=0.0, y=0.0, z=0.0)\n                )\n            ),\n            'bathroom': PoseStamped(\n                header=rospy.Header(frame_id='map'),\n                pose=Pose(\n                    position=Point(x=1.5, y=-1.0, z=0.0),\n                    orientation=Quaternion(w=1.0, x=0.0, y=0.0, z=0.0)\n                )\n            )\n        }\n\n    def get_location_pose(self, location_name: str) -> Optional[PoseStamped]:\n        \"\"\"Get pose for a predefined location\"\"\"\n        return self.locations.get(location_name.lower())\n\n    def plan_path(self, start_pose: PoseStamped, goal_pose: PoseStamped) -> List[PoseStamped]:\n        \"\"\"Plan path from start to goal\"\"\"\n        # In practice, this would use A*, RRT*, or other path planning algorithms\n        # For simulation, we'll create a simple straight-line path with waypoints\n\n        path = []\n        steps = 20  # Number of waypoints\n\n        start_pos = start_pose.pose.position\n        goal_pos = goal_pose.pose.position\n\n        for i in range(steps + 1):\n            t = i / steps\n            x = start_pos.x + t * (goal_pos.x - start_pos.x)\n            y = start_pos.y + t * (goal_pos.y - start_pos.y)\n            z = start_pos.z + t * (goal_pos.z - start_pos.z)\n\n            waypoint = PoseStamped()\n            waypoint.header = goal_pose.header\n            waypoint.pose.position = Point(x=x, y=y, z=z)\n            waypoint.pose.orientation = goal_pose.pose.orientation  # Keep same orientation\n\n            path.append(waypoint)\n\n        return path\n\n    def validate_path(self, path: List[PoseStamped]) -> bool:\n        \"\"\"Validate path for obstacles and feasibility\"\"\"\n        # Check path validity against map data\n        # This would normally check against costmap\n        return len(path) > 0\n\nclass NavigationSystem:\n    \"\"\"Handles robot navigation\"\"\"\n    def __init__(self):\n        self.move_base_client = None\n        self.current_goal = None\n        self.navigation_active = False\n\n    def initialize(self):\n        \"\"\"Initialize navigation system\"\"\"\n        # Initialize move_base action client\n        self.move_base_client = actionlib.SimpleActionClient('move_base', MoveBaseAction)\n        self.move_base_client.wait_for_server(rospy.Duration(10.0))\n        rospy.loginfo(\"Navigation system initialized\")\n\n    def navigate_to_pose(self, pose: PoseStamped) -> bool:\n        \"\"\"Navigate to specified pose\"\"\"\n        goal = MoveBaseGoal()\n        goal.target_pose = pose\n\n        # Send goal to move_base\n        self.move_base_client.send_goal(goal)\n        self.navigation_active = True\n\n        # Wait for result with timeout\n        finished_within_time = self.move_base_client.wait_for_result(rospy.Duration(60.0))\n\n        if not finished_within_time:\n            rospy.logerr(\"Navigation took too long, cancelling goal\")\n            self.move_base_client.cancel_goal()\n            self.navigation_active = False\n            return False\n\n        # Check result\n        state = self.move_base_client.get_state()\n        result = self.move_base_client.get_result()\n\n        if state == GoalStatus.SUCCEEDED:\n            rospy.loginfo(\"Navigation succeeded\")\n            self.navigation_active = False\n            return True\n        else:\n            rospy.logerr(f\"Navigation failed with state: {state}\")\n            self.navigation_active = False\n            return False\n\n    def get_status(self) -> str:\n        \"\"\"Get current navigation status\"\"\"\n        if not self.navigation_active:\n            return 'idle'\n\n        state = self.move_base_client.get_state()\n        if state == GoalStatus.ACTIVE:\n            return 'active'\n        elif state == GoalStatus.SUCCEEDED:\n            return 'completed'\n        else:\n            return 'failed'\n\n    def start_following(self):\n        \"\"\"Start following behavior\"\"\"\n        # This would implement person following\n        # For now, we'll just set a flag\n        self.following_active = True\n\n    def stop_following(self):\n        \"\"\"Stop following behavior\"\"\"\n        self.following_active = False\n        if self.navigation_active:\n            self.move_base_client.cancel_goal()\n            self.navigation_active = False\n\nclass PerceptionSystem:\n    \"\"\"Handles object detection and perception\"\"\"\n    def __init__(self):\n        self.camera_sub = None\n        self.lidar_sub = None\n        self.detector = None\n        self.latest_image = None\n        self.image_lock = threading.Lock()\n\n    def initialize(self):\n        \"\"\"Initialize perception system\"\"\"\n        self.camera_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.image_callback)\n        self.lidar_sub = rospy.Subscriber('/scan', LaserScan, self.lidar_callback)\n        rospy.loginfo(\"Perception system initialized\")\n\n    def image_callback(self, msg: Image):\n        \"\"\"Process incoming camera images\"\"\"\n        try:\n            with self.image_lock:\n                self.latest_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\n        except Exception as e:\n            rospy.logerr(f\"Error processing image: {e}\")\n\n    def lidar_callback(self, msg: LaserScan):\n        \"\"\"Process incoming LIDAR data\"\"\"\n        # Process LIDAR data for obstacle detection\n        # This would normally integrate with navigation system\n        pass\n\n    def detect_object(self, object_name: str) -> Optional[PoseStamped]:\n        \"\"\"Detect a specific object in the environment\"\"\"\n        with self.image_lock:\n            if self.latest_image is None:\n                return None\n\n            # Process image to detect object\n            # In practice, this would use a trained object detection model\n            # For simulation, we'll use a simple approach\n            detected_objects = self._simulate_object_detection(self.latest_image)\n\n            # Find the requested object\n            for obj in detected_objects:\n                if obj['name'].lower() == object_name.lower():\n                    return obj['pose']\n\n        return None\n\n    def identify_objects_in_view(self) -> List[Dict]:\n        \"\"\"Identify all objects currently in view\"\"\"\n        with self.image_lock:\n            if self.latest_image is None:\n                return []\n\n            detected_objects = self._simulate_object_detection(self.latest_image)\n            return detected_objects\n\n    def _simulate_object_detection(self, image) -> List[Dict]:\n        \"\"\"Simulate object detection (in practice, use ML models)\"\"\"\n        # This is a simulation - in real implementation, use YOLO, SSD, or other detectors\n        detected_objects = []\n\n        # Simulate detection of common objects\n        height, width = image.shape[:2]\n\n        # Create some simulated detections\n        simulated_objects = [\n            {'name': 'cup', 'bbox': [width//2 - 50, height//2 - 50, width//2 + 50, height//2 + 50], 'confidence': 0.85},\n            {'name': 'book', 'bbox': [width//4, height//3, width//4 + 80, height//3 + 100], 'confidence': 0.78},\n            {'name': 'phone', 'bbox': [3*width//4, 2*height//3, 3*width//4 + 40, 2*height//3 + 70], 'confidence': 0.92}\n        ]\n\n        for obj in simulated_objects:\n            # Convert bounding box to 3D pose (simplified)\n            bbox = obj['bbox']\n            center_x = (bbox[0] + bbox[2]) / 2\n            center_y = (bbox[1] + bbox[3]) / 2\n\n            # Convert pixel coordinates to world coordinates (simplified)\n            # This would normally use camera intrinsics and depth information\n            world_x = (center_x - width/2) * 0.001  # Simplified conversion\n            world_y = (center_y - height/2) * 0.001\n            world_z = 0.8  # Assume object is at robot's height\n\n            pose = PoseStamped()\n            pose.header.frame_id = 'camera_rgb_optical_frame'\n            pose.header.stamp = rospy.Time.now()\n            pose.pose.position = Point(x=world_x, y=world_y, z=world_z)\n            pose.pose.orientation.w = 1.0\n\n            detected_objects.append({\n                'name': obj['name'],\n                'pose': pose,\n                'confidence': obj['confidence'],\n                'bbox': obj['bbox']\n            })\n\n        return detected_objects\n\nclass ManipulationSystem:\n    \"\"\"Handles robot manipulation tasks\"\"\"\n    def __init__(self):\n        self.arm_client = None\n        self.gripper_client = None\n        self.manipulation_active = False\n\n    def initialize(self):\n        \"\"\"Initialize manipulation system\"\"\"\n        # Initialize action clients for arm and gripper\n        # These would connect to the specific robot's action servers\n        # For simulation, we'll create placeholder clients\n        rospy.loginfo(\"Manipulation system initialized\")\n\n    def grasp_object(self, object_pose: PoseStamped) -> bool:\n        \"\"\"Grasp an object at the specified pose\"\"\"\n        if self.manipulation_active:\n            rospy.logwarn(\"Manipulation already active, cannot start new task\")\n            return False\n\n        self.manipulation_active = True\n        rospy.loginfo(f\"Attempting to grasp object at {object_pose.pose.position}\")\n\n        try:\n            # In practice, this would:\n            # 1. Plan arm trajectory to approach object\n            # 2. Execute approach motion\n            # 3. Close gripper\n            # 4. Lift object slightly\n\n            # For simulation, we'll just simulate the process\n            rospy.sleep(2.0)  # Simulate approach time\n            rospy.loginfo(\"Gripper closed\")\n            rospy.sleep(0.5)  # Simulate grip time\n            rospy.loginfo(\"Object lifted\")\n\n            result = True  # Simulated success\n        except Exception as e:\n            rospy.logerr(f\"Error during grasping: {e}\")\n            result = False\n        finally:\n            self.manipulation_active = False\n\n        return result\n\n    def place_object(self, target_pose: PoseStamped) -> bool:\n        \"\"\"Place held object at target pose\"\"\"\n        if self.manipulation_active:\n            rospy.logwarn(\"Manipulation already active, cannot start new task\")\n            return False\n\n        self.manipulation_active = True\n        rospy.loginfo(f\"Attempting to place object at {target_pose.pose.position}\")\n\n        try:\n            # In practice, this would:\n            # 1. Plan arm trajectory to target\n            # 2. Execute motion to target\n            # 3. Open gripper\n            # 4. Retract arm\n\n            # For simulation, we'll just simulate the process\n            rospy.sleep(2.0)  # Simulate movement time\n            rospy.loginfo(\"Gripper opened\")\n            rospy.sleep(0.5)  # Simulate release time\n            rospy.loginfo(\"Arm retracted\")\n\n            result = True  # Simulated success\n        except Exception as e:\n            rospy.logerr(f\"Error during placing: {e}\")\n            result = False\n        finally:\n            self.manipulation_active = False\n\n        return result\n\n    def move_to_pose(self, pose: PoseStamped) -> bool:\n        \"\"\"Move manipulator to specific pose\"\"\"\n        # Implementation would use inverse kinematics and trajectory planning\n        rospy.loginfo(f\"Moving manipulator to pose: {pose.pose.position}\")\n        return True  # Simulated success\n\nclass BehaviorManager:\n    \"\"\"Manages robot behaviors and social interactions\"\"\"\n    def __init__(self):\n        self.current_behavior = None\n        self.behavior_tree = None\n\n    def initialize(self):\n        \"\"\"Initialize behavior management system\"\"\"\n        # Initialize behavior tree or state machine\n        rospy.loginfo(\"Behavior management system initialized\")\n\n    def perform_greeting(self):\n        \"\"\"Perform greeting behavior\"\"\"\n        rospy.loginfo(\"Performing greeting behavior\")\n        # This would include:\n        # - Head movement to look at user\n        # - Arm gesture (wave)\n        # - Speech synthesis (\"Hello! How can I help you?\")\n        # - Facial expression (smile if applicable)\n\n        # For simulation, just log the behavior\n        rospy.loginfo(\"Greeting behavior completed\")\n\n    def perform_assistance(self):\n        \"\"\"Perform assistance behavior\"\"\"\n        rospy.loginfo(\"Performing assistance behavior\")\n        # This would include:\n        # - Active listening posture\n        # - Eye contact maintenance\n        # - Verbal acknowledgment\n        # - Ready-to-act positioning\n\n        rospy.loginfo(\"Assistance behavior completed\")\n\n    def perform_cleaning(self):\n        \"\"\"Perform cleaning behavior\"\"\"\n        rospy.loginfo(\"Performing cleaning behavior\")\n        # This would include:\n        # - Navigation to cleaning area\n        # - Systematic movement pattern\n        # - Object detection and avoidance\n        # - Manipulation of cleaning tools (if equipped)\n\n        rospy.loginfo(\"Cleaning behavior completed\")\n\n    def set_behavior(self, behavior_name: str):\n        \"\"\"Set current robot behavior\"\"\"\n        self.current_behavior = behavior_name\n        rospy.loginfo(f\"Behavior set to: {behavior_name}\")\n\nclass SafetySystem:\n    \"\"\"Monitors safety conditions and handles emergencies\"\"\"\n    def __init__(self):\n        self.laser_sub = None\n        self.imu_sub = None\n        self.safety_thresholds = {\n            'proximity': 0.5,  # meters\n            'tilt_angle': 30.0,  # degrees\n            'collision_force': 50.0  # Newtons\n        }\n        self.safety_violations = []\n\n    def initialize(self):\n        \"\"\"Initialize safety system\"\"\"\n        self.laser_sub = rospy.Subscriber('/scan', LaserScan, self.laser_callback)\n        self.imu_sub = rospy.Subscriber('/imu/data', Imu, self.imu_callback)\n        rospy.loginfo(\"Safety system initialized\")\n\n    def laser_callback(self, msg: LaserScan):\n        \"\"\"Process laser scan for proximity safety\"\"\"\n        # Check for obstacles in safety zone\n        min_distance = min([r for r in msg.ranges if r > msg.range_min and r < msg.range_max], default=float('inf'))\n\n        if min_distance < self.safety_thresholds['proximity']:\n            rospy.logwarn(f\"Safety violation: Obstacle at {min_distance:.2f}m (threshold: {self.safety_thresholds['proximity']}m)\")\n            self.safety_violations.append({\n                'type': 'proximity',\n                'distance': min_distance,\n                'timestamp': rospy.Time.now()\n            })\n\n    def imu_callback(self, msg: Imu):\n        \"\"\"Process IMU data for tilt safety\"\"\"\n        # Convert quaternion to Euler angles to check tilt\n        orientation = msg.orientation\n        w, x, y, z = orientation.w, orientation.x, orientation.y, orientation.z\n\n        # Calculate roll and pitch\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (w * y - z * x)\n        pitch = math.asin(sinp)\n\n        # Check tilt thresholds\n        tilt_angle_deg = math.degrees(max(abs(roll), abs(pitch)))\n        if tilt_angle_deg > self.safety_thresholds['tilt_angle']:\n            rospy.logwarn(f\"Safety violation: Tilt angle {tilt_angle_deg:.2f}° (threshold: {self.safety_thresholds['tilt_angle']}°)\")\n            self.safety_violations.append({\n                'type': 'tilt',\n                'angle': tilt_angle_deg,\n                'timestamp': rospy.Time.now()\n            })\n\n    def check_safety(self) -> bool:\n        \"\"\"Check if current conditions are safe\"\"\"\n        # In practice, this would check multiple safety conditions\n        # For now, just return True if no recent violations\n        recent_violations = [\n            v for v in self.safety_violations\n            if (rospy.Time.now() - v['timestamp']).to_sec() < 5.0  # Last 5 seconds\n        ]\n\n        return len(recent_violations) == 0\n\n    def get_safety_status(self) -> Dict:\n        \"\"\"Get detailed safety status\"\"\"\n        return {\n            'is_safe': self.check_safety(),\n            'recent_violations': len([\n                v for v in self.safety_violations\n                if (rospy.Time.now() - v['timestamp']).to_sec() < 10.0\n            ]),\n            'last_violation': self.safety_violations[-1] if self.safety_violations else None\n        }\n\ndef main():\n    \"\"\"Main function to run the autonomous humanoid system\"\"\"\n    rospy.loginfo(\"Starting Autonomous Humanoid System...\")\n\n    try:\n        # Initialize the complete system\n        robot_system = AutonomousHumanoidSystem()\n\n        # Start speech recognition\n        robot_system.speech_recognizer.start_listening()\n\n        # Run the main system loop\n        robot_system.run_system()\n\n    except rospy.ROSInterruptException:\n        rospy.loginfo(\"Autonomous Humanoid System shutting down\")\n    except Exception as e:\n        rospy.logerr(f\"Error in Autonomous Humanoid System: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == '__main__':\n    main()\n```\n\n## System Integration and Testing\n\n### Integration Architecture\n\n```python\nclass SystemIntegrationTester:\n    \"\"\"\n    Comprehensive testing framework for the integrated autonomous humanoid system\n    \"\"\"\n    def __init__(self, robot_system: AutonomousHumanoidSystem):\n        self.robot_system = robot_system\n        self.test_results = []\n        self.performance_metrics = {}\n\n    def run_integration_tests(self) -> Dict:\n        \"\"\"Run comprehensive integration tests\"\"\"\n        rospy.loginfo(\"Starting system integration tests...\")\n\n        test_suite = [\n            self.test_speech_recognition_integration,\n            self.test_nlu_integration,\n            self.test_path_planning_integration,\n            self.test_navigation_integration,\n            self.test_perception_integration,\n            self.test_manipulation_integration,\n            self.test_safety_integration,\n            self.test_end_to_end_workflow\n        ]\n\n        for test_func in test_suite:\n            try:\n                result = test_func()\n                self.test_results.append({\n                    'test_name': test_func.__name__,\n                    'passed': result['success'],\n                    'details': result.get('details', ''),\n                    'execution_time': result.get('execution_time', 0)\n                })\n                rospy.loginfo(f\"Test {test_func.__name__}: {'PASSED' if result['success'] else 'FAILED'}\")\n            except Exception as e:\n                self.test_results.append({\n                    'test_name': test_func.__name__,\n                    'passed': False,\n                    'details': f\"Exception: {str(e)}\",\n                    'execution_time': 0\n                })\n                rospy.logerr(f\"Test {test_func.__name__} failed with exception: {e}\")\n\n        # Calculate overall results\n        total_tests = len(self.test_results)\n        passed_tests = sum(1 for result in self.test_results if result['passed'])\n\n        overall_result = {\n            'total_tests': total_tests,\n            'passed_tests': passed_tests,\n            'pass_rate': passed_tests / total_tests if total_tests > 0 else 0,\n            'test_results': self.test_results\n        }\n\n        rospy.loginfo(f\"Integration tests completed: {passed_tests}/{total_tests} passed\")\n        return overall_result\n\n    def test_speech_recognition_integration(self) -> Dict:\n        \"\"\"Test speech recognition system integration\"\"\"\n        start_time = time.time()\n\n        # Simulate speech input\n        test_commands = [\n            \"navigate to kitchen\",\n            \"fetch the red cup\",\n            \"find my phone\"\n        ]\n\n        success = True\n        details = []\n\n        for command in test_commands:\n            try:\n                intent, entities = self.robot_system.nlu_processor.process_command(command)\n                if intent == 'unknown':\n                    success = False\n                    details.append(f\"Failed to process command: {command}\")\n            except Exception as e:\n                success = False\n                details.append(f\"Error processing command '{command}': {e}\")\n\n        execution_time = time.time() - start_time\n\n        return {\n            'success': success,\n            'details': '; '.join(details) if details else 'All commands processed successfully',\n            'execution_time': execution_time\n        }\n\n    def test_nlu_integration(self) -> Dict:\n        \"\"\"Test natural language understanding integration\"\"\"\n        start_time = time.time()\n\n        test_cases = [\n            (\"Go to the kitchen\", \"navigation\", {\"location\": \"kitchen\"}),\n            (\"Bring me the blue bottle\", \"object_fetch\", {\"object\": \"bottle\"}),\n            (\"Where is my phone\", \"object_identify\", {\"object\": \"phone\"}),\n            (\"Clean the office\", \"clean\", {\"location\": \"office\"})\n        ]\n\n        success = True\n        details = []\n\n        for command, expected_intent, expected_entities in test_cases:\n            intent, entities = self.robot_system.nlu_processor.process_command(command)\n\n            if intent != expected_intent:\n                success = False\n                details.append(f\"Intent mismatch for '{command}': got {intent}, expected {expected_intent}\")\n\n            for key, value in expected_entities.items():\n                if entities.get(key) != value:\n                    success = False\n                    details.append(f\"Entity mismatch for '{command}': got {entities.get(key)}, expected {value}\")\n\n        execution_time = time.time() - start_time\n\n        return {\n            'success': success,\n            'details': '; '.join(details) if details else 'All NLU tests passed',\n            'execution_time': execution_time\n        }\n\n    def test_path_planning_integration(self) -> Dict:\n        \"\"\"Test path planning system integration\"\"\"\n        start_time = time.time()\n\n        # Test path planning between known locations\n        start_pose = PoseStamped()\n        start_pose.header.frame_id = 'map'\n        start_pose.pose.position = Point(x=0, y=0, z=0)\n        start_pose.pose.orientation.w = 1.0\n\n        kitchen_pose = self.robot_system.path_planner.get_location_pose('kitchen')\n        if kitchen_pose:\n            path = self.robot_system.path_planner.plan_path(start_pose, kitchen_pose)\n            success = len(path) > 0 and self.robot_system.path_planner.validate_path(path)\n            details = f\"Path length: {len(path)} waypoints\" if success else \"Path planning failed\"\n        else:\n            success = False\n            details = \"Could not get kitchen location\"\n\n        execution_time = time.time() - start_time\n\n        return {\n            'success': success,\n            'details': details,\n            'execution_time': execution_time\n        }\n\n    def test_navigation_integration(self) -> Dict:\n        \"\"\"Test navigation system integration (simulated)\"\"\"\n        start_time = time.time()\n\n        # This would normally test actual navigation, but we'll simulate\n        # by checking if the navigation system is properly initialized\n        success = self.robot_system.navigation_system.move_base_client is not None\n        details = \"Navigation system initialized\" if success else \"Navigation system not initialized\"\n\n        execution_time = time.time() - start_time\n\n        return {\n            'success': success,\n            'details': details,\n            'execution_time': execution_time\n        }\n\n    def test_perception_integration(self) -> Dict:\n        \"\"\"Test perception system integration\"\"\"\n        start_time = time.time()\n\n        # Check if perception system is initialized\n        success = self.robot_system.perception_system.camera_sub is not None\n        details = \"Perception system initialized\" if success else \"Perception system not initialized\"\n\n        # Test object detection (with simulated image)\n        if success:\n            # Create a simulated image for testing\n            test_image = np.zeros((480, 640, 3), dtype=np.uint8)\n            # The perception system would process this in its callback\n            rospy.sleep(0.1)  # Allow time for processing\n            objects = self.robot_system.perception_system.identify_objects_in_view()\n            details += f\"; Found {len(objects)} objects in simulated view\"\n\n        execution_time = time.time() - start_time\n\n        return {\n            'success': success,\n            'details': details,\n            'execution_time': execution_time\n        }\n\n    def test_manipulation_integration(self) -> Dict:\n        \"\"\"Test manipulation system integration\"\"\"\n        start_time = time.time()\n\n        # Check if manipulation system is initialized\n        success = self.robot_system.manipulation_system is not None\n        details = \"Manipulation system initialized\" if success else \"Manipulation system not initialized\"\n\n        execution_time = time.time() - start_time\n\n        return {\n            'success': success,\n            'details': details,\n            'execution_time': execution_time\n        }\n\n    def test_safety_integration(self) -> Dict:\n        \"\"\"Test safety system integration\"\"\"\n        start_time = time.time()\n\n        # Check if safety system is initialized\n        success = (self.robot_system.safety_system.laser_sub is not None and\n                  self.robot_system.safety_system.imu_sub is not None)\n        details = \"Safety system initialized\" if success else \"Safety system not initialized\"\n\n        # Test safety status\n        if success:\n            safety_status = self.robot_system.safety_system.get_safety_status()\n            details += f\"; Safety status: {safety_status['is_safe']}\"\n\n        execution_time = time.time() - start_time\n\n        return {\n            'success': success,\n            'details': details,\n            'execution_time': execution_time\n        }\n\n    def test_end_to_end_workflow(self) -> Dict:\n        \"\"\"Test complete end-to-end workflow\"\"\"\n        start_time = time.time()\n\n        # Simulate a complete task: \"Go to kitchen and bring me the red cup\"\n        try:\n            # This would normally go through the complete pipeline\n            # For testing, we'll simulate the process\n            command = \"Go to kitchen and bring me the red cup\"\n\n            # Process command\n            intent, entities = self.robot_system.nlu_processor.process_command(command)\n\n            # Create task\n            task = self.robot_system.create_task_from_intent(intent, entities)\n\n            # Check if task was created correctly\n            success = (task.intent == 'fetch_object' and\n                      task.target_object == 'cup' and\n                      task.target_location == 'kitchen')\n\n            details = f\"Task created: {task.intent}, object: {task.target_object}, location: {task.target_location}\" if success else \"Task creation failed\"\n        except Exception as e:\n            success = False\n            details = f\"End-to-end workflow failed: {e}\"\n\n        execution_time = time.time() - start_time\n\n        return {\n            'success': success,\n            'details': details,\n            'execution_time': execution_time\n        }\n\n    def generate_performance_report(self) -> Dict:\n        \"\"\"Generate performance report for the system\"\"\"\n        if not self.test_results:\n            return {'error': 'No test results available'}\n\n        # Calculate performance metrics\n        total_execution_time = sum(result.get('execution_time', 0) for result in self.test_results)\n        passed_tests = [r for r in self.test_results if r['passed']]\n        failed_tests = [r for r in self.test_results if not r['passed']]\n\n        report = {\n            'total_tests': len(self.test_results),\n            'passed_tests': len(passed_tests),\n            'failed_tests': len(failed_tests),\n            'pass_rate': len(passed_tests) / len(self.test_results) if self.test_results else 0,\n            'total_execution_time': total_execution_time,\n            'average_test_time': total_execution_time / len(self.test_results) if self.test_results else 0,\n            'fastest_test': min((r for r in self.test_results if r.get('execution_time')),\n                              key=lambda x: x['execution_time'], default=None),\n            'slowest_test': max((r for r in self.test_results if r.get('execution_time')),\n                              key=lambda x: x['execution_time'], default=None),\n            'test_breakdown': {\n                'speech_recognition': [r for r in self.test_results if 'speech' in r['test_name'].lower()],\n                'nlu': [r for r in self.test_results if 'nlu' in r['test_name'].lower()],\n                'path_planning': [r for r in self.test_results if 'path' in r['test_name'].lower()],\n                'navigation': [r for r in self.test_results if 'nav' in r['test_name'].lower()],\n                'perception': [r for r in self.test_results if 'percept' in r['test_name'].lower()],\n                'manipulation': [r for r in self.test_results if 'manip' in r['test_name'].lower()],\n                'safety': [r for r in self.test_results if 'safety' in r['test_name'].lower()],\n                'end_to_end': [r for r in self.test_results if 'end_to_end' in r['test_name'].lower()]\n            }\n        }\n\n        return report\n\nclass PerformanceBenchmark:\n    \"\"\"\n    Performance benchmarking for the autonomous humanoid system\n    \"\"\"\n    def __init__(self, robot_system: AutonomousHumanoidSystem):\n        self.robot_system = robot_system\n        self.benchmarks = {}\n        self.results = {}\n\n    def run_benchmarks(self) -> Dict:\n        \"\"\"Run performance benchmarks\"\"\"\n        rospy.loginfo(\"Running performance benchmarks...\")\n\n        benchmarks = {\n            'speech_recognition_latency': self.benchmark_speech_recognition,\n            'nlu_processing_speed': self.benchmark_nlu_processing,\n            'path_planning_efficiency': self.benchmark_path_planning,\n            'perception_throughput': self.benchmark_perception,\n            'system_response_time': self.benchmark_system_response\n        }\n\n        for bench_name, bench_func in benchmarks.items():\n            try:\n                result = bench_func()\n                self.results[bench_name] = result\n                rospy.loginfo(f\"Benchmark {bench_name}: {result}\")\n            except Exception as e:\n                rospy.logerr(f\"Benchmark {bench_name} failed: {e}\")\n                self.results[bench_name] = {'error': str(e)}\n\n        return self.results\n\n    def benchmark_speech_recognition(self) -> Dict:\n        \"\"\"Benchmark speech recognition performance\"\"\"\n        import time\n        import numpy as np\n\n        test_phrases = [\n            \"Navigate to the kitchen\",\n            \"Fetch the red cup from the table\",\n            \"Identify my phone in the room\",\n            \"Clean the office desk\",\n            \"Follow me to the meeting room\"\n        ]\n\n        latencies = []\n        for phrase in test_phrases:\n            start_time = time.time()\n            # Simulate speech recognition processing\n            intent, entities = self.robot_system.nlu_processor.process_command(phrase)\n            end_time = time.time()\n            latencies.append(end_time - start_time)\n\n        return {\n            'average_latency': np.mean(latencies),\n            'min_latency': min(latencies),\n            'max_latency': max(latencies),\n            'std_deviation': np.std(latencies),\n            'throughput': len(test_phrases) / sum(latencies)\n        }\n\n    def benchmark_nlu_processing(self) -> Dict:\n        \"\"\"Benchmark NLU processing performance\"\"\"\n        import time\n        import numpy as np\n\n        test_commands = [\n            \"Go to the kitchen and fetch me a glass of water\",\n            \"Please find my keys and bring them to me\",\n            \"Navigate to the bedroom and tidy up the desk\",\n            \"Locate the red ball and place it in the toy box\",\n            \"Move to the office and organize the papers\"\n        ]\n\n        processing_times = []\n        for command in test_commands:\n            start_time = time.time()\n            intent, entities = self.robot_system.nlu_processor.process_command(command)\n            end_time = time.time()\n            processing_times.append(end_time - start_time)\n\n        return {\n            'average_processing_time': np.mean(processing_times),\n            'min_processing_time': min(processing_times),\n            'max_processing_time': max(processing_times),\n            'std_deviation': np.std(processing_times),\n            'commands_per_second': len(test_commands) / sum(processing_times)\n        }\n\n    def benchmark_path_planning(self) -> Dict:\n        \"\"\"Benchmark path planning performance\"\"\"\n        import time\n        import numpy as np\n\n        # Create test poses\n        start_pose = PoseStamped()\n        start_pose.header.frame_id = 'map'\n        start_pose.pose.position = Point(x=0, y=0, z=0)\n        start_pose.pose.orientation.w = 1.0\n\n        test_destinations = ['kitchen', 'bedroom', 'office', 'living_room', 'bathroom']\n\n        planning_times = []\n        path_lengths = []\n\n        for dest_name in test_destinations:\n            dest_pose = self.robot_system.path_planner.get_location_pose(dest_name)\n            if dest_pose:\n                start_time = time.time()\n                path = self.robot_system.path_planner.plan_path(start_pose, dest_pose)\n                end_time = time.time()\n\n                planning_times.append(end_time - start_time)\n                path_lengths.append(len(path))\n\n        return {\n            'average_planning_time': np.mean(planning_times) if planning_times else 0,\n            'min_planning_time': min(planning_times) if planning_times else 0,\n            'max_planning_time': max(planning_times) if planning_times else 0,\n            'average_path_length': np.mean(path_lengths) if path_lengths else 0,\n            'planning_success_rate': len([p for p in path_lengths if p > 0]) / len(test_destinations)\n        }\n\n    def benchmark_perception(self) -> Dict:\n        \"\"\"Benchmark perception system performance\"\"\"\n        import time\n        import numpy as np\n\n        # Simulate processing multiple images\n        test_iterations = 10\n        processing_times = []\n\n        for i in range(test_iterations):\n            # Create a simulated image\n            test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n\n            start_time = time.time()\n            # Simulate object detection\n            with self.robot_system.perception_system.image_lock:\n                self.robot_system.perception_system.latest_image = test_image\n\n            # Wait briefly for processing\n            rospy.sleep(0.01)\n\n            objects = self.robot_system.perception_system.identify_objects_in_view()\n            end_time = time.time()\n\n            processing_times.append(end_time - start_time)\n\n        return {\n            'average_processing_time': np.mean(processing_times),\n            'frames_per_second': 1.0 / np.mean(processing_times) if processing_times else 0,\n            'objects_detected_per_frame': np.mean([len(self.robot_system.perception_system._simulate_object_detection(test_image))]),\n            'processing_variance': np.std(processing_times)\n        }\n\n    def benchmark_system_response(self) -> Dict:\n        \"\"\"Benchmark overall system response time\"\"\"\n        import time\n        import numpy as np\n\n        # Simulate end-to-end task execution\n        test_tasks = [\n            {'intent': 'navigation', 'target_location': 'kitchen'},\n            {'intent': 'object_fetch', 'target_object': 'cup', 'target_location': 'kitchen'},\n            {'intent': 'object_identify', 'target_object': 'phone'}\n        ]\n\n        response_times = []\n        for task_data in test_tasks:\n            # Create a task similar to how it would be created in the real system\n            task = TaskPlan(\n                id=f\"benchmark_task_{int(time.time())}\",\n                command=task_data['intent'],\n                intent=task_data['intent'],\n                target_object=task_data.get('target_object'),\n                target_location=task_data.get('target_location'),\n                priority=1\n            )\n\n            start_time = time.time()\n            # Simulate task execution\n            if task.intent == 'navigation':\n                # Simulate navigation task\n                rospy.sleep(0.1)  # Simulated navigation time\n            elif task.intent == 'object_fetch':\n                # Simulate fetch task\n                rospy.sleep(0.2)  # Simulated perception and manipulation time\n            elif task.intent == 'object_identify':\n                # Simulate identification task\n                rospy.sleep(0.05)  # Simulated perception time\n\n            end_time = time.time()\n            response_times.append(end_time - start_time)\n\n        return {\n            'average_response_time': np.mean(response_times),\n            'min_response_time': min(response_times),\n            'max_response_time': max(response_times),\n            'tasks_per_minute': (len(test_tasks) * 60) / sum(response_times) if response_times else 0\n        }\n\ndef run_comprehensive_tests():\n    \"\"\"Run comprehensive tests and benchmarks on the autonomous humanoid system\"\"\"\n    rospy.loginfo(\"=== Autonomous Humanoid System - Comprehensive Testing ===\")\n\n    # Initialize the robot system\n    robot_system = AutonomousHumanoidSystem()\n\n    # Run integration tests\n    tester = SystemIntegrationTester(robot_system)\n    integration_results = tester.run_integration_tests()\n\n    print(f\"\\nIntegration Test Results:\")\n    print(f\"Total tests: {integration_results['total_tests']}\")\n    print(f\"Passed: {integration_results['passed_tests']}\")\n    print(f\"Pass rate: {integration_results['pass_rate']:.2%}\")\n\n    # Run performance benchmarks\n    benchmark = PerformanceBenchmark(robot_system)\n    benchmark_results = benchmark.run_benchmarks()\n\n    print(f\"\\nPerformance Benchmark Results:\")\n    for bench_name, result in benchmark_results.items():\n        if 'error' not in result:\n            print(f\"{bench_name}:\")\n            for metric, value in result.items():\n                print(f\"  {metric}: {value:.4f}\" if isinstance(value, float) else f\"  {metric}: {value}\")\n        else:\n            print(f\"{bench_name}: ERROR - {result['error']}\")\n\n    # Generate final report\n    performance_report = tester.generate_performance_report()\n\n    print(f\"\\nPerformance Summary:\")\n    print(f\"  Total execution time: {performance_report['total_execution_time']:.2f}s\")\n    print(f\"  Average test time: {performance_report['average_test_time']:.3f}s\")\n    print(f\"  Pass rate: {performance_report['pass_rate']:.2%}\")\n\n    print(f\"\\nSystem testing completed successfully!\")\n    print(f\"The autonomous humanoid system has been validated with {integration_results['passed_tests']}/{integration_results['total_tests']} integration tests passed.\")\n\n    return integration_results, benchmark_results, performance_report\n\nif __name__ == '__main__':\n    try:\n        integration_results, benchmark_results, performance_report = run_comprehensive_tests()\n    except Exception as e:\n        rospy.logerr(f\"Error during comprehensive testing: {e}\")\n        import traceback\n        traceback.print_exc()\n```\n\n## Deployment and Validation\n\n### Simulation Environment Setup\n\n```python\nclass SimulationEnvironment:\n    \"\"\"\n    Setup and management of simulation environment for the autonomous humanoid\n    \"\"\"\n    def __init__(self, environment_type: str = \"indoor_office\"):\n        self.environment_type = environment_type\n        self.objects = []\n        self.obstacles = []\n        self.navigation_goals = []\n        self.lighting_conditions = {}\n        self.physics_properties = {}\n\n    def setup_environment(self):\n        \"\"\"Setup the simulation environment\"\"\"\n        rospy.loginfo(f\"Setting up {self.environment_type} environment\")\n\n        if self.environment_type == \"indoor_office\":\n            self._setup_indoor_office_environment()\n        elif self.environment_type == \"home_environment\":\n            self._setup_home_environment()\n        elif self.environment_type == \"industrial_warehouse\":\n            self._setup_warehouse_environment()\n        else:\n            rospy.logwarn(f\"Unknown environment type: {self.environment_type}, using default\")\n            self._setup_default_environment()\n\n    def _setup_indoor_office_environment(self):\n        \"\"\"Setup indoor office environment\"\"\"\n        # Add office furniture\n        self.objects.extend([\n            {\"name\": \"desk1\", \"type\": \"furniture\", \"position\": [2.0, 0.0, 0.0], \"size\": [1.5, 0.8, 0.8], \"movable\": False},\n            {\"name\": \"chair1\", \"type\": \"furniture\", \"position\": [2.5, 0.5, 0.0], \"size\": [0.6, 0.6, 0.8], \"movable\": True},\n            {\"name\": \"bookshelf\", \"type\": \"furniture\", \"position\": [0.0, 2.0, 0.0], \"size\": [0.4, 1.2, 2.0], \"movable\": False},\n            {\"name\": \"coffee_table\", \"type\": \"furniture\", \"position\": [-1.0, -1.0, 0.0], \"size\": [0.8, 0.8, 0.4], \"movable\": False},\n            {\"name\": \"plant\", \"type\": \"decoration\", \"position\": [1.5, -1.5, 0.0], \"size\": [0.3, 0.3, 0.6], \"movable\": False}\n        ])\n\n        # Add small objects for manipulation tasks\n        self.objects.extend([\n            {\"name\": \"cup\", \"type\": \"object\", \"position\": [2.2, 0.2, 0.8], \"size\": [0.08, 0.08, 0.1], \"movable\": True},\n            {\"name\": \"book\", \"type\": \"object\", \"position\": [2.1, -0.1, 0.8], \"size\": [0.2, 0.15, 0.03], \"movable\": True},\n            {\"name\": \"phone\", \"type\": \"object\", \"position\": [2.3, 0.1, 0.8], \"size\": [0.1, 0.05, 0.15], \"movable\": True}\n        ])\n\n        # Define navigation goals/locations\n        self.navigation_goals = [\n            {\"name\": \"desk_area\", \"position\": [2.0, 0.0, 0.0]},\n            {\"name\": \"meeting_room\", \"position\": [-2.0, 1.0, 0.0]},\n            {\"name\": \"kitchen\", \"position\": [0.0, -2.0, 0.0]},\n            {\"name\": \"entrance\", \"position\": [0.0, 2.0, 0.0]}\n        ]\n\n        # Set lighting conditions\n        self.lighting_conditions = {\n            \"ambient_light\": 500,  # lux\n            \"directional_light\": {\"direction\": [-0.5, -1, -0.5], \"intensity\": 800},\n            \"color_temperature\": 4000  # Kelvin (warm white)\n        }\n\n        # Set physics properties\n        self.physics_properties = {\n            \"gravity\": [0, 0, -9.81],\n            \"friction_coefficient\": 0.7,\n            \"restitution\": 0.1\n        }\n\n    def _setup_home_environment(self):\n        \"\"\"Setup home environment\"\"\"\n        # Add home furniture and objects\n        self.objects.extend([\n            {\"name\": \"sofa\", \"type\": \"furniture\", \"position\": [0.0, 2.0, 0.0], \"size\": [2.0, 0.9, 0.8], \"movable\": False},\n            {\"name\": \"dining_table\", \"type\": \"furniture\", \"position\": [-1.5, -1.0, 0.0], \"size\": [1.8, 0.9, 0.75], \"movable\": False},\n            {\"name\": \"tv_stand\", \"type\": \"furniture\", \"position\": [2.0, 1.5, 0.0], \"size\": [1.2, 0.5, 0.6], \"movable\": False},\n            {\"name\": \"bed\", \"type\": \"furniture\", \"position\": [-2.0, -2.0, 0.0], \"size\": [2.0, 1.5, 0.5], \"movable\": False}\n        ])\n\n        # Add household objects\n        self.objects.extend([\n            {\"name\": \"plate\", \"type\": \"object\", \"position\": [-1.4, -0.9, 0.75], \"size\": [0.25, 0.25, 0.02], \"movable\": True},\n            {\"name\": \"glass\", \"type\": \"object\", \"position\": [-1.2, -1.1, 0.75], \"size\": [0.08, 0.08, 0.15], \"movable\": True},\n            {\"name\": \"remote\", \"type\": \"object\", \"position\": [0.1, 1.9, 0.8], \"size\": [0.15, 0.1, 0.02], \"movable\": True}\n        ])\n\n        # Navigation goals\n        self.navigation_goals = [\n            {\"name\": \"living_room\", \"position\": [0.0, 1.0, 0.0]},\n            {\"name\": \"kitchen\", \"position\": [1.0, -2.0, 0.0]},\n            {\"name\": \"bedroom\", \"position\": [-2.0, -2.0, 0.0]},\n            {\"name\": \"dining_area\", \"position\": [-1.5, -1.0, 0.0]}\n        ]\n\n    def _setup_warehouse_environment(self):\n        \"\"\"Setup industrial warehouse environment\"\"\"\n        # Add warehouse elements\n        self.objects.extend([\n            {\"name\": \"rack1\", \"type\": \"storage\", \"position\": [0.0, 5.0, 0.0], \"size\": [0.8, 2.0, 2.5], \"movable\": False},\n            {\"name\": \"rack2\", \"type\": \"storage\", \"position\": [0.0, 8.0, 0.0], \"size\": [0.8, 2.0, 2.5], \"movable\": False},\n            {\"name\": \"pallet\", \"type\": \"storage\", \"position\": [2.0, 2.0, 0.0], \"size\": [1.2, 1.0, 0.15], \"movable\": False},\n            {\"name\": \"conveyor\", \"type\": \"equipment\", \"position\": [-2.0, 0.0, 0.0], \"size\": [3.0, 0.5, 0.8], \"movable\": False}\n        ])\n\n        # Add industrial objects\n        self.objects.extend([\n            {\"name\": \"toolbox\", \"type\": \"object\", \"position\": [0.2, 4.8, 0.2], \"size\": [0.4, 0.3, 0.25], \"movable\": True},\n            {\"name\": \"safety_goggles\", \"type\": \"object\", \"position\": [0.3, 4.9, 0.5], \"size\": [0.15, 0.1, 0.05], \"movable\": True},\n            {\"name\": \"manual\", \"type\": \"object\", \"position\": [0.1, 4.7, 0.5], \"size\": [0.3, 0.2, 0.01], \"movable\": True}\n        ])\n\n        # Warehouse navigation goals\n        self.navigation_goals = [\n            {\"name\": \"loading_dock\", \"position\": [3.0, -2.0, 0.0]},\n            {\"name\": \"assembly_station\", \"position\": [-1.0, 1.0, 0.0]},\n            {\"name\": \"storage_area\", \"position\": [0.0, 6.0, 0.0]},\n            {\"name\": \"quality_control\", \"position\": [2.0, 4.0, 0.0]}\n        ]\n\n    def _setup_default_environment(self):\n        \"\"\"Setup default simple environment\"\"\"\n        self.objects.extend([\n            {\"name\": \"simple_table\", \"type\": \"furniture\", \"position\": [1.0, 0.0, 0.0], \"size\": [1.0, 0.6, 0.75], \"movable\": False},\n            {\"name\": \"object1\", \"type\": \"object\", \"position\": [1.2, 0.1, 0.75], \"size\": [0.1, 0.1, 0.1], \"movable\": True}\n        ])\n\n        self.navigation_goals = [\n            {\"name\": \"center\", \"position\": [0.0, 0.0, 0.0]},\n            {\"name\": \"north\", \"position\": [0.0, 2.0, 0.0]},\n            {\"name\": \"east\", \"position\": [2.0, 0.0, 0.0]}\n        ]\n\n    def spawn_objects_in_gazebo(self):\n        \"\"\"Spawn objects in Gazebo simulation\"\"\"\n        import subprocess\n        import time\n\n        for obj in self.objects:\n            # Create a simple model for each object\n            model_name = obj[\"name\"]\n            x, y, z = obj[\"position\"]\n            size_x, size_y, size_z = obj[\"size\"]\n\n            # Create a simple box model (in practice, use proper URDF/SDF models)\n            model_xml = f\"\"\"\n            <?xml version=\"1.0\" ?>\n            <sdf version=\"1.6\">\n              <model name=\"{model_name}\">\n                <pose>{x} {y} {z+size_z/2} 0 0 0</pose>\n                <link name=\"link\">\n                  <pose>0 0 0 0 0 0</pose>\n                  <inertial>\n                    <mass>1.0</mass>\n                    <inertia>\n                      <ixx>0.166667</ixx>\n                      <ixy>0</ixy>\n                      <ixz>0</ixz>\n                      <iyy>0.166667</iyy>\n                      <iyz>0</iyz>\n                      <izz>0.166667</izz>\n                    </inertia>\n                  </inertial>\n                  <collision name=\"collision\">\n                    <geometry>\n                      <box>\n                        <size>{size_x} {size_y} {size_z}</size>\n                      </box>\n                    </geometry>\n                  </collision>\n                  <visual name=\"visual\">\n                    <geometry>\n                      <box>\n                        <size>{size_x} {size_y} {size_z}</size>\n                      </box>\n                    </geometry>\n                  </visual>\n                </link>\n              </model>\n            </sdf>\n            \"\"\"\n\n            # Write model to temporary file\n            model_path = f\"/tmp/{model_name}_model.sdf\"\n            with open(model_path, 'w') as f:\n                f.write(model_xml)\n\n            # Spawn model in Gazebo\n            try:\n                subprocess.run([\n                    'rosrun', 'gazebo_ros', 'spawn_model',\n                    '-file', model_path,\n                    '-sdf',\n                    '-model', model_name,\n                    '-x', str(x),\n                    '-y', str(y),\n                    '-z', str(z + size_z/2)\n                ], check=True)\n                rospy.loginfo(f\"Spawned model: {model_name}\")\n            except subprocess.CalledProcessError as e:\n                rospy.logerr(f\"Failed to spawn model {model_name}: {e}\")\n            except Exception as e:\n                rospy.logerr(f\"Error spawning model {model_name}: {e}\")\n\n            time.sleep(0.5)  # Brief pause between spawns\n\n    def configure_environment_for_task(self, task_type: str):\n        \"\"\"Configure environment based on task requirements\"\"\"\n        if task_type == \"object_fetch\":\n            # Ensure there are movable objects in the environment\n            movable_objects = [obj for obj in self.objects if obj.get(\"movable\", False)]\n            if not movable_objects:\n                rospy.logwarn(\"No movable objects for fetch task, adding default object\")\n                self.objects.append({\n                    \"name\": \"default_object\",\n                    \"type\": \"object\",\n                    \"position\": [1.0, 0.5, 0.0],\n                    \"size\": [0.1, 0.1, 0.1],\n                    \"movable\": True\n                })\n        elif task_type == \"navigation\":\n            # Ensure navigation goals are properly defined\n            if not self.navigation_goals:\n                rospy.logwarn(\"No navigation goals defined, adding default goals\")\n                self.navigation_goals = [\n                    {\"name\": \"start\", \"position\": [0.0, 0.0, 0.0]},\n                    {\"name\": \"goal\", \"position\": [2.0, 2.0, 0.0]}\n                ]\n        elif task_type == \"perception\":\n            # Add various objects to test perception\n            perception_objects = [\n                {\"name\": \"red_cube\", \"type\": \"object\", \"position\": [0.5, 0.5, 0.0], \"size\": [0.1, 0.1, 0.1], \"movable\": True, \"color\": \"red\"},\n                {\"name\": \"blue_sphere\", \"type\": \"object\", \"position\": [-0.5, 0.5, 0.0], \"size\": [0.1, 0.1, 0.1], \"movable\": True, \"shape\": \"sphere\", \"color\": \"blue\"},\n                {\"name\": \"green_cylinder\", \"type\": \"object\", \"position\": [0.0, -0.5, 0.0], \"size\": [0.1, 0.1, 0.1], \"movable\": True, \"shape\": \"cylinder\", \"color\": \"green\"}\n            ]\n            self.objects.extend(perception_objects)\n\nclass AutonomousHumanoidValidator:\n    \"\"\"\n    Validator for the autonomous humanoid system\n    \"\"\"\n    def __init__(self, robot_system: AutonomousHumanoidSystem):\n        self.robot_system = robot_system\n        self.validation_results = []\n        self.metrics = {\n            'task_success_rate': 0.0,\n            'navigation_accuracy': 0.0,\n            'object_detection_rate': 0.0,\n            'manipulation_success_rate': 0.0,\n            'response_time': 0.0,\n            'safety_violations': 0\n        }\n\n    def run_validation_tests(self, test_scenarios: List[Dict]) -> Dict:\n        \"\"\"\n        Run validation tests for the autonomous humanoid system\n\n        Args:\n            test_scenarios: List of test scenarios to execute\n\n        Returns:\n            Validation results and metrics\n        \"\"\"\n        rospy.loginfo(f\"Running {len(test_scenarios)} validation tests...\")\n\n        for i, scenario in enumerate(test_scenarios):\n            rospy.loginfo(f\"Running test scenario {i+1}/{len(test_scenarios)}: {scenario['name']}\")\n\n            result = self._execute_test_scenario(scenario)\n            self.validation_results.append(result)\n\n            # Update metrics based on result\n            self._update_metrics(result)\n\n        # Calculate final metrics\n        final_metrics = self._calculate_final_metrics()\n\n        validation_report = {\n            'total_scenarios': len(test_scenarios),\n            'successful_scenarios': sum(1 for r in self.validation_results if r['success']),\n            'metrics': final_metrics,\n            'detailed_results': self.validation_results\n        }\n\n        return validation_report\n\n    def _execute_test_scenario(self, scenario: Dict) -> Dict:\n        \"\"\"\n        Execute a single test scenario\n        \"\"\"\n        scenario_start_time = time.time()\n\n        try:\n            # Set up environment for scenario\n            env = SimulationEnvironment(scenario.get('environment', 'indoor_office'))\n            env.setup_environment()\n            env.configure_environment_for_task(scenario['task_type'])\n\n            # Spawn objects in simulation\n            env.spawn_objects_in_gazebo()\n\n            # Execute the task\n            task_result = self._execute_task_in_scenario(scenario)\n\n            # Validate results\n            validation_passed = self._validate_scenario_result(scenario, task_result)\n\n            scenario_duration = time.time() - scenario_start_time\n\n            return {\n                'name': scenario['name'],\n                'task_type': scenario['task_type'],\n                'success': validation_passed,\n                'execution_time': scenario_duration,\n                'details': task_result,\n                'metrics': self._calculate_scenario_metrics(scenario, task_result)\n            }\n\n        except Exception as e:\n            rospy.logerr(f\"Error in scenario {scenario['name']}: {e}\")\n            return {\n                'name': scenario['name'],\n                'task_type': scenario['task_type'],\n                'success': False,\n                'execution_time': time.time() - scenario_start_time,\n                'details': f\"Error: {str(e)}\",\n                'metrics': {}\n            }\n\n    def _execute_task_in_scenario(self, scenario: Dict) -> Dict:\n        \"\"\"\n        Execute the specific task within a scenario\n        \"\"\"\n        task_type = scenario['task_type']\n        target = scenario.get('target', '')\n        location = scenario.get('location', '')\n\n        # Create a task similar to how it would be created from voice command\n        if task_type == 'navigation':\n            command = f\"go to {location}\"\n            intent = 'navigation'\n            entities = {'location': location}\n        elif task_type == 'object_fetch':\n            command = f\"fetch the {target}\"\n            intent = 'object_fetch'\n            entities = {'object': target, 'location': location}\n        elif task_type == 'object_place':\n            command = f\"place {target} at {location}\"\n            intent = 'object_place'\n            entities = {'object': target, 'location': location}\n        elif task_type == 'object_identify':\n            command = f\"find {target}\"\n            intent = 'object_identify'\n            entities = {'object': target}\n        else:\n            raise ValueError(f\"Unknown task type: {task_type}\")\n\n        # Create and execute task\n        task = self.robot_system.create_task_from_intent(intent, entities)\n\n        # For validation, we'll simulate the task execution rather than run the full system\n        # In a real validation, this would involve the complete system\n        simulation_result = {\n            'task_executed': True,\n            'completion_time': np.random.uniform(5, 30),  # Simulated time\n            'accuracy': np.random.uniform(0.8, 1.0),     # Simulated accuracy\n            'safety_compliance': True,\n            'resource_utilization': np.random.uniform(0.6, 0.9)\n        }\n\n        return simulation_result\n\n    def _validate_scenario_result(self, scenario: Dict, result: Dict) -> bool:\n        \"\"\"\n        Validate if the scenario result meets requirements\n        \"\"\"\n        required_accuracy = scenario.get('required_accuracy', 0.8)\n        required_time = scenario.get('required_time', 60.0)\n        safety_critical = scenario.get('safety_critical', False)\n\n        # Check if task was executed successfully\n        if not result.get('task_executed', False):\n            return False\n\n        # Check accuracy\n        accuracy = result.get('accuracy', 0.0)\n        if accuracy < required_accuracy:\n            rospy.logwarn(f\"Accuracy {accuracy:.2f} below required {required_accuracy}\")\n            if scenario.get('strict_accuracy', True):\n                return False\n\n        # Check time constraints\n        completion_time = result.get('completion_time', float('inf'))\n        if completion_time > required_time:\n            rospy.logwarn(f\"Completion time {completion_time:.2f}s exceeds required {required_time}s\")\n            if scenario.get('strict_timing', True):\n                return False\n\n        # Check safety compliance\n        safety_compliance = result.get('safety_compliance', True)\n        if safety_critical and not safety_compliance:\n            rospy.logerr(\"Safety compliance failed in safety-critical scenario\")\n            return False\n\n        return True\n\n    def _update_metrics(self, result: Dict):\n        \"\"\"\n        Update validation metrics based on test result\n        \"\"\"\n        if result['success']:\n            # Update success-based metrics\n            task_type = result['task_type']\n\n            if task_type == 'navigation':\n                self.metrics['navigation_accuracy'] += result['metrics'].get('accuracy', 0.0)\n            elif task_type == 'object_fetch':\n                self.metrics['object_detection_rate'] += result['metrics'].get('detection_rate', 0.0)\n                self.metrics['manipulation_success_rate'] += result['metrics'].get('manipulation_success', 0.0)\n            elif task_type == 'object_identify':\n                self.metrics['object_detection_rate'] += result['metrics'].get('detection_rate', 0.0)\n\n        # Update response time\n        self.metrics['response_time'] += result['execution_time']\n\n        # Check for safety violations\n        if not result.get('details', {}).get('safety_compliance', True):\n            self.metrics['safety_violations'] += 1\n\n    def _calculate_scenario_metrics(self, scenario: Dict, result: Dict) -> Dict:\n        \"\"\"\n        Calculate specific metrics for a scenario\n        \"\"\"\n        metrics = {}\n\n        if scenario['task_type'] == 'navigation':\n            # Calculate navigation-specific metrics\n            distance_to_goal = np.random.uniform(0.05, 0.3)  # Simulated error\n            metrics['accuracy'] = max(0.0, 1.0 - distance_to_goal)  # Higher accuracy for smaller error\n            metrics['path_efficiency'] = np.random.uniform(0.7, 1.0)\n        elif scenario['task_type'] in ['object_fetch', 'object_place']:\n            # Calculate manipulation-specific metrics\n            detection_success = np.random.choice([True, False], p=[0.9, 0.1])  # 90% success rate\n            manipulation_success = np.random.choice([True, False], p=[0.85, 0.15])  # 85% success rate\n\n            metrics['detection_rate'] = 1.0 if detection_success else 0.0\n            metrics['manipulation_success'] = 1.0 if manipulation_success else 0.0\n            metrics['grasp_success'] = np.random.uniform(0.8, 1.0) if manipulation_success else 0.0\n        elif scenario['task_type'] == 'object_identify':\n            # Calculate perception-specific metrics\n            detection_success = np.random.choice([True, False], p=[0.95, 0.05])  # 95% success rate\n            metrics['detection_rate'] = 1.0 if detection_success else 0.0\n            metrics['classification_accuracy'] = np.random.uniform(0.85, 0.98)\n\n        return metrics\n\n    def _calculate_final_metrics(self) -> Dict:\n        \"\"\"\n        Calculate final validation metrics\n        \"\"\"\n        total_scenarios = len(self.validation_results)\n        if total_scenarios == 0:\n            return self.metrics\n\n        successful_scenarios = sum(1 for r in self.validation_results if r['success'])\n        self.metrics['task_success_rate'] = successful_scenarios / total_scenarios\n\n        # Calculate averages for the metrics that accumulated values\n        navigation_results = [r for r in self.validation_results\n                            if r.get('task_type') == 'navigation' and r['success']]\n        if navigation_results:\n            avg_accuracy = np.mean([r['metrics'].get('accuracy', 0) for r in navigation_results])\n            self.metrics['navigation_accuracy'] = avg_accuracy\n\n        manipulation_results = [r for r in self.validation_results\n                              if r.get('task_type') in ['object_fetch', 'object_place'] and r['success']]\n        if manipulation_results:\n            avg_detection = np.mean([r['metrics'].get('detection_rate', 0) for r in manipulation_results])\n            avg_manipulation = np.mean([r['metrics'].get('manipulation_success', 0) for r in manipulation_results])\n            self.metrics['object_detection_rate'] = avg_detection\n            self.metrics['manipulation_success_rate'] = avg_manipulation\n\n        perception_results = [r for r in self.validation_results\n                            if r.get('task_type') == 'object_identify' and r['success']]\n        if perception_results:\n            avg_detection = np.mean([r['metrics'].get('detection_rate', 0) for r in perception_results])\n            self.metrics['object_detection_rate'] = max(self.metrics['object_detection_rate'], avg_detection)\n\n        # Calculate average response time\n        if self.validation_results:\n            total_time = sum(r['execution_time'] for r in self.validation_results)\n            self.metrics['response_time'] = total_time / total_scenarios\n\n        return self.metrics\n\ndef validate_autonomous_humanoid():\n    \"\"\"\n    Validate the complete autonomous humanoid system\n    \"\"\"\n    rospy.loginfo(\"=== Autonomous Humanoid System Validation ===\")\n\n    # Initialize the robot system\n    robot_system = AutonomousHumanoidSystem()\n\n    # Define validation test scenarios\n    test_scenarios = [\n        {\n            \"name\": \"Basic Navigation Test\",\n            \"task_type\": \"navigation\",\n            \"location\": \"kitchen\",\n            \"environment\": \"indoor_office\",\n            \"required_accuracy\": 0.9,\n            \"required_time\": 30.0,\n            \"strict_accuracy\": True,\n            \"strict_timing\": False\n        },\n        {\n            \"name\": \"Object Fetch Test\",\n            \"task_type\": \"object_fetch\",\n            \"target\": \"cup\",\n            \"location\": \"kitchen\",\n            \"environment\": \"indoor_office\",\n            \"required_accuracy\": 0.85,\n            \"required_time\": 60.0,\n            \"strict_accuracy\": False,\n            \"strict_timing\": False\n        },\n        {\n            \"name\": \"Object Identification Test\",\n            \"task_type\": \"object_identify\",\n            \"target\": \"phone\",\n            \"environment\": \"indoor_office\",\n            \"required_accuracy\": 0.95,\n            \"required_time\": 20.0,\n            \"strict_accuracy\": True,\n            \"strict_timing\": False\n        },\n        {\n            \"name\": \"Room Navigation Test\",\n            \"task_type\": \"navigation\",\n            \"location\": \"bedroom\",\n            \"environment\": \"home_environment\",\n            \"required_accuracy\": 0.85,\n            \"required_time\": 45.0,\n            \"strict_accuracy\": False,\n            \"strict_timing\": False\n        },\n        {\n            \"name\": \"Household Object Fetch\",\n            \"task_type\": \"object_fetch\",\n            \"target\": \"book\",\n            \"location\": \"living_room\",\n            \"environment\": \"home_environment\",\n            \"required_accuracy\": 0.8,\n            \"required_time\": 75.0,\n            \"strict_accuracy\": False,\n            \"strict_timing\": False\n        }\n    ]\n\n    # Initialize validator\n    validator = AutonomousHumanoidValidator(robot_system)\n\n    # Run validation tests\n    validation_report = validator.run_validation_tests(test_scenarios)\n\n    # Print validation results\n    print(f\"\\nValidation Summary:\")\n    print(f\"  Total scenarios: {validation_report['total_scenarios']}\")\n    print(f\"  Successful scenarios: {validation_report['successful_scenarios']}\")\n    print(f\"  Success rate: {validation_report['metrics']['task_success_rate']:.2%}\")\n\n    print(f\"\\nDetailed Metrics:\")\n    for metric, value in validation_report['metrics'].items():\n        if isinstance(value, float):\n            print(f\"  {metric}: {value:.3f}\")\n        else:\n            print(f\"  {metric}: {value}\")\n\n    print(f\"\\nIndividual Test Results:\")\n    for result in validation_report['detailed_results']:\n        status = \"✓ PASS\" if result['success'] else \"✗ FAIL\"\n        print(f\"  {status} - {result['name']} ({result['task_type']}), \"\n              f\"time: {result['execution_time']:.2f}s\")\n\n    # Generate validation certificate\n    validation_certificate = {\n        \"validation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"system_version\": \"1.0.0\",\n        \"validator_name\": \"AutonomousHumanoidValidator\",\n        \"total_tests\": validation_report['total_scenarios'],\n        \"passed_tests\": validation_report['successful_scenarios'],\n        \"success_rate\": validation_report['metrics']['task_success_rate'],\n        \"overall_rating\": \"PASS\" if validation_report['metrics']['task_success_rate'] >= 0.8 else \"FAIL\",\n        \"notes\": [\n            \"System demonstrated capability for basic navigation and manipulation tasks\",\n            \"Object detection accuracy meets requirements\",\n            \"Response times are within acceptable limits\",\n            f\"Total safety violations: {validation_report['metrics']['safety_violations']}\"\n        ]\n    }\n\n    print(f\"\\nValidation Certificate:\")\n    for key, value in validation_certificate.items():\n        if key != \"notes\":\n            print(f\"  {key}: {value}\")\n    print(f\"  notes:\")\n    for note in validation_certificate['notes']:\n        print(f\"    - {note}\")\n\n    print(f\"\\nAutonomous humanoid system validation completed!\")\n    print(f\"Overall rating: {validation_certificate['overall_rating']}\")\n\n    return validation_report, validation_certificate\n\nif __name__ == '__main__':\n    try:\n        validation_report, validation_certificate = validate_autonomous_humanoid()\n    except Exception as e:\n        rospy.logerr(f\"Error during validation: {e}\")\n        import traceback\n        traceback.print_exc()\n```\n\n## Summary\n\nThe Autonomous Humanoid Capstone Project integrates all the key concepts learned throughout the book into a complete, functional system. This project demonstrates:\n\n1. **Multi-Subsystem Integration**: Bringing together ROS 2, simulation, perception, and control systems\n2. **Natural Language Processing**: Converting voice commands to actionable robot tasks\n3. **Path Planning and Navigation**: Moving the robot through complex environments\n4. **Computer Vision**: Identifying and localizing objects for manipulation\n5. **Manipulation Control**: Grasping and manipulating objects with precision\n6. **Balance and Safety**: Maintaining stability and ensuring safe operation\n7. **System Validation**: Comprehensive testing and validation of the integrated system\n\nThe project showcases the complete pipeline from high-level command interpretation to low-level motor control, demonstrating the practical application of humanoid robotics concepts.\n\n## Key Implementation Considerations\n\n1. **Modularity**: Each subsystem (speech, perception, navigation, manipulation) is designed to be modular and testable independently.\n\n2. **Safety First**: Multiple safety checks and emergency stop capabilities ensure safe operation.\n\n3. **Robustness**: Error handling and fallback behaviors ensure the system can recover from failures gracefully.\n\n4. **Scalability**: The architecture is designed to accommodate additional sensors, actuators, and capabilities.\n\n5. **Validation**: Comprehensive testing framework ensures the system behaves correctly in various scenarios.\n\nThis capstone project serves as a comprehensive demonstration of autonomous humanoid robotics capabilities, providing a solid foundation for developing more advanced robotic systems.",
    "module": "Module 4: Vision-Language-Action (VLA)",
    "chapter": "capstone project autonomous humanoid.md",
    "path": "/docs/module-4/4.4-capstone-project-autonomous-humanoid"
  },
  {
    "id": "ros2-basics",
    "title": "ROS 2 Basics: Robotic Nervous System",
    "content": "\n# ROS 2 Basics: Robotic Nervous System\n\n## Introduction to ROS 2\n\nROS 2 (Robot Operating System 2) is a flexible framework for writing robot software. It's a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robot platforms.\n\n## Core Concepts\n\n### Nodes\n\nA node is a process that performs computation. ROS 2 is designed to be modular, with each node running independently and communicating with other components via messages. Nodes are the fundamental building blocks of a ROS 2 system.\n\n### Topics and Messages\n\nTopics are named buses over which nodes exchange messages. Messages are the data packets sent from publishers to subscribers over topics. This publisher-subscriber communication pattern allows for asynchronous communication between nodes.\n\n### Services\n\nServices provide a request/reply communication pattern. Unlike topics which are asynchronous, services are synchronous and block until a response is received. Services are useful for operations that require a specific response.\n\n## Creating Your First ROS 2 Node\n\nLet's create a simple ROS 2 node using Python:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\n\nclass MinimalPublisher(Node):\n\n    def __init__(self):\n        super().__init__('minimal_publisher')\n        self.publisher_ = self.create_publisher(String, 'topic', 10)\n        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n        self.i = 0\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = 'Hello World: %d' % self.i\n        self.publisher_.publish(msg)\n        self.get_logger().info('Publishing: \"%s\"' % msg.data)\n        self.i += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    minimal_publisher = MinimalPublisher()\n    rclpy.spin(minimal_publisher)\n    minimal_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## ROS 2 with AI Agents\n\nROS 2 can be integrated with AI agents through the rclpy library, allowing for sophisticated control and decision-making capabilities. This bridge between traditional robotics and AI enables more intelligent robot behavior.\n\n### Agent-to-ROS Bridge\n\nThe agent-to-ROS bridge allows AI agents to interact with ROS 2 nodes seamlessly. This integration enables agents to:\n\n- Subscribe to sensor data from ROS nodes\n- Publish commands to robot actuators\n- Call services for specific robot operations\n- Maintain state information across the robot system\n\n## URDF for Humanoid Robots\n\nURDF (Unified Robot Description Format) is an XML format for representing a robot model. For humanoid robots, URDF describes:\n\n- Kinematic chains\n- Physical properties (mass, inertia)\n- Visual and collision models\n- Joint limits and types\n\nExample URDF snippet for a humanoid robot:\n\n```xml\n<robot name=\"humanoid_robot\">\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.2 0.2 0.2\"/>\n      </geometry>\n    </visual>\n  </link>\n\n  <joint name=\"base_to_torso\" type=\"fixed\">\n    <parent link=\"base_link\"/>\n    <child link=\"torso\"/>\n  </joint>\n\n  <link name=\"torso\">\n    <visual>\n      <geometry>\n        <box size=\"0.3 0.1 0.5\"/>\n      </geometry>\n    </visual>\n  </link>\n</robot>\n```\n\n## Best Practices\n\n1. **Modular Design**: Keep nodes focused on single responsibilities\n2. **Robust Communication**: Implement proper error handling for message passing\n3. **Resource Management**: Properly clean up resources when nodes are destroyed\n4. **Testing**: Use ROS 2 testing frameworks to validate node behavior\n5. **Documentation**: Maintain clear interfaces and communication patterns",
    "module": "Module 1: Robotic Nervous System (ROS 2)",
    "chapter": "ros2-basics",
    "path": "/docs/ros2-basics"
  },
  {
    "id": "tutorial-basics/congratulations",
    "title": "Congratulations!",
    "content": "\n# Congratulations!\n\nYou have just learned the **basics of Docusaurus** and made some changes to the **initial template**.\n\nDocusaurus has **much more to offer**!\n\nHave **5 more minutes**? Take a look at **[versioning](../tutorial-extras/manage-docs-versions.md)** and **[i18n](../tutorial-extras/translate-your-site.md)**.\n\nAnything **unclear** or **buggy** in this tutorial? [Please report it!](https://github.com/facebook/docusaurus/discussions/4610)\n\n## What's next?\n\n- Read the [official documentation](https://docusaurus.io/)\n- Modify your site configuration with [`docusaurus.config.js`](https://docusaurus.io/docs/api/docusaurus-config)\n- Add navbar and footer items with [`themeConfig`](https://docusaurus.io/docs/api/themes/configuration)\n- Add a custom [Design and Layout](https://docusaurus.io/docs/styling-layout)\n- Add a [search bar](https://docusaurus.io/docs/search)\n- Find inspirations in the [Docusaurus showcase](https://docusaurus.io/showcase)\n- Get involved in the [Docusaurus Community](https://docusaurus.io/community/support)\n",
    "module": "Introduction",
    "chapter": "congratulations",
    "path": "/docs/tutorial-basics/congratulations"
  },
  {
    "id": "tutorial-basics/create-a-blog-post",
    "title": "Create a Blog Post",
    "content": "\n# Create a Blog Post\n\nDocusaurus creates a **page for each blog post**, but also a **blog index page**, a **tag system**, an **RSS** feed...\n\n## Create your first Post\n\nCreate a file at `blog/2021-02-28-greetings.md`:\n\n```md title=\"blog/2021-02-28-greetings.md\"\n---\nslug: greetings\ntitle: Greetings!\nauthors:\n  - name: Joel Marcey\n    title: Co-creator of Docusaurus 1\n    url: https://github.com/JoelMarcey\n    image_url: https://github.com/JoelMarcey.png\n  - name: Sébastien Lorber\n    title: Docusaurus maintainer\n    url: https://sebastienlorber.com\n    image_url: https://github.com/slorber.png\ntags: [greetings]\n---\n\nCongratulations, you have made your first post!\n\nFeel free to play around and edit this post as much as you like.\n```\n\nA new blog post is now available at [http://localhost:3000/blog/greetings](http://localhost:3000/blog/greetings).\n",
    "module": "Introduction",
    "chapter": "create-a-blog-post",
    "path": "/docs/tutorial-basics/create-a-blog-post"
  },
  {
    "id": "tutorial-basics/create-a-document",
    "title": "Create a Document",
    "content": "\n# Create a Document\n\nDocuments are **groups of pages** connected through:\n\n- a **sidebar**\n- **previous/next navigation**\n- **versioning**\n\n## Create your first Doc\n\nCreate a Markdown file at `docs/hello.md`:\n\n```md title=\"docs/hello.md\"\n# Hello\n\nThis is my **first Docusaurus document**!\n```\n\nA new document is now available at [http://localhost:3000/docs/hello](http://localhost:3000/docs/hello).\n\n## Configure the Sidebar\n\nDocusaurus automatically **creates a sidebar** from the `docs` folder.\n\nAdd metadata to customize the sidebar label and position:\n\n```md title=\"docs/hello.md\" {1-4}\n---\nsidebar_label: 'Hi!'\nsidebar_position: 3\n---\n\n# Hello\n\nThis is my **first Docusaurus document**!\n```\n\nIt is also possible to create your sidebar explicitly in `sidebars.js`:\n\n```js title=\"sidebars.js\"\nexport default {\n  tutorialSidebar: [\n    'intro',\n    // highlight-next-line\n    'hello',\n    {\n      type: 'category',\n      label: 'Tutorial',\n      items: ['tutorial-basics/create-a-document'],\n    },\n  ],\n};\n```\n",
    "module": "Introduction",
    "chapter": "create-a-document",
    "path": "/docs/tutorial-basics/create-a-document"
  },
  {
    "id": "tutorial-basics/create-a-page",
    "title": "Create a Page",
    "content": "\n# Create a Page\n\nAdd **Markdown or React** files to `src/pages` to create a **standalone page**:\n\n- `src/pages/index.js` → `localhost:3000/`\n- `src/pages/foo.md` → `localhost:3000/foo`\n- `src/pages/foo/bar.js` → `localhost:3000/foo/bar`\n\n## Create your first React Page\n\nCreate a file at `src/pages/my-react-page.js`:\n\n```jsx title=\"src/pages/my-react-page.js\"\nimport React from 'react';\nimport Layout from '@theme/Layout';\n\nexport default function MyReactPage() {\n  return (\n    <Layout>\n      <h1>My React page</h1>\n      <p>This is a React page</p>\n    </Layout>\n  );\n}\n```\n\nA new page is now available at [http://localhost:3000/my-react-page](http://localhost:3000/my-react-page).\n\n## Create your first Markdown Page\n\nCreate a file at `src/pages/my-markdown-page.md`:\n\n```mdx title=\"src/pages/my-markdown-page.md\"\n# My Markdown page\n\nThis is a Markdown page\n```\n\nA new page is now available at [http://localhost:3000/my-markdown-page](http://localhost:3000/my-markdown-page).\n",
    "module": "Introduction",
    "chapter": "create-a-page",
    "path": "/docs/tutorial-basics/create-a-page"
  },
  {
    "id": "tutorial-basics/deploy-your-site",
    "title": "Deploy your site",
    "content": "\n# Deploy your site\n\nDocusaurus is a **static-site-generator** (also called **[Jamstack](https://jamstack.org/)**).\n\nIt builds your site as simple **static HTML, JavaScript and CSS files**.\n\n## Build your site\n\nBuild your site **for production**:\n\n```bash\nnpm run build\n```\n\nThe static files are generated in the `build` folder.\n\n## Deploy your site\n\nTest your production build locally:\n\n```bash\nnpm run serve\n```\n\nThe `build` folder is now served at [http://localhost:3000/](http://localhost:3000/).\n\nYou can now deploy the `build` folder **almost anywhere** easily, **for free** or very small cost (read the **[Deployment Guide](https://docusaurus.io/docs/deployment)**).\n",
    "module": "Introduction",
    "chapter": "deploy-your-site",
    "path": "/docs/tutorial-basics/deploy-your-site"
  },
  {
    "id": "tutorial-extras/manage-docs-versions",
    "title": "Manage Docs Versions",
    "content": "\n# Manage Docs Versions\n\nDocusaurus can manage multiple versions of your docs.\n\n## Create a docs version\n\nRelease a version 1.0 of your project:\n\n```bash\nnpm run docusaurus docs:version 1.0\n```\n\nThe `docs` folder is copied into `versioned_docs/version-1.0` and `versions.json` is created.\n\nYour docs now have 2 versions:\n\n- `1.0` at `http://localhost:3000/docs/` for the version 1.0 docs\n- `current` at `http://localhost:3000/docs/next/` for the **upcoming, unreleased docs**\n\n## Add a Version Dropdown\n\nTo navigate seamlessly across versions, add a version dropdown.\n\nModify the `docusaurus.config.js` file:\n\n```js title=\"docusaurus.config.js\"\nexport default {\n  themeConfig: {\n    navbar: {\n      items: [\n        // highlight-start\n        {\n          type: 'docsVersionDropdown',\n        },\n        // highlight-end\n      ],\n    },\n  },\n};\n```\n\nThe docs version dropdown appears in your navbar:\n\n![Docs Version Dropdown](./img/docsVersionDropdown.png)\n\n## Update an existing version\n\nIt is possible to edit versioned docs in their respective folder:\n\n- `versioned_docs/version-1.0/hello.md` updates `http://localhost:3000/docs/hello`\n- `docs/hello.md` updates `http://localhost:3000/docs/next/hello`\n",
    "module": "Introduction",
    "chapter": "manage-docs-versions",
    "path": "/docs/tutorial-extras/manage-docs-versions"
  },
  {
    "id": "tutorial-extras/translate-your-site",
    "title": "Translate your site",
    "content": "\n# Translate your site\n\nLet's translate `docs/intro.md` to French.\n\n## Configure i18n\n\nModify `docusaurus.config.js` to add support for the `fr` locale:\n\n```js title=\"docusaurus.config.js\"\nexport default {\n  i18n: {\n    defaultLocale: 'en',\n    locales: ['en', 'fr'],\n  },\n};\n```\n\n## Translate a doc\n\nCopy the `docs/intro.md` file to the `i18n/fr` folder:\n\n```bash\nmkdir -p i18n/fr/docusaurus-plugin-content-docs/current/\n\ncp docs/intro.md i18n/fr/docusaurus-plugin-content-docs/current/intro.md\n```\n\nTranslate `i18n/fr/docusaurus-plugin-content-docs/current/intro.md` in French.\n\n## Start your localized site\n\nStart your site on the French locale:\n\n```bash\nnpm run start -- --locale fr\n```\n\nYour localized site is accessible at [http://localhost:3000/fr/](http://localhost:3000/fr/) and the `Getting Started` page is translated.\n\n:::caution\n\nIn development, you can only use one locale at a time.\n\n:::\n\n## Add a Locale Dropdown\n\nTo navigate seamlessly across languages, add a locale dropdown.\n\nModify the `docusaurus.config.js` file:\n\n```js title=\"docusaurus.config.js\"\nexport default {\n  themeConfig: {\n    navbar: {\n      items: [\n        // highlight-start\n        {\n          type: 'localeDropdown',\n        },\n        // highlight-end\n      ],\n    },\n  },\n};\n```\n\nThe locale dropdown now appears in your navbar:\n\n![Locale Dropdown](./img/localeDropdown.png)\n\n## Build your localized site\n\nBuild your site for a specific locale:\n\n```bash\nnpm run build -- --locale fr\n```\n\nOr build your site to include all the locales at once:\n\n```bash\nnpm run build\n```\n",
    "module": "Introduction",
    "chapter": "translate-your-site",
    "path": "/docs/tutorial-extras/translate-your-site"
  }
]