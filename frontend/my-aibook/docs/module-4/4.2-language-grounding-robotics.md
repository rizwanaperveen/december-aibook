---
sidebar_position: 2
---

# Chapter 4.2 – Language Grounding in Robotics

## Learning Objectives
- Implement symbol grounding for natural language understanding
- Create world models for humanoid robots
- Develop task abstraction mechanisms for complex commands
- Define constraints and affordances for language-based robot control

## Symbol Grounding

Symbol grounding is the challenge of connecting linguistic symbols to real-world entities and actions. For humanoid robots, this involves mapping language to perceptions, actions, and environmental states:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Pose, Point
from sensor_msgs.msg import Image, PointCloud2
from visualization_msgs.msg import MarkerArray
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import spacy
import openai
from transformers import pipeline
from dataclasses import dataclass

@dataclass
class GroundedObject:
    """Represents a grounded object with spatial and semantic information"""
    name: str
    semantic_label: str
    position: Point
    orientation: Tuple[float, float, float, float]  # quaternion
    confidence: float
    bounding_box: Optional[Tuple[float, float, float, float]] = None  # x, y, w, h
    affordances: List[str] = None
    attributes: Dict[str, Any] = None

@dataclass
class GroundedAction:
    """Represents a grounded action with parameters"""
    action_type: str  # move, grasp, manipulate, etc.
    target_object: Optional[str] = None
    target_location: Optional[Point] = None
    parameters: Dict[str, Any] = None
    constraints: List[str] = None

class SymbolGroundingNode(Node):
    def __init__(self):
        super().__init__('symbol_grounding_node')

        # Initialize NLP components
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            self.get_logger().warn("spaCy model not found. Install with: python -m spacy download en_core_web_sm")
            self.nlp = None

        # Object grounding
        self.object_detector = pipeline("object-detection", model="facebook/detr-resnet-50")
        self.scene_graph = SceneGraph()
        self.semantic_map = SemanticMap()

        # Spatial grounding
        self.spatial_reasoner = SpatialReasoner()

        # Subscriptions
        self.language_sub = self.create_subscription(
            String, '/natural_language_command', self.language_callback, 10
        )
        self.image_sub = self.create_subscription(
            Image, '/camera/rgb/image_raw', self.image_callback, 10
        )
        self.pointcloud_sub = self.create_subscription(
            PointCloud2, '/camera/depth/points', self.pointcloud_callback, 10
        )

        # Publishers
        self.grounded_command_pub = self.create_publisher(
            String, '/grounded_robot_command', 10
        )
        self.visualization_pub = self.create_publisher(
            MarkerArray, '/grounding_visualization', 10
        )

        # Object and scene databases
        self.objects_in_scene: Dict[str, GroundedObject] = {}
        self.language_command_buffer = []

        self.get_logger().info('Symbol Grounding Node initialized')

    def language_callback(self, msg: String):
        """Process natural language command"""
        command = msg.data
        self.get_logger().info(f'Received language command: "{command}"')

        try:
            # Parse the command using NLP
            parsed_command = self.parse_command(command)

            # Ground symbols in the command to objects in the scene
            grounded_command = self.ground_command(parsed_command)

            # Publish grounded command
            grounded_msg = String()
            grounded_msg.data = str(grounded_command)
            self.grounded_command_pub.publish(grounded_msg)

            self.get_logger().info(f'Grounded command: {grounded_command}')

        except Exception as e:
            self.get_logger().error(f'Error processing language command: {e}')

    def image_callback(self, msg: Image):
        """Process image to detect and ground objects"""
        try:
            # Convert ROS image to format suitable for object detection
            image_np = self.ros_image_to_numpy(msg)

            # Detect objects in the image
            detections = self.object_detector(image_np)

            # Create grounded objects from detections
            for detection in detections:
                label = detection['label']
                score = detection['score']
                bbox = detection['box']

                if score > 0.5:  # Confidence threshold
                    # Convert 2D bounding box to 3D position using depth information
                    position_3d = self.get_3d_position_from_bbox(bbox)

                    grounded_obj = GroundedObject(
                        name=self.generate_object_name(label),
                        semantic_label=label,
                        position=position_3d,
                        orientation=(0.0, 0.0, 0.0, 1.0),  # Identity quaternion
                        confidence=score,
                        bounding_box=(bbox['xmin'], bbox['ymin'],
                                   bbox['xmax'] - bbox['xmin'],
                                   bbox['ymax'] - bbox['ymin']),
                        affordances=self.get_affordances_for_object(label),
                        attributes={'color': self.get_object_color(image_np, bbox)}
                    )

                    # Add to scene graph
                    self.scene_graph.add_object(grounded_obj)

                    # Store in local database
                    self.objects_in_scene[grounded_obj.name] = grounded_obj

                    self.get_logger().info(f'Detected and grounded object: {grounded_obj.name}')

        except Exception as e:
            self.get_logger().error(f'Error processing image: {e}')

    def pointcloud_callback(self, msg: PointCloud2):
        """Process point cloud for 3D object grounding"""
        try:
            # Convert PointCloud2 to numpy array
            pointcloud_np = self.pointcloud_to_numpy(msg)

            # Perform 3D object detection and clustering
            clusters = self.cluster_pointcloud(pointcloud_np)

            # Update object positions and add new objects
            for cluster in clusters:
                obj_position = self.calculate_cluster_centroid(cluster)
                # Update existing objects or create new ones based on proximity
                self.update_object_positions(obj_position)

        except Exception as e:
            self.get_logger().error(f'Error processing point cloud: {e}')

    def parse_command(self, command: str) -> Dict[str, Any]:
        """Parse natural language command using NLP"""
        if not self.nlp:
            return {'raw_command': command, 'entities': [], 'actions': []}

        doc = self.nlp(command)

        entities = []
        actions = []

        # Extract named entities
        for ent in doc.ents:
            entities.append({
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char
            })

        # Extract verbs (potential actions)
        for token in doc:
            if token.pos_ == 'VERB':
                actions.append({
                    'text': token.text,
                    'lemma': token.lemma_,
                    'pos': token.pos_
                })

        # Extract dependencies and relationships
        relationships = []
        for token in doc:
            if token.dep_ in ['nsubj', 'dobj', 'prep', 'pobj']:
                relationships.append({
                    'token': token.text,
                    'dependency': token.dep_,
                    'head': token.head.text
                })

        return {
            'raw_command': command,
            'entities': entities,
            'actions': actions,
            'relationships': relationships,
            'parsed_doc': doc
        }

    def ground_command(self, parsed_command: Dict[str, Any]) -> GroundedAction:
        """Ground symbols in command to objects and locations in the scene"""
        command = parsed_command['raw_command']

        # Extract action
        action = self.extract_action(parsed_command)

        # Ground target object
        target_obj_name = self.extract_target_object(parsed_command)
        target_object = self.find_closest_object(target_obj_name) if target_obj_name else None

        # Ground target location
        target_location = self.extract_target_location(parsed_command)

        # Extract action parameters
        parameters = self.extract_action_parameters(parsed_command)

        # Determine constraints
        constraints = self.extract_constraints(parsed_command)

        return GroundedAction(
            action_type=action,
            target_object=target_object.name if target_object else None,
            target_location=target_location,
            parameters=parameters,
            constraints=constraints
        )

    def extract_action(self, parsed_command: Dict[str, Any]) -> str:
        """Extract the main action from the command"""
        # Map common action words to robot actions
        action_mappings = {
            'move': ['go', 'move', 'walk', 'travel', 'approach'],
            'grasp': ['grasp', 'grab', 'take', 'pick up', 'hold'],
            'release': ['release', 'drop', 'put down', 'place'],
            'manipulate': ['manipulate', 'move', 'push', 'pull', 'turn'],
            'look_at': ['look at', 'see', 'observe', 'watch'],
            'point_at': ['point at', 'point to', 'indicate'],
            'navigate': ['navigate', 'go to', 'reach', 'move to']
        }

        command_lower = parsed_command['raw_command'].lower()

        for action, keywords in action_mappings.items():
            for keyword in keywords:
                if keyword in command_lower:
                    return action

        # Default action if none found
        return 'navigate'

    def extract_target_object(self, parsed_command: Dict[str, Any]) -> Optional[str]:
        """Extract target object from command"""
        # Look for entities that are objects
        for entity in parsed_command['entities']:
            if entity['label'] in ['OBJECT', 'PRODUCT', 'ENTITY']:  # Custom labels
                # Try to match with detected objects
                for obj_name, obj in self.objects_in_scene.items():
                    if entity['text'].lower() in obj_name.lower() or \
                       entity['text'].lower() in obj.semantic_label.lower():
                        return obj_name

        # If no direct match, use closest object based on semantic similarity
        for entity in parsed_command['entities']:
            for obj_name, obj in self.objects_in_scene.items():
                if self.semantic_similarity(entity['text'], obj.semantic_label) > 0.7:
                    return obj_name

        return None

    def extract_target_location(self, parsed_command: Dict[str, Any]) -> Optional[Point]:
        """Extract target location from command"""
        command_lower = parsed_command['raw_command'].lower()

        # Look for location-related keywords
        location_keywords = {
            'near': ['near', 'close to', 'beside', 'next to'],
            'on': ['on', 'on top of', 'above'],
            'under': ['under', 'below', 'beneath'],
            'behind': ['behind', 'back of'],
            'in_front_of': ['in front of', 'before', 'ahead of'],
            'left': ['left', 'to the left', 'left side'],
            'right': ['right', 'to the right', 'right side'],
            'relative': ['relative to', 'compared to']
        }

        for relation, keywords in location_keywords.items():
            for keyword in keywords:
                if keyword in command_lower:
                    # Find reference object
                    reference_obj = self.extract_reference_object(parsed_command)
                    if reference_obj:
                        # Calculate relative position
                        return self.spatial_reasoner.calculate_relative_position(
                            reference_obj.position, relation
                        )

        return None

    def semantic_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity between two texts"""
        # In practice, this would use embeddings or more sophisticated methods
        # For now, simple word overlap
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        intersection = words1.intersection(words2)
        union = words1.union(words2)

        if not union:
            return 0.0

        return len(intersection) / len(union)

    def find_closest_object(self, target_name: str) -> Optional[GroundedObject]:
        """Find the closest object matching the target name"""
        best_match = None
        best_similarity = 0.0

        for obj_name, obj in self.objects_in_scene.items():
            similarity = self.semantic_similarity(target_name, obj_name)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = obj

        return best_match

    def get_affordances_for_object(self, object_type: str) -> List[str]:
        """Get possible affordances for an object type"""
        affordance_map = {
            'chair': ['sit_on', 'move', 'push'],
            'table': ['place_on', 'touch', 'move_around'],
            'cup': ['grasp', 'lift', 'drink_from'],
            'bottle': ['grasp', 'lift', 'pour_from'],
            'door': ['open', 'close', 'push', 'pull'],
            'box': ['move', 'grasp', 'stack'],
            'ball': ['grasp', 'throw', 'kick'],
            'person': ['greet', 'follow', 'avoid'],
            'robot': ['interact', 'follow', 'avoid'],
            'wall': ['avoid', 'follow_along'],
            'floor': ['walk_on', 'stand_on'],
            'ceiling': ['look_up_at']
        }

        return affordance_map.get(object_type.lower(), ['approach', 'observe'])

    def generate_object_name(self, semantic_label: str) -> str:
        """Generate a unique name for an object"""
        counter = 1
        base_name = semantic_label.lower().replace(' ', '_')

        while f"{base_name}_{counter}" in self.objects_in_scene:
            counter += 1

        return f"{base_name}_{counter}"

    def get_object_color(self, image: np.ndarray, bbox: Dict) -> str:
        """Extract dominant color from object bounding box"""
        xmin, ymin = int(bbox['xmin']), int(bbox['ymin'])
        xmax, ymax = int(bbox['xmax']), int(bbox['ymax'])

        # Extract region of interest
        roi = image[ymin:ymax, xmin:xmax]

        # Calculate dominant color (simplified)
        if roi.size > 0:
            avg_color = np.mean(roi, axis=(0, 1))
            return self.rgb_to_color_name(avg_color)
        else:
            return "unknown"

    def rgb_to_color_name(self, rgb: np.ndarray) -> str:
        """Convert RGB values to color name"""
        r, g, b = rgb.astype(int)

        # Simplified color classification
        if r > g and r > b:
            return "red" if r > 100 else "dark"
        elif g > r and g > b:
            return "green" if g > 100 else "dark"
        elif b > r and b > g:
            return "blue" if b > 100 else "dark"
        elif r == g == b:
            return "gray" if r > 100 else "black"
        else:
            return "multicolor"

    def get_3d_position_from_bbox(self, bbox: Dict) -> Point:
        """Estimate 3D position from 2D bounding box using depth information"""
        # This is a simplified approach
        # In practice, you'd use depth from aligned depth image
        center_x = (bbox['xmin'] + bbox['xmax']) / 2
        center_y = (bbox['ymin'] + bbox['ymax']) / 2

        # Placeholder - in real implementation, you'd get depth at this pixel
        depth_estimate = 1.0  # meters

        # Convert pixel coordinates to 3D position
        # This assumes known camera parameters
        fx, fy = 554.0, 554.0  # Camera intrinsic parameters (placeholder)
        cx, cy = 320.0, 240.0  # Camera center (placeholder)

        x = (center_x - cx) * depth_estimate / fx
        y = (center_y - cy) * depth_estimate / fy
        z = depth_estimate

        position = Point()
        position.x = x
        position.y = y
        position.z = z

        return position

class SpatialReasoner:
    """Handles spatial relationships and reasoning"""
    def __init__(self):
        self.reference_frame = 'map'  # or 'base_link', 'camera', etc.

    def calculate_relative_position(self, reference_pos: Point,
                                  relation: str) -> Point:
        """Calculate position relative to a reference object"""
        offset = Point()

        if relation == 'near':
            offset.x = 0.5  # 0.5m in front
            offset.y = 0.0
            offset.z = 0.0
        elif relation == 'on':
            offset.x = 0.0
            offset.y = 0.0
            offset.z = 0.5  # 0.5m above
        elif relation == 'under':
            offset.x = 0.0
            offset.y = 0.0
            offset.z = -0.5  # 0.5m below
        elif relation == 'behind':
            offset.x = -0.5  # 0.5m behind
            offset.y = 0.0
            offset.z = 0.0
        elif relation == 'in_front_of':
            offset.x = 0.5  # 0.5m in front
            offset.y = 0.0
            offset.z = 0.0
        elif relation == 'left':
            offset.x = 0.0
            offset.y = 0.5  # 0.5m to the left
            offset.z = 0.0
        elif relation == 'right':
            offset.x = 0.0
            offset.y = -0.5  # 0.5m to the right
            offset.z = 0.0

        # Calculate absolute position
        result = Point()
        result.x = reference_pos.x + offset.x
        result.y = reference_pos.y + offset.y
        result.z = reference_pos.z + offset.z

        return result

    def is_within_workspace(self, position: Point, robot_base: Point) -> bool:
        """Check if position is within robot's workspace"""
        # Calculate distance from robot base
        distance = np.sqrt(
            (position.x - robot_base.x)**2 +
            (position.y - robot_base.y)**2 +
            (position.z - robot_base.z)**2
        )

        # Assume humanoid workspace is roughly 1m radius
        return distance <= 1.0

class SceneGraph:
    """Maintains relationships between objects in the scene"""
    def __init__(self):
        self.nodes = {}  # Object name -> GroundedObject
        self.edges = {}  # Relationships between objects

    def add_object(self, obj: GroundedObject):
        """Add an object to the scene graph"""
        self.nodes[obj.name] = obj

    def add_relationship(self, obj1_name: str, relationship: str, obj2_name: str):
        """Add a relationship between two objects"""
        if obj1_name not in self.edges:
            self.edges[obj1_name] = {}
        self.edges[obj1_name][relationship] = obj2_name

    def find_objects_by_property(self, property_name: str, property_value: Any) -> List[str]:
        """Find objects with a specific property"""
        matches = []
        for name, obj in self.nodes.items():
            if hasattr(obj, property_name) and getattr(obj, property_name) == property_value:
                matches.append(name)
        return matches

    def get_spatial_relationship(self, obj1_name: str, obj2_name: str) -> str:
        """Get the spatial relationship between two objects"""
        # Calculate geometric relationship
        if obj1_name in self.nodes and obj2_name in self.nodes:
            pos1 = self.nodes[obj1_name].position
            pos2 = self.nodes[obj2_name].position

            # Calculate relative position
            dx = pos2.x - pos1.x
            dy = pos2.y - pos1.y
            dz = pos2.z - pos1.z

            # Determine spatial relationship
            if abs(dx) > abs(dy) and abs(dx) > abs(dz):
                if dx > 0:
                    return 'right_of'
                else:
                    return 'left_of'
            elif abs(dy) > abs(dx) and abs(dy) > abs(dz):
                if dy > 0:
                    return 'above'
                else:
                    return 'below'
            else:
                if dz > 0:
                    return 'in_front_of'
                else:
                    return 'behind'

        return 'unknown'

class SemanticMap:
    """Maintains semantic information about the environment"""
    def __init__(self):
        self.regions = {}  # Area name -> objects and properties
        self.semantic_annotations = {}  # Object -> semantic properties

    def annotate_object(self, object_name: str, annotations: Dict[str, Any]):
        """Add semantic annotations to an object"""
        self.semantic_annotations[object_name] = annotations

    def get_region_for_position(self, position: Point) -> Optional[str]:
        """Get the semantic region for a given position"""
        # In practice, this would use a semantic map
        # For now, return a generic region
        return "unknown_region"

# Example usage and integration
class LanguageGroundingDemo:
    """Demonstration of language grounding capabilities"""
    def __init__(self):
        self.symbol_grounding = SymbolGroundingNode()

    def demonstrate_grounding(self):
        """Demonstrate various grounding scenarios"""
        test_commands = [
            "Go to the red chair",
            "Grasp the blue cup on the table",
            "Move the box to the left of the door",
            "Look at the person near the window",
            "Navigate to the kitchen"
        ]

        for command in test_commands:
            print(f"\nProcessing command: '{command}'")

            # Parse command
            parsed = self.symbol_grounding.parse_command(command)
            print(f"Parsed: {parsed['actions']}, {parsed['entities']}")

            # Ground command (simulated)
            # In practice, this would require actual objects in the scene
            print(f"Grounded command would target specific objects and locations")

def main(args=None):
    rclpy.init(args=args)
    node = SymbolGroundingNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## World Models for Humanoid Robots

Humanoid robots require sophisticated world models that account for their unique capabilities and constraints:

```python
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
import numpy as np
from geometry_msgs.msg import Point, Pose
from std_msgs.msg import Header

@dataclass
class HumanoidState:
    """Represents the state of a humanoid robot"""
    base_pose: Pose
    joint_positions: Dict[str, float]
    joint_velocities: Dict[str, float]
    center_of_mass: Point
    zero_moment_point: Point
    support_polygon: List[Point]  # Points of contact with ground
    balance_state: str  # stable, unstable, recovering
    energy_level: float  # Battery or power level

@dataclass
class HumanoidCapabilities:
    """Defines what a humanoid robot can do"""
    max_linear_velocity: float = 0.5  # m/s
    max_angular_velocity: float = 0.5  # rad/s
    step_height: float = 0.1  # Maximum step height
    step_width: float = 0.3  # Maximum step width
    reach_distance: float = 0.8  # Max reach from base
    lifting_capacity: float = 5.0  # Max weight (kg)
    balance_recovery_time: float = 2.0  # Time to recover balance (s)
    walking_types: List[str] = None  # Walk types: 'flat', 'stairs', 'uneven'

@dataclass
class WorldState:
    """Represents the current state of the world"""
    timestamp: float
    robot_state: HumanoidState
    objects: Dict[str, GroundedObject]
    static_environment: Dict[str, Any]  # Maps, layouts, etc.
    dynamic_entities: List[Any]  # Moving objects, people, etc.
    affordances: Dict[str, List[str]]  # What can be done with objects

class WorldModel:
    """Maintains and updates the robot's world model"""
    def __init__(self, robot_capabilities: HumanoidCapabilities):
        self.capabilities = robot_capabilities
        self.current_state = None
        self.belief_state = {}  # Probabilistic beliefs about world
        self.action_history = []
        self.goal_stack = []

    def update_from_perception(self, sensor_data: Dict[str, Any]):
        """Update world model based on sensor data"""
        # Update objects based on new detections
        if 'objects' in sensor_data:
            for obj_data in sensor_data['objects']:
                self.update_object_beliefs(obj_data)

        # Update robot state based on odometry and IMU
        if 'robot_state' in sensor_data:
            self.update_robot_state(sensor_data['robot_state'])

        # Update spatial relationships
        self.update_spatial_relations()

    def update_object_beliefs(self, obj_data: Dict[str, Any]):
        """Update beliefs about a specific object"""
        obj_name = obj_data.get('name', 'unknown')

        # Update confidence in object existence
        current_confidence = self.belief_state.get(f'exists_{obj_name}', 0.0)
        detection_confidence = obj_data.get('confidence', 0.8)

        # Apply Bayes rule to update belief
        new_confidence = self.update_probability(
            current_confidence,
            detection_confidence,
            positive=True
        )

        self.belief_state[f'exists_{obj_name}'] = new_confidence

        # Update object properties
        if new_confidence > 0.5:  # Only update if object likely exists
            if 'position' in obj_data:
                self.belief_state[f'position_{obj_name}'] = obj_data['position']
            if 'affordances' in obj_data:
                self.belief_state[f'affordances_{obj_name}'] = obj_data['affordances']

    def update_robot_state(self, robot_data: Dict[str, Any]):
        """Update the robot's internal state"""
        if self.current_state is None:
            self.current_state = HumanoidState(
                base_pose=robot_data.get('pose', Pose()),
                joint_positions=robot_data.get('joint_positions', {}),
                joint_velocities=robot_data.get('joint_velocities', {}),
                center_of_mass=robot_data.get('com', Point()),
                zero_moment_point=robot_data.get('zmp', Point()),
                support_polygon=robot_data.get('support_polygon', []),
                balance_state=robot_data.get('balance_state', 'stable'),
                energy_level=robot_data.get('energy', 1.0)
            )
        else:
            # Update existing state
            self.current_state.base_pose = robot_data.get('pose', self.current_state.base_pose)
            self.current_state.joint_positions.update(robot_data.get('joint_positions', {}))
            self.current_state.joint_velocities.update(robot_data.get('joint_velocities', {}))
            self.current_state.center_of_mass = robot_data.get('com', self.current_state.center_of_mass)
            self.current_state.zero_moment_point = robot_data.get('zmp', self.current_state.zero_moment_point)
            self.current_state.balance_state = robot_data.get('balance_state', self.current_state.balance_state)
            self.current_state.energy_level = robot_data.get('energy', self.current_state.energy_level)

    def can_execute_action(self, action: GroundedAction) -> Tuple[bool, List[str]]:
        """Check if robot can execute a given action"""
        reasons = []

        # Check if target object exists with sufficient confidence
        if action.target_object:
            obj_confidence = self.belief_state.get(f'exists_{action.target_object}', 0.0)
            if obj_confidence < 0.7:
                reasons.append(f"Target object '{action.target_object}' not confidently detected")
                return False, reasons

            # Check if object is reachable
            obj_position = self.belief_state.get(f'position_{action.target_object}')
            if obj_position and self.current_state:
                distance = self.calculate_distance(
                    self.current_state.base_pose.position,
                    obj_position
                )

                if distance > self.capabilities.reach_distance:
                    reasons.append(f"Target object out of reach ({distance:.2f}m > {self.capabilities.reach_distance}m)")
                    return False, reasons

        # Check action-specific constraints
        if action.action_type == 'grasp':
            # Check lifting capacity
            obj_weight = self.belief_state.get(f'weight_{action.target_object}', 1.0)  # Assume 1kg if unknown
            if obj_weight > self.capabilities.lifting_capacity:
                reasons.append(f"Object too heavy ({obj_weight}kg > {self.capabilities.lifting_capacity}kg)")
                return False, reasons

        elif action.action_type == 'navigate':
            # Check if target location is feasible
            if action.target_location:
                # Check if location is traversable for humanoid
                if not self.is_traversable(action.target_location):
                    reasons.append("Target location not traversable for humanoid")
                    return False, reasons

        elif action.action_type == 'balance':
            # Check if robot is currently stable enough to perform balance action
            if self.current_state and self.current_state.balance_state != 'stable':
                reasons.append("Robot not in stable state for balance action")
                return False, reasons

        return True, reasons

    def calculate_distance(self, pos1: Point, pos2: Point) -> float:
        """Calculate Euclidean distance between two points"""
        return np.sqrt(
            (pos2.x - pos1.x)**2 +
            (pos2.y - pos1.y)**2 +
            (pos2.z - pos1.z)**2
        )

    def is_traversable(self, location: Point) -> bool:
        """Check if location is traversable for humanoid"""
        # In practice, this would check a traversability map
        # For now, assume flat ground is traversable
        # Check if location is not too steep
        # Check for obstacles
        return True  # Simplified for example

    def predict_action_outcome(self, action: GroundedAction) -> WorldState:
        """Predict the outcome of executing an action"""
        # Create a copy of current world state
        predicted_state = WorldState(
            timestamp=self.current_state.timestamp if self.current_state else 0.0,
            robot_state=self.current_state,
            objects=self.belief_state.copy(),
            static_environment={},
            dynamic_entities=[],
            affordances={}
        )

        # Predict changes based on action
        if action.action_type == 'navigate':
            if action.target_location:
                # Update robot position prediction
                if predicted_state.robot_state:
                    predicted_state.robot_state.base_pose.position = action.target_location

        elif action.action_type == 'grasp':
            if action.target_object:
                # Update object state to indicate it's grasped
                predicted_state.belief_state[f'grasped_by_robot_{action.target_object}'] = True

        return predicted_state

    def update_probability(self, prior: float, likelihood: float, positive: bool = True) -> float:
        """Update probability using Bayesian inference"""
        if positive:
            # P(H|D) = P(D|H) * P(H) / P(D)
            # Where P(D) = P(D|H) * P(H) + P(D|¬H) * P(¬H)
            p_d_given_h = likelihood
            p_d_given_not_h = 1 - likelihood  # False positive rate

            numerator = p_d_given_h * prior
            denominator = (p_d_given_h * prior) + (p_d_given_not_h * (1 - prior))

            if denominator == 0:
                return 0.0

            return min(numerator / denominator, 1.0)
        else:
            # Negative evidence
            return prior * (1 - likelihood)
```

## Task Abstraction

Complex commands need to be decomposed into simpler actions:

```python
from enum import Enum
from typing import Callable, Any

class TaskStatus(Enum):
    PENDING = "pending"
    EXECUTING = "executing"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class Task:
    """Represents an abstract task"""
    name: str
    description: str
    prerequisites: List[str]  # Other tasks that must be completed first
    subtasks: List['Task']  # Decomposed subtasks
    action: Optional[Callable] = None  # Direct action if primitive
    parameters: Dict[str, Any] = None
    status: TaskStatus = TaskStatus.PENDING
    dependencies: List[str] = None

class TaskDecomposer:
    """Decomposes high-level tasks into executable subtasks"""
    def __init__(self, world_model: WorldModel):
        self.world_model = world_model
        self.task_library = self.build_task_library()

    def build_task_library(self) -> Dict[str, Callable]:
        """Build a library of known task decompositions"""
        return {
            'fetch_object': self.decompose_fetch_object,
            'navigate_to_location': self.decompose_navigate_to_location,
            'manipulate_object': self.decompose_manipulate_object,
            'perform_household_task': self.decompose_household_task,
            'assist_human': self.decompose_assist_human,
        }

    def decompose_command(self, command: GroundedAction) -> Task:
        """Decompose a grounded command into a task hierarchy"""
        # Determine the appropriate decomposition based on action type
        if command.action_type in self.task_library:
            return self.task_library[command.action_type](command)
        else:
            # Default decomposition for unknown actions
            return self.decompose_generic_action(command)

    def decompose_fetch_object(self, command: GroundedAction) -> Task:
        """Decompose 'fetch object' command"""
        if not command.target_object:
            raise ValueError("Fetch command requires target object")

        # Check if object is known
        obj_confidence = self.world_model.belief_state.get(f'exists_{command.target_object}', 0.0)
        if obj_confidence < 0.5:
            # Need to search for object first
            return Task(
                name=f"search_and_fetch_{command.target_object}",
                description=f"Search for {command.target_object} and then fetch it",
                prerequisites=[],
                subtasks=[
                    self.create_search_task(command.target_object),
                    self.create_navigate_task(command.target_object),
                    self.create_grasp_task(command.target_object),
                    self.create_return_task()
                ],
                parameters={'target_object': command.target_object}
            )
        else:
            # Object is known, proceed directly
            return Task(
                name=f"fetch_{command.target_object}",
                description=f"Fetch {command.target_object}",
                prerequisites=[],
                subtasks=[
                    self.create_navigate_task(command.target_object),
                    self.create_grasp_task(command.target_object),
                    self.create_return_task()
                ],
                parameters={'target_object': command.target_object}
            )

    def decompose_navigate_to_location(self, command: GroundedAction) -> Task:
        """Decompose navigation command"""
        if not command.target_location:
            raise ValueError("Navigation command requires target location")

        return Task(
            name=f"navigate_to_{hash(str(command.target_location))}",
            description=f"Navigate to location {command.target_location}",
            prerequisites=[],
            subtasks=[
                self.create_path_planning_task(command.target_location),
                self.create_motion_execution_task(command.target_location)
            ],
            parameters={'target_location': command.target_location}
        )

    def create_search_task(self, target_object: str) -> Task:
        """Create a task to search for an object"""
        return Task(
            name=f"search_for_{target_object}",
            description=f"Search for {target_object}",
            prerequisites=[],
            subtasks=[
                Task(
                    name=f"scan_area_for_{target_object}",
                    description="Scan surrounding area for the object",
                    prerequisites=[],
                    subtasks=[],
                    action=self.execute_scan_action,
                    parameters={'target_object': target_object}
                )
            ],
            action=self.execute_search_action,
            parameters={'target_object': target_object}
        )

    def create_navigate_task(self, target_object: str) -> Task:
        """Create a navigation task to reach an object"""
        return Task(
            name=f"navigate_to_{target_object}",
            description=f"Navigate to {target_object}",
            prerequisites=[f"locate_{target_object}"],  # Need to know where it is
            subtasks=[
                Task(
                    name=f"plan_path_to_{target_object}",
                    description="Plan path to object",
                    prerequisites=[],
                    subtasks=[],
                    action=self.execute_path_planning,
                    parameters={'target_object': target_object}
                ),
                Task(
                    name=f"execute_navigation_to_{target_object}",
                    description="Execute planned navigation",
                    prerequisites=[f"plan_path_to_{target_object}"],
                    subtasks=[],
                    action=self.execute_navigation,
                    parameters={'target_object': target_object}
                )
            ],
            parameters={'target_object': target_object}
        )

    def create_grasp_task(self, target_object: str) -> Task:
        """Create a grasping task"""
        return Task(
            name=f"grasp_{target_object}",
            description=f"Grasp {target_object}",
            prerequisites=[f"reach_{target_object}"],  # Need to be close first
            subtasks=[
                Task(
                    name=f"approach_{target_object}",
                    description="Approach the object carefully",
                    prerequisites=[],
                    subtasks=[],
                    action=self.execute_approach,
                    parameters={'target_object': target_object}
                ),
                Task(
                    name=f"align_gripper_{target_object}",
                    description="Align gripper with object",
                    prerequisites=[f"approach_{target_object}"],
                    subtasks=[],
                    action=self.execute_alignment,
                    parameters={'target_object': target_object}
                ),
                Task(
                    name=f"execute_grasp_{target_object}",
                    description="Execute grasp motion",
                    prerequisites=[f"align_gripper_{target_object}"],
                    subtasks=[],
                    action=self.execute_grasp,
                    parameters={'target_object': target_object}
                )
            ],
            parameters={'target_object': target_object}
        )

    def create_path_planning_task(self, target_location: Point) -> Task:
        """Create a path planning task"""
        return Task(
            name=f"plan_path_to_{hash(str(target_location))}",
            description="Plan path to target location",
            prerequisites=[],
            subtasks=[],
            action=self.execute_path_planning,
            parameters={'target_location': target_location}
        )

    def create_motion_execution_task(self, target_location: Point) -> Task:
        """Create a motion execution task"""
        return Task(
            name=f"move_to_{hash(str(target_location))}",
            description="Execute motion to target location",
            prerequisites=[f"plan_path_to_{hash(str(target_location))}"],
            subtasks=[],
            action=self.execute_navigation,
            parameters={'target_location': target_location}
        )

    def execute_search_action(self, params: Dict[str, Any]) -> bool:
        """Execute search action"""
        target_object = params['target_object']
        self.world_model.symbol_grounding.get_logger().info(f"Searching for {target_object}")

        # In practice, this would involve:
        # - Turning head/body to scan area
        # - Using vision to detect the object
        # - Updating world model with detection results

        # For simulation, assume search succeeds
        return True

    def execute_path_planning(self, params: Dict[str, Any]) -> bool:
        """Execute path planning"""
        target_location = params['target_location']
        self.world_model.symbol_grounding.get_logger().info(f"Planning path to {target_location}")

        # Use navigation stack for path planning
        # This would interface with Nav2 or similar
        return True

    def execute_navigation(self, params: Dict[str, Any]) -> bool:
        """Execute navigation"""
        target_location = params.get('target_location')
        target_object = params.get('target_object')

        if target_location:
            self.world_model.symbol_grounding.get_logger().info(f"Navigating to {target_location}")
        elif target_object:
            self.world_model.symbol_grounding.get_logger().info(f"Navigating to {target_object}")

        # This would send navigation goals to the robot
        return True

    def execute_grasp(self, params: Dict[str, Any]) -> bool:
        """Execute grasping action"""
        target_object = params['target_object']
        self.world_model.symbol_grounding.get_logger().info(f"Attempting to grasp {target_object}")

        # This would send grasp commands to the robot
        return True

# Integration example
def integrate_language_grounding_with_task_system():
    """Example of integrating language grounding with task abstraction"""

    # Initialize capabilities
    capabilities = HumanoidCapabilities(
        max_linear_velocity=0.3,
        max_angular_velocity=0.4,
        step_height=0.15,
        reach_distance=0.9,
        lifting_capacity=3.0
    )

    # Initialize world model
    world_model = WorldModel(capabilities)

    # Initialize task decomposer
    task_decomposer = TaskDecomposer(world_model)

    # Example: Ground a command and decompose it
    grounded_command = GroundedAction(
        action_type='fetch_object',
        target_object='red_cup',
        target_location=None,
        parameters={'precision': 'high'},
        constraints=['avoid_people', 'be_safe']
    )

    # Decompose the command into tasks
    root_task = task_decomposer.decompose_command(grounded_command)

    print(f"Decomposed command into task: {root_task.name}")
    print(f"Subtasks: {[subtask.name for subtask in root_task.subtasks]}")

    return root_task
```

## Constraints and Affordances

Defining what the robot can and cannot do based on language commands:

```python
class ConstraintHandler:
    """Handles constraints derived from language commands"""
    def __init__(self, world_model: WorldModel):
        self.world_model = world_model
        self.constraint_templates = self.define_constraint_templates()

    def define_constraint_templates(self) -> Dict[str, Callable]:
        """Define constraint templates that can be instantiated from language"""
        return {
            'spatial': self.handle_spacial_constraint,
            'temporal': self.handle_temporal_constraint,
            'physical': self.handle_physical_constraint,
            'social': self.handle_social_constraint,
            'safety': self.handle_safety_constraint
        }

    def parse_constraints_from_command(self, command: str) -> List[Dict[str, Any]]:
        """Parse constraints from natural language command"""
        constraints = []

        # Look for spatial constraints
        if 'avoid' in command.lower() or 'stay away' in command.lower():
            constraints.append({
                'type': 'spatial',
                'constraint_subtype': 'avoid_region',
                'parameters': self.extract_region(command)
            })

        if 'near' in command.lower() or 'close to' in command.lower():
            constraints.append({
                'type': 'spatial',
                'constraint_subtype': 'proximity',
                'parameters': self.extract_reference_object(command)
            })

        # Look for safety constraints
        if 'carefully' in command.lower() or 'safely' in command.lower():
            constraints.append({
                'type': 'safety',
                'constraint_subtype': 'reduce_speed',
                'parameters': {'factor': 0.5}  # Reduce speed by half
            })

        if 'slowly' in command.lower():
            constraints.append({
                'type': 'safety',
                'constraint_subtype': 'reduce_speed',
                'parameters': {'factor': 0.3}  # Reduce speed significantly
            })

        # Look for temporal constraints
        if 'quickly' in command.lower() or 'fast' in command.lower():
            constraints.append({
                'type': 'temporal',
                'constraint_subtype': 'minimize_time',
                'parameters': {}
            })

        # Look for social constraints
        if 'people' in command.lower() or 'person' in command.lower():
            constraints.append({
                'type': 'social',
                'constraint_subtype': 'maintain_personal_space',
                'parameters': {'distance': 1.0}  # 1 meter personal space
            })

        return constraints

    def apply_constraints_to_plan(self, task_plan: Task, constraints: List[Dict[str, Any]]) -> Task:
        """Apply constraints to a task plan"""
        constrained_plan = task_plan

        for constraint in constraints:
            constraint_handler = self.constraint_templates.get(constraint['type'])
            if constraint_handler:
                constrained_plan = constraint_handler(constrained_plan, constraint)

        return constrained_plan

    def handle_spacial_constraint(self, task_plan: Task, constraint: Dict[str, Any]) -> Task:
        """Handle spatial constraints"""
        if constraint['constraint_subtype'] == 'avoid_region':
            # Modify navigation tasks to avoid specific regions
            for subtask in task_plan.subtasks:
                if 'navigate' in subtask.name:
                    # Add avoidance constraints to path planning
                    if 'avoid_regions' not in subtask.parameters:
                        subtask.parameters['avoid_regions'] = []
                    subtask.parameters['avoid_regions'].append(
                        constraint['parameters']['region']
                    )

        elif constraint['constraint_subtype'] == 'proximity':
            # Ensure robot stays near a specific object
            for subtask in task_plan.subtasks:
                if 'navigate' in subtask.name:
                    subtask.parameters['proximity_constraint'] = constraint['parameters']['object']

        return task_plan

    def handle_safety_constraint(self, task_plan: Task, constraint: Dict[str, Any]) -> Task:
        """Handle safety constraints"""
        if constraint['constraint_subtype'] == 'reduce_speed':
            # Reduce speed for all motion-related tasks
            factor = constraint['parameters']['factor']
            for subtask in task_plan.subtasks:
                if any(motion_word in subtask.name for motion_word in ['navigate', 'move', 'motion']):
                    if 'speed_factor' not in subtask.parameters:
                        subtask.parameters['speed_factor'] = factor
                    else:
                        # Use the most conservative (lowest) speed factor
                        subtask.parameters['speed_factor'] = min(
                            subtask.parameters['speed_factor'],
                            factor
                        )

        return task_plan

    def extract_region(self, command: str) -> Dict[str, Any]:
        """Extract region information from command"""
        # In practice, this would use more sophisticated NLP
        # For now, return a placeholder
        return {'region': 'unknown', 'object': self.extract_reference_object(command)}

    def extract_reference_object(self, command: str) -> str:
        """Extract reference object from command"""
        # Simple keyword extraction
        # In practice, use proper NLP
        if 'table' in command.lower():
            return 'table'
        elif 'chair' in command.lower():
            return 'chair'
        elif 'person' in command.lower():
            return 'person'
        else:
            return 'unknown'

# Example of complete integration
def complete_language_grounding_example():
    """Complete example integrating all components"""

    # Initialize system components
    capabilities = HumanoidCapabilities()
    world_model = WorldModel(capabilities)
    task_decomposer = TaskDecomposer(world_model)
    constraint_handler = ConstraintHandler(world_model)

    # Simulate a natural language command
    raw_command = "Carefully navigate near the table and gently grasp the red cup, avoiding people"

    # Parse command (simplified - in practice use NLP)
    parsed = {
        'raw_command': raw_command,
        'entities': [{'text': 'table', 'label': 'OBJECT'}, {'text': 'red cup', 'label': 'OBJECT'}],
        'actions': [{'text': 'navigate', 'lemma': 'navigate'}, {'text': 'grasp', 'lemma': 'grasp'}]
    }

    # Create grounded action (simplified)
    grounded_action = GroundedAction(
        action_type='fetch_object',
        target_object='red_cup',
        target_location=None,  # Will be determined from context
        parameters={'precision': 'high'},
        constraints=[]
    )

    # Parse constraints from command
    constraints = constraint_handler.parse_constraints_from_command(raw_command)

    # Decompose command into tasks
    task_plan = task_decomposer.decompose_command(grounded_action)

    # Apply constraints to task plan
    constrained_task_plan = constraint_handler.apply_constraints_to_plan(task_plan, constraints)

    print(f"Original command: {raw_command}")
    print(f"Constraints identified: {[(c['constraint_subtype'], c.get('parameters', {})) for c in constraints]}")
    print(f"Task plan: {constrained_task_plan.name}")
    print(f"Subtasks: {[sub.name for sub in constrained_task_plan.subtasks]}")

    # Check if robot can execute the constrained plan
    can_execute, reasons = world_model.can_execute_action(grounded_action)
    print(f"Can execute: {can_execute}, Reasons: {reasons}")

    return constrained_task_plan

if __name__ == "__main__":
    # Run the complete example
    task_plan = complete_language_grounding_example()
```

## Practical Exercise

1. Implement symbol grounding for common household objects
2. Create a world model that tracks object states and relationships
3. Develop task decomposition for multi-step commands
4. Define constraints based on natural language modifiers

## Summary

Language grounding in robotics connects natural language to real-world entities, actions, and states. For humanoid robots, this requires sophisticated understanding of spatial relationships, affordances, and constraints. The system must maintain a dynamic world model, decompose complex commands into executable tasks, and respect constraints expressed in natural language. Successful language grounding enables intuitive human-robot interaction for complex manipulation and navigation tasks.